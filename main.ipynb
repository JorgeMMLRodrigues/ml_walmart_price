{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6100c02c",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/code/jorgemmlrodrigues/walmart-price-pred?scriptVersionId=247661249\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701bea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T02:01:35.466416Z",
     "iopub.status.busy": "2025-06-27T02:01:35.466122Z",
     "iopub.status.idle": "2025-06-27T02:01:44.183096Z",
     "shell.execute_reply": "2025-06-27T02:01:44.182411Z",
     "shell.execute_reply.started": "2025-06-27T02:01:35.466392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Kaggle\n",
    "!git -C /kaggle/working clone --depth 1 https://github.com/JorgeMMLRodrigues/ml_walmart_price.git || \\\n",
    " git -C /kaggle/working/ml_walmart_price pull    \n",
    "!pip install -q -r /kaggle/working/ml_walmart_price/requirements.txt\n",
    "\n",
    "import sys, os\n",
    "os.chdir(\"/kaggle/working/ml_walmart_price\")\n",
    "sys.path.insert(0, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d466e-5ee6-4c6d-ba0e-e5007bdbc98d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T02:01:45.752693Z",
     "iopub.status.busy": "2025-06-27T02:01:45.751766Z",
     "iopub.status.idle": "2025-06-27T02:01:52.517537Z",
     "shell.execute_reply": "2025-06-27T02:01:52.516853Z",
     "shell.execute_reply.started": "2025-06-27T02:01:45.752644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from utils.trainer import ModelTrainer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import random, time, inspect,time,json,os,requests,sys, math\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "from dateutil.easter import easter\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "import akshare as ak\n",
    "\n",
    "from pandas_datareader.data import DataReader\n",
    "from pandas_datareader import wb\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, f1_score, roc_auc_score,make_scorer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.linear_model import RidgeCV, Lasso,ElasticNetCV\n",
    "from sklearn.experimental import enable_halving_search_cv \n",
    "from sklearn.model_selection import TimeSeriesSplit,ParameterGrid,GridSearchCV, HalvingRandomSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.inspection import PartialDependenceDisplay,permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor,ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83caba5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.064462Z",
     "iopub.status.idle": "2025-06-26T22:10:14.06474Z",
     "shell.execute_reply": "2025-06-26T22:10:14.06463Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.064618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# skip cells:\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ecb136",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73590342",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca79ac9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.066252Z",
     "iopub.status.idle": "2025-06-26T22:10:14.0666Z",
     "shell.execute_reply": "2025-06-26T22:10:14.066478Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.066461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_walmart_train = pd.read_csv('csv_files/walmart_data/train.csv')\n",
    "df_walmart_test = pd.read_csv('csv_files/walmart_data/test.csv')\n",
    "df_walmart_features = pd.read_csv('csv_files/walmart_data/features.csv')\n",
    "df_walmart_stores = pd.read_csv('csv_files/walmart_data/stores.csv')\n",
    "\n",
    "df_walmart_train[\"Date\"] = pd.to_datetime(df_walmart_train[\"Date\"], errors=\"raise\")\n",
    "df_walmart_test[\"Date\"] = pd.to_datetime(df_walmart_test[\"Date\"], errors=\"raise\")\n",
    "df_walmart_features[\"Date\"] = pd.to_datetime(df_walmart_features[\"Date\"], errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231e068",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.068037Z",
     "iopub.status.idle": "2025-06-26T22:10:14.068349Z",
     "shell.execute_reply": "2025-06-26T22:10:14.068213Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.068195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_walmart_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576dec3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.069739Z",
     "iopub.status.idle": "2025-06-26T22:10:14.070061Z",
     "shell.execute_reply": "2025-06-26T22:10:14.069932Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.069918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_walmart_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3115b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.071027Z",
     "iopub.status.idle": "2025-06-26T22:10:14.071324Z",
     "shell.execute_reply": "2025-06-26T22:10:14.07119Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.071174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_walmart_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6bce62",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4045cd2",
   "metadata": {},
   "source": [
    "#### Sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a568aad2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.072069Z",
     "iopub.status.idle": "2025-06-26T22:10:14.072323Z",
     "shell.execute_reply": "2025-06-26T22:10:14.072214Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.072202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# SP500 Index\n",
    "START_DATE = \"2000-01-01\"\n",
    "END_DATE   = date.today().isoformat()\n",
    "\n",
    "\n",
    "daily = yf.download(\n",
    "    \"^GSPC\",           # S&P 500 index ticker\n",
    "    start=START_DATE,\n",
    "    end=END_DATE,\n",
    "    interval=\"1d\",\n",
    "    auto_adjust=True,\n",
    "    progress=False,\n",
    ")\n",
    "\n",
    "if isinstance(daily.columns, pd.MultiIndex):\n",
    "    lvl0 = daily.columns.get_level_values(0)\n",
    "    if \"Close\" in lvl0:\n",
    "        close_prices = daily.xs(\"Close\", axis=1, level=0)\n",
    "    elif \"Adj Close\" in lvl0:\n",
    "        close_prices = daily.xs(\"Adj Close\", axis=1, level=0)\n",
    "    else:\n",
    "        raise KeyError(\"Neither 'Close' nor 'Adj Close' found in data\")\n",
    "else:\n",
    "    if \"Close\" in daily.columns:\n",
    "        close_prices = daily[\"Close\"]\n",
    "    else:\n",
    "        close_prices = daily[\"Adj Close\"]\n",
    "\n",
    "\n",
    "weekly_mean_close = close_prices.resample(\"W-FRI\").mean()\n",
    "\n",
    "if isinstance(weekly_mean_close, pd.Series):\n",
    "    df_sp500 = weekly_mean_close.to_frame(name=\"SPX_Weekly_Mean_Close\")\n",
    "else:\n",
    "    df_sp500 = weekly_mean_close.copy()\n",
    "    df_sp500.columns = [\"SPX_Weekly_Mean_Close\"]\n",
    "\n",
    "\n",
    "OUTFILE = \"csv_files/idea_csv/sp500_weekly_mean_close.csv\"\n",
    "df_sp500.to_csv(OUTFILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf15d9d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.073094Z",
     "iopub.status.idle": "2025-06-26T22:10:14.073328Z",
     "shell.execute_reply": "2025-06-26T22:10:14.073231Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.073214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_sp500 = pd.read_csv(\"csv_files/idea_csv/sp500_weekly_mean_close.csv\")\n",
    "df_sp500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb5f51d",
   "metadata": {},
   "source": [
    "#### Walmart Stock Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4ab5a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.074871Z",
     "iopub.status.idle": "2025-06-26T22:10:14.075144Z",
     "shell.execute_reply": "2025-06-26T22:10:14.075037Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.075024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START_DATE = \"2000-01-01\"\n",
    "END_DATE   = date.today().isoformat()\n",
    "\n",
    "\n",
    "daily = yf.download(\n",
    "    \"WMT\",           \n",
    "    start=START_DATE,\n",
    "    end=END_DATE,\n",
    "    interval=\"1d\",\n",
    "    auto_adjust=True, \n",
    "    progress=False,\n",
    ")\n",
    "\n",
    "\n",
    "if isinstance(daily.columns, pd.MultiIndex):\n",
    "    lvl0 = daily.columns.get_level_values(0)\n",
    "    if \"Close\" in lvl0:\n",
    "        close_prices = daily.xs(\"Close\", axis=1, level=0)\n",
    "    elif \"Adj Close\" in lvl0:\n",
    "        close_prices = daily.xs(\"Adj Close\", axis=1, level=0)\n",
    "    else:\n",
    "        raise KeyError(\"Neither 'Close' nor 'Adj Close' found in data\")\n",
    "else:\n",
    "    if \"Close\" in daily.columns:\n",
    "        close_prices = daily[\"Close\"]\n",
    "    else:\n",
    "        close_prices = daily[\"Adj Close\"]\n",
    "\n",
    "\n",
    "weekly_mean_close = close_prices.resample(\"W-FRI\").mean()\n",
    "\n",
    "\n",
    "if isinstance(weekly_mean_close, pd.Series):\n",
    "    df_walmart_stock = weekly_mean_close.to_frame(name=\"WMT_Weekly_Mean_Close\")\n",
    "else:\n",
    "    df_walmart_stock = weekly_mean_close.copy()\n",
    "    df_walmart_stock.columns = [\"WMT_Weekly_Mean_Close\"]\n",
    "\n",
    "\n",
    "OUTFILE = \"csv_files/idea_csv/wmt_weekly_mean_close.csv\"\n",
    "df_walmart_stock.to_csv(OUTFILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9a073",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.076035Z",
     "iopub.status.idle": "2025-06-26T22:10:14.07631Z",
     "shell.execute_reply": "2025-06-26T22:10:14.076201Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.076189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_walmart_stock = pd.read_csv(\"csv_files/idea_csv/wmt_weekly_mean_close.csv\")\n",
    "df_walmart_stock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26393546",
   "metadata": {},
   "source": [
    "#### External Logistic companies Walmart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a9bff",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.077189Z",
     "iopub.status.idle": "2025-06-26T22:10:14.077467Z",
     "shell.execute_reply": "2025-06-26T22:10:14.07733Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.077319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "#   - ARCB: ArcBest Corporation (ABF Logistics / ArcBest Freight)\n",
    "#   - AIT: AIT Worldwide Logistics\n",
    "#   - CEVA: CEVA Logistics\n",
    "#   - DPW.DE: Deutsche Post (DHL Freight / DHL Supply Chain) on XETRA\n",
    "#   - FDX: FedEx Corporation (FedEx Freight)\n",
    "#   - SAIA: Saia, Inc. (Saia Motor Freight Line)\n",
    "#   - TFII.TO: TFI International (TForce Freight) on TSX\n",
    "#   - XPO: XPO Logistics, Inc.\n",
    "#   - ODFL: Old Dominion Freight Line, Inc.\n",
    "#   - UPS: United Parcel Service, Inc.\n",
    "#   - JBHT: J.B. Hunt Transport Services, Inc.\n",
    "# Note: Some private carriers (Estes Express, R+L Carriers) are not publicly traded.\n",
    "\n",
    "TICKERS = [\n",
    "    \"ARCB\", \"AIT\", \"CEVA\", \"DPW.DE\", \"FDX\",\n",
    "    \"SAIA\", \"TFII.TO\", \"XPO\", \"ODFL\", \"UPS\", \"JBHT\"\n",
    "]\n",
    "\n",
    "START_DATE = \"2000-01-01\"\n",
    "END_DATE   = date.today().isoformat()\n",
    "\n",
    "daily = yf.download(\n",
    "    TICKERS,\n",
    "    start=START_DATE,\n",
    "    end=END_DATE,\n",
    "    interval=\"1d\",\n",
    "    auto_adjust=True,\n",
    "    group_by=\"ticker\",\n",
    "    threads=True,\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "close = pd.DataFrame()\n",
    "for sym in TICKERS:\n",
    "    try:\n",
    "        series = daily[sym][\"Close\"]\n",
    "        close[sym] = series\n",
    "    except Exception:\n",
    "        print(f\"Skipping {sym!r}: no data available or ticker invalid\")\n",
    "\n",
    "\n",
    "before = close.shape[1]\n",
    "close = close.dropna(axis=1, how=\"all\")\n",
    "after = close.shape[1]\n",
    "print(f\"Dropped {before-after} tickers; {after} tickers remain for analysis\")\n",
    "\n",
    "df_logistics = close.resample(\"W-FRI\").mean().round(4)\n",
    "\n",
    "df_logistics.columns = [f\"{sym}_df_logistics_Close\" for sym in df_logistics.columns]\n",
    "\n",
    "OUTFILE = \"csv_files/idea_csv/logistics_df_logistics_close.csv\"\n",
    "df_logistics.to_csv(OUTFILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1238b66",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.079013Z",
     "iopub.status.idle": "2025-06-26T22:10:14.079359Z",
     "shell.execute_reply": "2025-06-26T22:10:14.079202Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.079187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_logistics = pd.read_csv(\"csv_files/idea_csv/logistics_df_logistics_close.csv\")\n",
    "df_logistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876333ca",
   "metadata": {},
   "source": [
    "#### Official China PMI (Caixin PMI only starts in 2014)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f73f2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.080372Z",
     "iopub.status.idle": "2025-06-26T22:10:14.080708Z",
     "shell.execute_reply": "2025-06-26T22:10:14.080562Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.080545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# All AkShare functions containing \"pmi\"\n",
    "\n",
    "df_official = ak.macro_china_pmi()\n",
    "\n",
    "print(\"Columns in df_official:\", df_official.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78a461",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.081768Z",
     "iopub.status.idle": "2025-06-26T22:10:14.082081Z",
     "shell.execute_reply": "2025-06-26T22:10:14.081922Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.081908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# 1) Fetch the official China PMI data\n",
    "df_official = ak.macro_china_pmi()\n",
    "\n",
    "# 2) Rename Chinese column names to English\n",
    "df_official = df_official.rename(columns={\n",
    "    \"月份\": \"Date\",\n",
    "    \"制造业-指数\": \"Official_Manufacturing_PMI\",\n",
    "    \"制造业-同比增长\": \"Official_Manufacturing_PMI_YoY\",\n",
    "    \"非制造业-指数\": \"Official_Services_PMI\",\n",
    "    \"非制造业-同比增长\": \"Official_Services_PMI_YoY\",\n",
    "})\n",
    "\n",
    "# 3) Clean and parse the 'Date' column (\"YYYY年MM月份\" → \"YYYY-MM\")\n",
    "df_official[\"Date\"] = (\n",
    "    df_official[\"Date\"]\n",
    "      .str.replace(\"年\", \"-\", regex=False)\n",
    "      .str.replace(\"月份\", \"\", regex=False)\n",
    ")\n",
    "df_official[\"Date\"] = pd.to_datetime(df_official[\"Date\"], format=\"%Y-%m\")\n",
    "\n",
    "# 4) Set Date as the index and sort\n",
    "df_official = df_official.set_index(\"Date\").sort_index()\n",
    "\n",
    "# 5) Subset to the period 2009-01-01 through 2014-12-31\n",
    "df_pmi_china = df_official.loc[\"2009-01-01\":\"2014-12-31\"]\n",
    "df_pmi_china.to_csv(\"csv_files/idea_csv/df_pmi_china.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ea0e7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.083746Z",
     "iopub.status.idle": "2025-06-26T22:10:14.084123Z",
     "shell.execute_reply": "2025-06-26T22:10:14.083963Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.083946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_pmi_china = pd.read_csv(\"csv_files/idea_csv/df_pmi_china.csv\")\n",
    "df_pmi_china"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824d1ce",
   "metadata": {},
   "source": [
    "#### PCE USA (Personal Consumption Expenditures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5091fcc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.085593Z",
     "iopub.status.idle": "2025-06-26T22:10:14.08584Z",
     "shell.execute_reply": "2025-06-26T22:10:14.085738Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.085728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START_DATE = \"2000-01-01\"\n",
    "END_DATE   = date.today().isoformat()\n",
    "\n",
    "df_pce = DataReader(\"PCE\", \"fred\", start=START_DATE, end=END_DATE)\n",
    "\n",
    "df_pce.columns = [\"Personal_Consumption_Expenditures\"]\n",
    "\n",
    "# 4) Save and quick sanity-check\n",
    "OUTFILE = \"csv_files/idea_csv/personal_consumption_expenditures.csv\"\n",
    "df_pce.to_csv(OUTFILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5374042",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.08678Z",
     "iopub.status.idle": "2025-06-26T22:10:14.087024Z",
     "shell.execute_reply": "2025-06-26T22:10:14.086922Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.086911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_pce = pd.read_csv(\"csv_files/idea_csv/personal_consumption_expenditures.csv\")\n",
    "df_pce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091ad03",
   "metadata": {},
   "source": [
    "#### Interest Rates USA (Fed Funds Rate & Tbill 3 Months Yield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d04fe0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.087815Z",
     "iopub.status.idle": "2025-06-26T22:10:14.088041Z",
     "shell.execute_reply": "2025-06-26T22:10:14.087944Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.087933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START = \"2001-06-01\"               \n",
    "END   = date.today().isoformat()\n",
    "\n",
    "\n",
    "fed  = DataReader(\"FEDFUNDS\", \"fred\", START, END)\n",
    "tbill = DataReader(\"DGS3MO\",  \"fred\", START, END)\n",
    "\n",
    "df_interest_rates = pd.concat([fed, tbill], axis=1).rename(columns={\n",
    "    \"FEDFUNDS\": \"Fed_Funds_Rate\",\n",
    "    \"DGS3MO\":   \"TBill_3mo_Yield\",\n",
    "})\n",
    "df_interest_rates = df_interest_rates.resample(\"W-FRI\").mean()\n",
    "\n",
    "OUTFILE = \"csv_files/idea_csv/df_interest_rates.csv\"\n",
    "df_interest_rates.to_csv(OUTFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da9b40",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.088934Z",
     "iopub.status.idle": "2025-06-26T22:10:14.089281Z",
     "shell.execute_reply": "2025-06-26T22:10:14.08913Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.089114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_interest_rates = pd.read_csv(\"csv_files/idea_csv/df_interest_rates.csv\")\n",
    "df_interest_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907aa88",
   "metadata": {},
   "source": [
    "#### CCI USA (Consumer Confidence Index) from University of Michigan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eff796",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.091569Z",
     "iopub.status.idle": "2025-06-26T22:10:14.091921Z",
     "shell.execute_reply": "2025-06-26T22:10:14.091761Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.091745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START_DATE = \"2001-06-17\"\n",
    "END_DATE   = date.today().isoformat()\n",
    "\n",
    "df_us_cci = DataReader(\"UMCSENT\", \"fred\", START_DATE, END_DATE)\n",
    "df_us_cci.columns = [\"Consumer_Sentiment_UMich\"]\n",
    "\n",
    "OUTFILE = \"csv_files/idea_csv/consumer_confidence_index.csv\"\n",
    "df_us_cci.to_csv(OUTFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51523471",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.093128Z",
     "iopub.status.idle": "2025-06-26T22:10:14.093452Z",
     "shell.execute_reply": "2025-06-26T22:10:14.093282Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.093266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_cci = pd.read_csv(\"csv_files/idea_csv/consumer_confidence_index.csv\")\n",
    "df_us_cci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939e092",
   "metadata": {},
   "source": [
    "#### U.S.A Advance Retail Sales: Retail Trade and Food Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ba7e2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.094932Z",
     "iopub.status.idle": "2025-06-26T22:10:14.095163Z",
     "shell.execute_reply": "2025-06-26T22:10:14.095067Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.095057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START_DATE = \"2000-01-01\"\n",
    "END_DATE   = date.today().isoformat()\n",
    "\n",
    "# RSAFS = Advance Retail Sales: Retail Trade and Food Services (Millions of Dollars, SA)\n",
    "df_us_retail = DataReader(\"RSAFS\", \"fred\", START_DATE, END_DATE)\n",
    "\n",
    "df_us_retail.columns = [\"Retail_Sales_Retail_and_Food_Services_USA\"]\n",
    "\n",
    "OUTFILE = \"csv_files/idea_csv/usa_retail_sales.csv\"\n",
    "df_us_retail.to_csv(OUTFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ef437",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.095771Z",
     "iopub.status.idle": "2025-06-26T22:10:14.096022Z",
     "shell.execute_reply": "2025-06-26T22:10:14.095907Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.095897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_retail = pd.read_csv(\"csv_files/idea_csv/usa_retail_sales.csv\")\n",
    "df_us_retail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556249e1",
   "metadata": {},
   "source": [
    "#### Exchange Rates (China, Mexico, Canada, India, Vietnam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac1803e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.097201Z",
     "iopub.status.idle": "2025-06-26T22:10:14.097582Z",
     "shell.execute_reply": "2025-06-26T22:10:14.097429Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.097412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START = \"2001-06-17\"                 \n",
    "END   = date.today().isoformat()\n",
    "\n",
    "#    China (CNY per USD): DEXCHUS  \n",
    "#    Mexico (MXN per USD): DEXMXUS  \n",
    "#    Canada (CAD per USD): DEXCAUS  \n",
    "#    India (INR per USD): DEXINUS  \n",
    "cny = DataReader(\"DEXCHUS\", \"fred\", START, END)\n",
    "mxn = DataReader(\"DEXMXUS\", \"fred\", START, END)\n",
    "cad = DataReader(\"DEXCAUS\", \"fred\", START, END)\n",
    "inr = DataReader(\"DEXINUS\", \"fred\", START, END)\n",
    "\n",
    "# Vietnam via yfinance\n",
    "vn_df = yf.download(\n",
    "    \"USDVND=X\",\n",
    "    start=START,\n",
    "    end=END,\n",
    "    progress=False\n",
    ")\n",
    "vn = vn_df[[\"Close\"]].rename(columns={\"Close\": \"VND_per_USD\"})\n",
    "\n",
    "\n",
    "fx = pd.concat([cny, mxn, cad, inr, vn], axis=1).rename(columns={\n",
    "    \"DEXCHUS\": \"CNY_per_USD\",\n",
    "    \"DEXMXUS\": \"MXN_per_USD\",\n",
    "    \"DEXCAUS\": \"CAD_per_USD\",\n",
    "    \"DEXINUS\": \"INR_per_USD\"\n",
    "})\n",
    "\n",
    "df_fx = fx.resample(\"W-FRI\").mean().dropna(how=\"all\").round(4)\n",
    "\n",
    "\n",
    "OUTFILE = \"csv_files/idea_csv/foreign_exchange.csv\"\n",
    "df_fx.to_csv(OUTFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e87636",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.098822Z",
     "iopub.status.idle": "2025-06-26T22:10:14.099158Z",
     "shell.execute_reply": "2025-06-26T22:10:14.099004Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.098989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_fx = pd.read_csv(\"csv_files/idea_csv/foreign_exchange.csv\")\n",
    "df_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b6f4e",
   "metadata": {},
   "source": [
    "#### US External Tax Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f4120",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.100168Z",
     "iopub.status.idle": "2025-06-26T22:10:14.100513Z",
     "shell.execute_reply": "2025-06-26T22:10:14.100336Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.100321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "INDICATOR   = \"TM.TAX.MRCH.WM.AR.ZS\"  # Tariff rate %\n",
    "COUNTRIES   = [\"CN\", \"IN\", \"MX\", \"CA\", \"VN\"]\n",
    "START_YEAR  = 2000\n",
    "END_YEAR    = date.today().year\n",
    "\n",
    "# from World Bank\n",
    "df_us_tariff = wb.download(\n",
    "    indicator=INDICATOR,\n",
    "    country=COUNTRIES,\n",
    "    start=START_YEAR,\n",
    "    end=END_YEAR\n",
    ")\n",
    "\n",
    "df_us_tariff = df_us_tariff.reset_index().pivot(index=\"year\", columns=\"country\", values=INDICATOR)\n",
    "\n",
    "df_us_tariff = df_us_tariff.rename(columns={\n",
    "    \"CN\": \"China_Applied_Tariff_%\", \n",
    "    \"IN\": \"India_Applied_Tariff_%\", \n",
    "    \"MX\": \"Mexico_Applied_Tariff_%\", \n",
    "    \"CA\": \"Canada_Applied_Tariff_%\", \n",
    "    \"VN\": \"Vietnam_Applied_Tariff_%\"\n",
    "})\n",
    "\n",
    "\n",
    "OUTFILE = \"csv_files/idea_csv/external_tax_rates.csv\"\n",
    "df_us_tariff.to_csv(OUTFILE, index_label=\"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffeee62",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.101818Z",
     "iopub.status.idle": "2025-06-26T22:10:14.102144Z",
     "shell.execute_reply": "2025-06-26T22:10:14.101994Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.101979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_tariff = pd.read_csv(\"csv_files/idea_csv/external_tax_rates.csv\")\n",
    "df_us_tariff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8530b",
   "metadata": {},
   "source": [
    "#### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fb08e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.103124Z",
     "iopub.status.idle": "2025-06-26T22:10:14.103504Z",
     "shell.execute_reply": "2025-06-26T22:10:14.103318Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.103303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "def nth_weekday(year, month, weekday, n):\n",
    "    \"\"\"\n",
    "    Return the date of the nth occurrence of the given weekday\n",
    "    in the specified month and year.\n",
    "    weekday: Monday=0, Sunday=6\n",
    "    \"\"\"\n",
    "    d = date(year, month, 1)\n",
    "    count = 0\n",
    "    while True:\n",
    "        if d.weekday() == weekday:\n",
    "            count += 1\n",
    "            if count == n:\n",
    "                return d\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "# Super Bowl dates per your list\n",
    "super_bowl_day = {2010: 12, 2011: 11, 2012: 10, 2013: 8}\n",
    "\n",
    "years = range(2010, 2014)\n",
    "records = []\n",
    "\n",
    "for year in years:\n",
    "    # Fixed‐date holidays\n",
    "    records += [\n",
    "        {\"Date\": pd.Timestamp(date(year, 2, 14)),  \"Holiday\": \"Valentine's Day\"},\n",
    "        {\"Date\": pd.Timestamp(date(year, 3, 17)),  \"Holiday\": \"St. Patrick's Day\"},\n",
    "        {\"Date\": pd.Timestamp(date(year, 7, 4)),   \"Holiday\": \"Independence Day\"},\n",
    "        {\"Date\": pd.Timestamp(date(year,10,31)),   \"Holiday\": \"Halloween\"},\n",
    "        {\"Date\": pd.Timestamp(date(year,12,24)),   \"Holiday\": \"Christmas Eve\"},\n",
    "        {\"Date\": pd.Timestamp(date(year,12,25)),   \"Holiday\": \"Christmas Day\"},\n",
    "        {\"Date\": pd.Timestamp(date(year,12,31)),   \"Holiday\": \"New Year's Eve\"},\n",
    "        # Super Bowl\n",
    "        {\"Date\": pd.Timestamp(date(year, 2, super_bowl_day[year])), \"Holiday\": \"Super Bowl\"},\n",
    "    ]\n",
    "    \n",
    "    # Presidents' Day: 3rd Monday in February\n",
    "    pd_day = nth_weekday(year, 2, 0, 3)\n",
    "    records.append({\"Date\": pd.Timestamp(pd_day), \"Holiday\": \"Presidents' Day\"})\n",
    "    \n",
    "    # Mother's Day: 2nd Sunday in May\n",
    "    md = nth_weekday(year, 5, 6, 2)\n",
    "    records.append({\"Date\": pd.Timestamp(md), \"Holiday\": \"Mother's Day\"})\n",
    "    \n",
    "    # Father's Day: 3rd Sunday in June\n",
    "    fd = nth_weekday(year, 6, 6, 3)\n",
    "    records.append({\"Date\": pd.Timestamp(fd), \"Holiday\": \"Father's Day\"})\n",
    "    \n",
    "    # Memorial Day: last Monday in May\n",
    "    d_mem = date(year, 5, 31)\n",
    "    while d_mem.weekday() != 0:  # 0 = Monday\n",
    "        d_mem -= timedelta(days=1)\n",
    "    records.append({\"Date\": pd.Timestamp(d_mem), \"Holiday\": \"Memorial Day\"})\n",
    "    \n",
    "    # Labor Day: 1st Monday in September\n",
    "    ld = nth_weekday(year, 9, 0, 1)\n",
    "    records.append({\"Date\": pd.Timestamp(ld), \"Holiday\": \"Labor Day\"})\n",
    "    \n",
    "    # Good Friday & Easter\n",
    "    eas = easter(year)\n",
    "    gf = eas - timedelta(days=2)\n",
    "    records.append({\"Date\": pd.Timestamp(gf), \"Holiday\": \"Good Friday\"})\n",
    "    records.append({\"Date\": pd.Timestamp(eas), \"Holiday\": \"Easter Sunday\"})\n",
    "    \n",
    "    # Daylight Saving Time\n",
    "    dst_start = nth_weekday(year, 3, 6, 2)   # 2nd Sunday in March\n",
    "    dst_end   = nth_weekday(year,11, 6, 1)   # 1st Sunday in November\n",
    "    records.append({\"Date\": pd.Timestamp(dst_start), \"Holiday\": \"DST Start\"})\n",
    "    records.append({\"Date\": pd.Timestamp(dst_end),   \"Holiday\": \"DST End\"})\n",
    "    \n",
    "    # Thanksgiving & related\n",
    "    th = nth_weekday(year, 11, 3, 4)  # 4th Thu in Nov\n",
    "    records.append({\"Date\": pd.Timestamp(th), \"Holiday\": \"Thanksgiving\"})\n",
    "    records.append({\"Date\": pd.Timestamp(th + timedelta(days=1)), \"Holiday\": \"Black Friday\"})\n",
    "    records.append({\"Date\": pd.Timestamp(th + timedelta(days=2)), \"Holiday\": \"Small Business Saturday\"})\n",
    "    records.append({\"Date\": pd.Timestamp(th + timedelta(days=4)), \"Holiday\": \"Cyber Monday\"})\n",
    "    # Super Saturday: last Saturday before Christmas Eve\n",
    "    d2 = date(year, 12, 24) - timedelta(days=1)\n",
    "    while d2.weekday() != 5: d2 -= timedelta(days=1)\n",
    "    records.append({\"Date\": pd.Timestamp(d2), \"Holiday\": \"Super Saturday\"})\n",
    "    \n",
    "    # Green Monday: 2nd Monday in December\n",
    "    gm = nth_weekday(year, 12, 0, 2)\n",
    "    records.append({\"Date\": pd.Timestamp(gm), \"Holiday\": \"Green Monday\"})\n",
    "    \n",
    "    # 2012‐only events\n",
    "    if year == 2012:\n",
    "        records.append({\"Date\": pd.Timestamp(date(2012, 7, 27)), \"Holiday\": \"Olympics Opening\"})\n",
    "        records.append({\"Date\": pd.Timestamp(date(2012,11, 6)), \"Holiday\": \"Presidential Election\"})\n",
    "\n",
    "# Build the DataFrame\n",
    "df_us_holidays = (\n",
    "    pd.DataFrame(records)\n",
    "      .drop_duplicates(subset=\"Date\")      # in case any collide\n",
    "      .sort_values(\"Date\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Save or merge as needed\n",
    "df_us_holidays.to_csv(\"csv_files/idea_csv/df_us_holidays.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611fc71",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.104262Z",
     "iopub.status.idle": "2025-06-26T22:10:14.104611Z",
     "shell.execute_reply": "2025-06-26T22:10:14.104463Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.104438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_holidays = pd.read_csv(\"csv_files/idea_csv/df_us_holidays.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10bbba2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.108715Z",
     "iopub.status.idle": "2025-06-26T22:10:14.108989Z",
     "shell.execute_reply": "2025-06-26T22:10:14.108884Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.108873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "#  parâmetros\n",
    "ramp_up_days   = 42\n",
    "ramp_down_days = 14\n",
    "window_days    = ramp_up_days + ramp_down_days    \n",
    "\n",
    "# dados de entrada \n",
    "df_us_holidays['Date'] = pd.to_datetime(df_us_holidays['Date']).dt.normalize()\n",
    "holidays = df_us_holidays['Date'].tolist()\n",
    "\n",
    "df_holiday_impact = df_walmart_train[['Date']].copy()\n",
    "df_holiday_impact['Date'] = pd.to_datetime(df_holiday_impact['Date']).dt.normalize()\n",
    "df_holiday_impact['HolidayImpact'] = 0.0\n",
    "\n",
    "#  função vectorizada para um único feriado\n",
    "def add_one_holiday(peak_date):\n",
    "    start = peak_date - timedelta(days=ramp_up_days)\n",
    "    end   = peak_date + timedelta(days=ramp_down_days)\n",
    "\n",
    "    mask = (df_holiday_impact['Date'] >= start) & (df_holiday_impact['Date'] <= end)\n",
    "    if not mask.any():   \n",
    "        return\n",
    "\n",
    "    diff = (df_holiday_impact.loc[mask, 'Date'] - peak_date).dt.days.to_numpy()\n",
    "\n",
    "    # parte esquerda (ramp-up: diff ∈ [-14, 0])\n",
    "    up_mask   = diff <= 0\n",
    "    x_up      = (ramp_up_days + diff[up_mask]) / ramp_up_days          # 0→1\n",
    "    weights   = np.zeros_like(diff, dtype=float)\n",
    "    weights[up_mask] = 0.5 * (1 - np.cos(np.pi * x_up))\n",
    "\n",
    "    # parte direita (ramp-down: diff ∈ (0, 42])\n",
    "    down_mask = diff > 0\n",
    "    x_down    = diff[down_mask] / ramp_down_days                       # 0→1\n",
    "    weights[down_mask] = 0.5 * (1 + np.cos(np.pi * x_down))\n",
    "\n",
    "    # soma ao total\n",
    "    df_holiday_impact.loc[mask, 'HolidayImpact'] += weights\n",
    "\n",
    "#  corre todos os feriados \n",
    "for hday in holidays:\n",
    "    add_one_holiday(hday)\n",
    "\n",
    "df_holiday_impact['HolidayImpact'] = df_holiday_impact['HolidayImpact']\n",
    "\n",
    "df_holiday_impact.to_csv('csv_files/idea_csv/df_holiday_impact.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a722ea",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.109953Z",
     "iopub.status.idle": "2025-06-26T22:10:14.110571Z",
     "shell.execute_reply": "2025-06-26T22:10:14.11038Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.110357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_holiday_impact = pd.read_csv('csv_files/idea_csv/df_holiday_impact.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0ce63",
   "metadata": {},
   "source": [
    "#### Tax Return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec547b96",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf355ee",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.113541Z",
     "iopub.status.idle": "2025-06-26T22:10:14.113878Z",
     "shell.execute_reply": "2025-06-26T22:10:14.113746Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.113729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_tax_return_train = df_walmart_train[[\"Date\"]].copy()\n",
    "df_tax_return_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428b117",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.115529Z",
     "iopub.status.idle": "2025-06-26T22:10:14.115854Z",
     "shell.execute_reply": "2025-06-26T22:10:14.115743Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.115727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "ramp_up_days   = 14 \n",
    "ramp_down_days = 42  \n",
    "\n",
    "def filing_deadline(year):\n",
    "    \"\"\"\n",
    "    IRS filing deadline April 15, bumped to Monday if on a weekend.\n",
    "    \"\"\"\n",
    "    d = date(year, 4, 15)\n",
    "    while d.weekday() >= 5:  # \n",
    "        d += timedelta(days=1)\n",
    "    return pd.Timestamp(d)\n",
    "\n",
    "\n",
    "def tax_return_weight(ts, ramp_up=ramp_up_days, ramp_down=ramp_down_days):\n",
    "    \"\"\"\n",
    "    Smooth raised-cosine weight:\n",
    "      • 0 before (deadline - ramp_up)\n",
    "      • ramps up from 0→1 over `ramp_up` days\n",
    "      • ramps down from 1→0 over `ramp_down` days\n",
    "      • 0 after (deadline + ramp_down)\n",
    "    \"\"\"\n",
    "    ts   = pd.Timestamp(ts).normalize()\n",
    "    peak = filing_deadline(ts.year)\n",
    "    start = peak - timedelta(days=ramp_up)\n",
    "    end   = peak + timedelta(days=ramp_down)\n",
    "\n",
    "    if ts < start or ts > end:\n",
    "        return 0.0\n",
    "\n",
    "    if ts <= peak:\n",
    "        # fraction of ramp-up completed [0…1]\n",
    "        x = (ts - start).days / ramp_up\n",
    "        # raised‐cosine from 0→1\n",
    "        return 0.5 * (1 - np.cos(np.pi * x))\n",
    "    else:\n",
    "        # fraction of ramp-down completed [0…1]\n",
    "        x = (ts - peak).days / ramp_down\n",
    "        # raised‐cosine from 1→0\n",
    "        return 0.5 * (1 + np.cos(np.pi * x))\n",
    "\n",
    "\n",
    "df_tax_return_train = df_walmart_train[['Date']].copy()\n",
    "df_tax_return_train['TaxReturnImpact'] = (\n",
    "    df_tax_return_train['Date']\n",
    "      .dt.normalize()\n",
    "      .map(tax_return_weight)\n",
    ")\n",
    "\n",
    "df_tax_return_train.to_csv('csv_files/idea_csv/df_tax_return_train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fafe35",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.116798Z",
     "iopub.status.idle": "2025-06-26T22:10:14.117193Z",
     "shell.execute_reply": "2025-06-26T22:10:14.117054Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.117036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_unique = df_tax_return_train.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd1022",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.120597Z",
     "iopub.status.idle": "2025-06-26T22:10:14.120893Z",
     "shell.execute_reply": "2025-06-26T22:10:14.120782Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.120769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_tax_return_train = pd.read_csv('csv_files/idea_csv/df_tax_return_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4f2ca",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd2010",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.122091Z",
     "iopub.status.idle": "2025-06-26T22:10:14.122349Z",
     "shell.execute_reply": "2025-06-26T22:10:14.12223Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.122218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "ramp_up_days   = 14 \n",
    "ramp_down_days = 42  \n",
    "\n",
    "def filing_deadline(year):\n",
    "    \"\"\"\n",
    "    IRS filing deadline April 15, bumped to Monday if on a weekend.\n",
    "    \"\"\"\n",
    "    d = date(year, 4, 15)\n",
    "    while d.weekday() >= 5:\n",
    "        d += timedelta(days=1)\n",
    "    return pd.Timestamp(d)\n",
    "\n",
    "# ─── SMOOTH WEIGHT FUNCTION ───────────────────────────────\n",
    "def tax_return_weight(ts, ramp_up=ramp_up_days, ramp_down=ramp_down_days):\n",
    "    \"\"\"\n",
    "    Smooth raised-cosine weight:\n",
    "      • 0 before (deadline - ramp_up)\n",
    "      • ramps up from 0→1 over `ramp_up` days\n",
    "      • ramps down from 1→0 over `ramp_down` days\n",
    "      • 0 after (deadline + ramp_down)\n",
    "    \"\"\"\n",
    "    ts   = pd.Timestamp(ts).normalize()\n",
    "    peak = filing_deadline(ts.year)\n",
    "    start = peak - timedelta(days=ramp_up)\n",
    "    end   = peak + timedelta(days=ramp_down)\n",
    "\n",
    "    if ts < start or ts > end:\n",
    "        return 0.0\n",
    "\n",
    "    if ts <= peak:\n",
    "        # fraction of ramp-up completed [0…1]\n",
    "        x = (ts - start).days / ramp_up\n",
    "        # raised‐cosine from 0→1\n",
    "        return 0.5 * (1 - np.cos(np.pi * x))\n",
    "    else:\n",
    "        # fraction of ramp-down completed [0…1]\n",
    "        x = (ts - peak).days / ramp_down\n",
    "        # raised‐cosine from 1→0\n",
    "        return 0.5 * (1 + np.cos(np.pi * x))\n",
    "\n",
    "df_tax_return_test = df_walmart_test[['Date']].copy()\n",
    "df_tax_return_test['TaxReturnImpact'] = (\n",
    "    df_tax_return_test['Date']\n",
    "      .dt.normalize()\n",
    "      .map(tax_return_weight)\n",
    ")\n",
    "\n",
    "df_tax_return_test.to_csv('csv_files/idea_csv/df_tax_return_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2931b9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.123652Z",
     "iopub.status.idle": "2025-06-26T22:10:14.123994Z",
     "shell.execute_reply": "2025-06-26T22:10:14.123846Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.123831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_tax_return_test = pd.read_csv('csv_files/idea_csv/df_tax_return_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bedab",
   "metadata": {},
   "source": [
    "#### Stores Types & Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cccb9d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.127707Z",
     "iopub.status.idle": "2025-06-26T22:10:14.129062Z",
     "shell.execute_reply": "2025-06-26T22:10:14.128871Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.128848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_walmart_train['Store']  = df_walmart_train['Store'].astype(str)\n",
    "df_walmart_stores['Store'] = df_walmart_stores['Store'].astype(str)\n",
    "\n",
    "# Merge the store metadata into your training DataFrame\n",
    "df_store_types_sizes = df_walmart_train.merge(\n",
    "    df_walmart_stores,     \n",
    "    on='Store',             \n",
    "    how='left',       \n",
    "    validate='many_to_one'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797265fe",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.130112Z",
     "iopub.status.idle": "2025-06-26T22:10:14.130417Z",
     "shell.execute_reply": "2025-06-26T22:10:14.130288Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.130265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_store_types_sizes = df_store_types_sizes.loc[:, [\"Store\",\"Type\",\"Size\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef73b4",
   "metadata": {},
   "source": [
    "#### Oil Price The U.S. domestic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e31d2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.133836Z",
     "iopub.status.idle": "2025-06-26T22:10:14.134165Z",
     "shell.execute_reply": "2025-06-26T22:10:14.134038Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.134022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "start = \"2010-02-05\"\n",
    "end   = date.today().isoformat()\n",
    " \n",
    "df_us_oil_price = DataReader(\"DCOILWTICO\", \"fred\", start, end)\n",
    "\n",
    "wti_weekly = df_us_oil_price.resample(\"W-FRI\").mean().rename(\n",
    "    columns={\"DCOILWTICO\":\"WTI_Weekly_Mean_Price\"}\n",
    ")\n",
    "df_us_oil_price.to_csv('csv_files/idea_csv/df_us_oil_price.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c47a64b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.135358Z",
     "iopub.status.idle": "2025-06-26T22:10:14.135715Z",
     "shell.execute_reply": "2025-06-26T22:10:14.135567Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.135551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_oil_price = pd.read_csv('csv_files/idea_csv/df_us_oil_price.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f1794",
   "metadata": {},
   "source": [
    "#### U.S. ISM Manufacturing PMI & ISM Services PMI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ba176",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.139348Z",
     "iopub.status.idle": "2025-06-26T22:10:14.139698Z",
     "shell.execute_reply": "2025-06-26T22:10:14.139582Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.139569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Get columns names\n",
    "df_man = ak.macro_usa_ism_pmi()\n",
    "\n",
    "print(\"Columns in df_man:\", df_man.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad5ebe",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.140809Z",
     "iopub.status.idle": "2025-06-26T22:10:14.141103Z",
     "shell.execute_reply": "2025-06-26T22:10:14.140995Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.14098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START, END = \"2009-01-01\", \"2014-12-31\"\n",
    "\n",
    "df_man = ak.macro_usa_ism_pmi()\n",
    "\n",
    "df_man = df_man.rename(columns={\n",
    "    \"日期\": \"Date\",\n",
    "    \"今值\": \"ISM_Manufacturing_PMI\"\n",
    "})\n",
    "df_man[\"Date\"] = pd.to_datetime(df_man[\"Date\"], format=\"%Y-%m\")\n",
    "df_man = (\n",
    "    df_man.set_index(\"Date\")[[\"ISM_Manufacturing_PMI\"]]\n",
    "    .sort_index()\n",
    "    .loc[START:END]\n",
    ")\n",
    "\n",
    "df_svc = ak.macro_usa_ism_non_pmi()\n",
    "\n",
    "df_svc = df_svc.rename(columns={\n",
    "    \"日期\": \"Date\",\n",
    "    \"今值\": \"ISM_Services_PMI\"\n",
    "})\n",
    "df_svc[\"Date\"] = pd.to_datetime(df_svc[\"Date\"], format=\"%Y-%m\")\n",
    "df_svc = (\n",
    "    df_svc.set_index(\"Date\")[[\"ISM_Services_PMI\"]]\n",
    "    .sort_index()\n",
    "    .loc[START:END]\n",
    ")\n",
    "\n",
    "df_us_ism = df_man.join(df_svc, how=\"outer\")\n",
    "\n",
    "df_us_ism.to_csv('csv_files/idea_csv/df_us_ism.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf123d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.142108Z",
     "iopub.status.idle": "2025-06-26T22:10:14.142513Z",
     "shell.execute_reply": "2025-06-26T22:10:14.142334Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.142316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_ism = pd.read_csv('csv_files/idea_csv/df_us_ism.csv')\n",
    "df_us_ism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e29c85",
   "metadata": {},
   "source": [
    "#### US CPI Food & Beverages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43223c8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.143891Z",
     "iopub.status.idle": "2025-06-26T22:10:14.144194Z",
     "shell.execute_reply": "2025-06-26T22:10:14.144084Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.144073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START, END = \"2009-01-01\", \"2014-12-31\"\n",
    "\n",
    "df_us_cpi_food = DataReader(\"CPIFABSL\", \"fred\", START, END)\n",
    "\n",
    "df_us_cpi_food.rename(columns={\"CPIFABSL\": \"CPI_Food_Beverages\"}, inplace=True)\n",
    "\n",
    "df_us_cpi_food = (\n",
    "    df_us_cpi_food[\"CPI_Food_Beverages\"]\n",
    "    .resample(\"W-FRI\")\n",
    "    .ffill()\n",
    "    .to_frame()\n",
    ")\n",
    "df_us_cpi_food.to_csv('csv_files/idea_csv/df_us_cpi_food.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574fba58",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.145273Z",
     "iopub.status.idle": "2025-06-26T22:10:14.145752Z",
     "shell.execute_reply": "2025-06-26T22:10:14.14544Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.145424Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_cpi_food = pd.read_csv('csv_files/idea_csv/df_us_cpi_food.csv')\n",
    "df_us_cpi_food"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76d45a",
   "metadata": {},
   "source": [
    "#### US CPI Shelter (Housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d7eb2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.147377Z",
     "iopub.status.idle": "2025-06-26T22:10:14.147693Z",
     "shell.execute_reply": "2025-06-26T22:10:14.147576Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.14756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START, END = \"2009-01-01\", \"2014-12-31\"\n",
    "\n",
    "df_us_cpi_shelter = DataReader(\"CUSR0000SAH1\", \"fred\", START, END)\n",
    "\n",
    "df_us_cpi_shelter.rename(columns={\"CUSR0000SAH1\": \"CPI_Shelter\"}, inplace=True)\n",
    "\n",
    "df_us_cpi_shelter = (\n",
    "    df_us_cpi_shelter[\"CPI_Shelter\"]\n",
    "      .resample(\"W-FRI\")\n",
    "      .ffill()            \n",
    "      .to_frame()       \n",
    ")\n",
    "\n",
    "df_us_cpi_shelter.to_csv('csv_files/idea_csv/df_us_cpi_shelter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae792c64",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.148695Z",
     "iopub.status.idle": "2025-06-26T22:10:14.148994Z",
     "shell.execute_reply": "2025-06-26T22:10:14.14884Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.148826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_cpi_shelter = pd.read_csv('csv_files/idea_csv/df_us_cpi_shelter.csv')\n",
    "df_us_cpi_shelter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84e0424",
   "metadata": {},
   "source": [
    "#### US CPI Medical Care\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddcd95",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.150507Z",
     "iopub.status.idle": "2025-06-26T22:10:14.150816Z",
     "shell.execute_reply": "2025-06-26T22:10:14.150678Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.150662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START, END = \"2009-01-01\", \"2014-12-31\"\n",
    "\n",
    "df_us_cpi_med = DataReader(\"CPIMEDSL\", \"fred\", START, END)\n",
    "\n",
    "df_us_cpi_med.rename(columns={\"CPIMEDSL\": \"CPI_Medical_Care\"}, inplace=True)\n",
    "\n",
    "df_us_cpi_med = (\n",
    "    df_us_cpi_med[\"CPI_Medical_Care\"]\n",
    "      .resample(\"W-FRI\")   # calendar‐weeks ending Fridays\n",
    "      .ffill()             # carry each month’s CPI forward until the next release\n",
    "      .to_frame()\n",
    ")\n",
    "\n",
    "df_us_cpi_med.to_csv('csv_files/idea_csv/df_us_cpi_med.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a5cbec",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.152002Z",
     "iopub.status.idle": "2025-06-26T22:10:14.152294Z",
     "shell.execute_reply": "2025-06-26T22:10:14.152168Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.152154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_cpi_med = pd.read_csv('csv_files/idea_csv/df_us_cpi_med.csv')\n",
    "df_us_cpi_med"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3d247",
   "metadata": {},
   "source": [
    "#### US CPI Transportation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d82656",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.153339Z",
     "iopub.status.idle": "2025-06-26T22:10:14.153906Z",
     "shell.execute_reply": "2025-06-26T22:10:14.153755Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.153737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START, END = \"2009-01-01\", \"2014-12-31\"\n",
    "\n",
    "df_us_cpi_trans = DataReader(\"CPITRNSL\", \"fred\", START, END)\n",
    "\n",
    "df_us_cpi_trans.rename(columns={\"CPITRNSL\": \"CPI_Transportation\"}, inplace=True)\n",
    "\n",
    "df_us_cpi_trans = (\n",
    "    df_us_cpi_trans[\"CPI_Transportation\"]\n",
    "      .resample(\"W-FRI\")\n",
    "      .ffill()\n",
    "      .to_frame()\n",
    ")\n",
    "\n",
    "df_us_cpi_trans.to_csv('csv_files/idea_csv/df_us_cpi_trans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095ed62",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.155182Z",
     "iopub.status.idle": "2025-06-26T22:10:14.155519Z",
     "shell.execute_reply": "2025-06-26T22:10:14.155357Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.155341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_cpi_trans = pd.read_csv('csv_files/idea_csv/df_us_cpi_trans.csv')\n",
    "df_us_cpi_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66507566",
   "metadata": {},
   "source": [
    "#### PCE: US Healthcare Services\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d3784",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.156493Z",
     "iopub.status.idle": "2025-06-26T22:10:14.156764Z",
     "shell.execute_reply": "2025-06-26T22:10:14.156652Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.156641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START, END = \"2009-01-01\", \"2014-12-31\"\n",
    "\n",
    "df_us_pce_health = DataReader(\"DHLCRC1Q027SBEA\", \"fred\", START, END)\n",
    "\n",
    "df_us_pce_health.rename(columns={\"DHLCRC1Q027SBEA\": \"PCE_Healthcare_Services\"}, inplace=True)\n",
    "\n",
    "df_us_pce_health = (\n",
    "    df_us_pce_health[\"PCE_Healthcare_Services\"]\n",
    "      .resample(\"W-FRI\")   \n",
    "      .ffill()      \n",
    "      .to_frame()\n",
    ")\n",
    "\n",
    "\n",
    "df_us_pce_health.to_csv('csv_files/idea_csv/df_us_pce_health.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7751d199",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.157901Z",
     "iopub.status.idle": "2025-06-26T22:10:14.158176Z",
     "shell.execute_reply": "2025-06-26T22:10:14.158042Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.158032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_pce_health = pd.read_csv('csv_files/idea_csv/df_us_pce_health.csv')\n",
    "df_us_pce_health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d4fd13",
   "metadata": {},
   "source": [
    "#### US ICSA (Jobless Claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601cfc27",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.159149Z",
     "iopub.status.idle": "2025-06-26T22:10:14.15949Z",
     "shell.execute_reply": "2025-06-26T22:10:14.159335Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.15932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START, END = \"2009-01-01\", \"2014-12-31\"\n",
    "\n",
    "\n",
    "df_us_icsa_jobless = DataReader(\"ICSA\", \"fred\", START, END)\n",
    "\n",
    "df_us_icsa_jobless.rename(columns={\"ICSA\": \"Weekly_Initial_Jobless_Claims\"}, inplace=True)\n",
    "\n",
    "df_us_icsa_jobless = (\n",
    "    df_us_icsa_jobless[\"Weekly_Initial_Jobless_Claims\"]\n",
    "      .resample(\"W-FRI\")  \n",
    "      .ffill()           \n",
    "      .to_frame()\n",
    ")\n",
    "\n",
    "df_us_icsa_jobless.to_csv('csv_files/idea_csv/df_us_icsa_jobless.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3cc0dc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.160335Z",
     "iopub.status.idle": "2025-06-26T22:10:14.160639Z",
     "shell.execute_reply": "2025-06-26T22:10:14.160531Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.160516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_icsa_jobless = pd.read_csv('csv_files/idea_csv/df_us_icsa_jobless.csv')\n",
    "df_us_icsa_jobless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaab7ab",
   "metadata": {},
   "source": [
    "#### US Rail , Freight & Carloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18106c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.161207Z",
     "iopub.status.idle": "2025-06-26T22:10:14.161469Z",
     "shell.execute_reply": "2025-06-26T22:10:14.161336Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.161326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START, END = \"2009-01-01\", \"2014-12-31\"\n",
    "\n",
    "rail = DataReader(\"RAILFRTCARLOADS\", \"fred\", START, END)\n",
    "rail.rename(columns={\"RAILFRTCARLOADS\": \"Rail_Freight_Carloads\"}, inplace=True)\n",
    "\n",
    "rail_weekly = (\n",
    "    rail[\"Rail_Freight_Carloads\"]\n",
    "        .resample(\"W-FRI\")\n",
    "        .ffill()\n",
    "        .to_frame()\n",
    ")\n",
    "\n",
    "truck = DataReader(\"TRUCKD11\", \"fred\", START, END)\n",
    "truck.rename(columns={\"TRUCKD11\": \"Truck_Tonnage_Index\"}, inplace=True)\n",
    "\n",
    "truck_weekly = (\n",
    "    truck[\"Truck_Tonnage_Index\"]\n",
    "         .resample(\"W-FRI\")\n",
    "         .ffill()\n",
    "         .to_frame()\n",
    ")\n",
    "\n",
    "df_us_rail_freight_carloads = rail_weekly.join(truck_weekly, how=\"outer\")\n",
    "\n",
    "df_us_rail_freight_carloads.to_csv('csv_files/idea_csv/df_us_rail_freight_carloads.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db603ad4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.162373Z",
     "iopub.status.idle": "2025-06-26T22:10:14.162932Z",
     "shell.execute_reply": "2025-06-26T22:10:14.162747Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.162724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_rail_freight_carloads = pd.read_csv('csv_files/idea_csv/df_us_rail_freight_carloads.csv')\n",
    "df_us_rail_freight_carloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0987caa",
   "metadata": {},
   "source": [
    "#### EU PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6433ffd1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.164878Z",
     "iopub.status.idle": "2025-06-26T22:10:14.165142Z",
     "shell.execute_reply": "2025-06-26T22:10:14.165034Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.16502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_eu = ak.macro_euro_manufacturing_pmi()\n",
    "\n",
    "print(df_eu.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92723d01",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.166306Z",
     "iopub.status.idle": "2025-06-26T22:10:14.166606Z",
     "shell.execute_reply": "2025-06-26T22:10:14.166495Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.166485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "START, END = \"2009-01-01\", \"2014-12-31\"\n",
    "\n",
    "df_eu_pmi = ak.macro_euro_manufacturing_pmi()\n",
    "\n",
    "df_eu_pmi = df_eu_pmi.rename(columns={\n",
    "    \"日期\": \"Date\",\n",
    "    \"今值\":  \"Euro_Manufacturing_PMI\"\n",
    "})\n",
    "df_eu_pmi[\"Date\"] = pd.to_datetime(df_eu_pmi[\"Date\"], format=\"%Y-%m\")\n",
    "\n",
    "df_eu_pmi = (\n",
    "    df_eu_pmi.set_index(\"Date\")[[\"Euro_Manufacturing_PMI\"]]\n",
    "              .sort_index()\n",
    "              .loc[START:END]\n",
    ")\n",
    "\n",
    "df_eu_pmi = (\n",
    "    df_eu_pmi[\"Euro_Manufacturing_PMI\"]\n",
    "      .resample(\"W-FRI\")\n",
    "      .ffill()\n",
    "      .to_frame()\n",
    ")\n",
    "df_eu_pmi.to_csv('csv_files/idea_csv/df_eu_pmi.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0842da7a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.167323Z",
     "iopub.status.idle": "2025-06-26T22:10:14.167601Z",
     "shell.execute_reply": "2025-06-26T22:10:14.167473Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.167463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_eu_pmi = pd.read_csv('csv_files/idea_csv/df_eu_pmi.csv')\n",
    "df_eu_pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f0bab",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ee0d3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.169066Z",
     "iopub.status.idle": "2025-06-26T22:10:14.169519Z",
     "shell.execute_reply": "2025-06-26T22:10:14.169323Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.169232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_walmart_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750503ef",
   "metadata": {},
   "source": [
    "### Data Cleaning - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654597fd",
   "metadata": {},
   "source": [
    "#### SP 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a154839",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.171312Z",
     "iopub.status.idle": "2025-06-26T22:10:14.171636Z",
     "shell.execute_reply": "2025-06-26T22:10:14.171513Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.171499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_sp500['Date'] = pd.to_datetime(df_sp500[\"Date\"], errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf2de3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.173728Z",
     "iopub.status.idle": "2025-06-26T22:10:14.174046Z",
     "shell.execute_reply": "2025-06-26T22:10:14.173898Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.173884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_sp500.rename(columns={\n",
    "    'SPX_Weekly_Mean_Close': 'SP500_Weekly_Mean_Close'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a5345",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.175969Z",
     "iopub.status.idle": "2025-06-26T22:10:14.176314Z",
     "shell.execute_reply": "2025-06-26T22:10:14.176145Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.176129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_merged = df_wm_train.merge(\n",
    "    df_sp500,\n",
    "    on=\"Date\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f21dc64",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.18016Z",
     "iopub.status.idle": "2025-06-26T22:10:14.180546Z",
     "shell.execute_reply": "2025-06-26T22:10:14.180407Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.180368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6d60b",
   "metadata": {},
   "source": [
    "#### Walmart Stock Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f51118",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.183541Z",
     "iopub.status.idle": "2025-06-26T22:10:14.183964Z",
     "shell.execute_reply": "2025-06-26T22:10:14.183801Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.183783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_walmart_stock['Date'] = pd.to_datetime(df_walmart_stock[\"Date\"], errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b05d6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.184664Z",
     "iopub.status.idle": "2025-06-26T22:10:14.185105Z",
     "shell.execute_reply": "2025-06-26T22:10:14.184841Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.184826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_merged = df_wm_train.merge(\n",
    "    df_walmart_stock,\n",
    "    on=\"Date\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b85d47",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.186207Z",
     "iopub.status.idle": "2025-06-26T22:10:14.186538Z",
     "shell.execute_reply": "2025-06-26T22:10:14.186409Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.186374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ce570",
   "metadata": {},
   "source": [
    "#### External Logistic companies Walmart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2840c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.191532Z",
     "iopub.status.idle": "2025-06-26T22:10:14.19187Z",
     "shell.execute_reply": "2025-06-26T22:10:14.191716Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.1917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_logistics['Date'] = pd.to_datetime(df_logistics[\"Date\"], errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79c62f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.193019Z",
     "iopub.status.idle": "2025-06-26T22:10:14.193244Z",
     "shell.execute_reply": "2025-06-26T22:10:14.193146Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.193136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_merged = df_wm_train.merge(\n",
    "    df_logistics,\n",
    "    on=\"Date\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a70cb4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.194008Z",
     "iopub.status.idle": "2025-06-26T22:10:14.194339Z",
     "shell.execute_reply": "2025-06-26T22:10:14.194191Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.194176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc856701",
   "metadata": {},
   "source": [
    "#### Official China PMI (Caixin PMI only starts in 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0aa189",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.197921Z",
     "iopub.status.idle": "2025-06-26T22:10:14.198203Z",
     "shell.execute_reply": "2025-06-26T22:10:14.198059Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.198046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_pmi_china.drop(columns=['Official_Manufacturing_PMI_YoY', 'Official_Services_PMI_YoY'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d53b8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.198944Z",
     "iopub.status.idle": "2025-06-26T22:10:14.199193Z",
     "shell.execute_reply": "2025-06-26T22:10:14.199087Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.199077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_pmi_china.rename(columns={\n",
    "    'Official_Manufacturing_PMI': 'China_Official_Manufacturing_PMI',\n",
    "    'Official_Services_PMI': 'China_Official_Services_PMI'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bfa335",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.199782Z",
     "iopub.status.idle": "2025-06-26T22:10:14.199997Z",
     "shell.execute_reply": "2025-06-26T22:10:14.199906Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.199897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_pmi_china['Date'] = pd.to_datetime(df_pmi_china[\"Date\"], errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d829f1a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.200968Z",
     "iopub.status.idle": "2025-06-26T22:10:14.201238Z",
     "shell.execute_reply": "2025-06-26T22:10:14.201101Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.201087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train['YM'] = df_wm_train['Date'].dt.to_period('M')\n",
    "df_pmi_china    ['YM'] = df_pmi_china    ['Date'].dt.to_period('M')\n",
    "\n",
    "df_merged = df_wm_train.merge(\n",
    "    df_pmi_china[['YM','China_Official_Manufacturing_PMI','China_Official_Services_PMI']],\n",
    "    on='YM',\n",
    "    how='left'\n",
    ").drop(columns='YM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5035e219",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.203128Z",
     "iopub.status.idle": "2025-06-26T22:10:14.203541Z",
     "shell.execute_reply": "2025-06-26T22:10:14.203343Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.20332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55872d6",
   "metadata": {},
   "source": [
    "#### PCE USA (Personal Consumption Expenditures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b48f4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.206335Z",
     "iopub.status.idle": "2025-06-26T22:10:14.206681Z",
     "shell.execute_reply": "2025-06-26T22:10:14.206532Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.206517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_pce.rename(columns={\n",
    "    'Personal_Consumption_Expenditures': 'US_Personal_Consumption_Expenditures'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa5fa96",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.2077Z",
     "iopub.status.idle": "2025-06-26T22:10:14.208024Z",
     "shell.execute_reply": "2025-06-26T22:10:14.207877Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.207863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_pce['DATE'] = pd.to_datetime(df_pce[\"DATE\"], errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53bf8ed",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.208898Z",
     "iopub.status.idle": "2025-06-26T22:10:14.209217Z",
     "shell.execute_reply": "2025-06-26T22:10:14.209069Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.209055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train['YM'] = df_wm_train['Date'].dt.to_period('M')\n",
    "df_pce    ['YM'] = df_pce    ['DATE'].dt.to_period('M')\n",
    "\n",
    "df_merged = df_wm_train.merge(\n",
    "    df_pce[['YM','US_Personal_Consumption_Expenditures']],\n",
    "    on='YM',\n",
    "    how='left'\n",
    ").drop(columns='YM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654ea5b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.210855Z",
     "iopub.status.idle": "2025-06-26T22:10:14.211107Z",
     "shell.execute_reply": "2025-06-26T22:10:14.211006Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.210995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063e49c",
   "metadata": {},
   "source": [
    "#### Interest Rates USA (Fed Funds Rate & Tbill 3 Months Yield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49657aa",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.213027Z",
     "iopub.status.idle": "2025-06-26T22:10:14.213292Z",
     "shell.execute_reply": "2025-06-26T22:10:14.21316Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.213151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_interest_rates['DATE'] = pd.to_datetime(df_interest_rates[\"DATE\"], errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88706b9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.214354Z",
     "iopub.status.idle": "2025-06-26T22:10:14.214817Z",
     "shell.execute_reply": "2025-06-26T22:10:14.214591Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.214575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_interest_rates.rename(columns={\n",
    "    'Fed_Funds_Rate': 'US_Fed_Funds_Rate',\n",
    "    'TBill_3mo_Yield': 'US_TBill_3mo_Yield',\n",
    "    'DATE': 'Date'\n",
    "\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563b328",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.215928Z",
     "iopub.status.idle": "2025-06-26T22:10:14.216374Z",
     "shell.execute_reply": "2025-06-26T22:10:14.216237Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.216226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_wm_train.sort_values('Date')\n",
    "df_interest_rates = df_interest_rates.sort_values('Date')\n",
    "\n",
    "df_merged = df_wm_train.merge(\n",
    "    df_interest_rates[['Date','US_TBill_3mo_Yield','US_Fed_Funds_Rate']],\n",
    "    on='Date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_merged['US_Fed_Funds_Rate'] = df_merged['US_Fed_Funds_Rate'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55aee9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.217649Z",
     "iopub.status.idle": "2025-06-26T22:10:14.217917Z",
     "shell.execute_reply": "2025-06-26T22:10:14.21779Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.217778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f574bc",
   "metadata": {},
   "source": [
    "#### CCI USA (Consumer Confidence Index) from University of Michigan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3db3d5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.219869Z",
     "iopub.status.idle": "2025-06-26T22:10:14.220088Z",
     "shell.execute_reply": "2025-06-26T22:10:14.219993Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.219983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_cci['Date']   = pd.to_datetime(df_us_cci['DATE'])\n",
    "df_us_cci = df_us_cci.sort_values('Date')\n",
    "\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_us_cci[['Date','Consumer_Sentiment_UMich']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444d26d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.221301Z",
     "iopub.status.idle": "2025-06-26T22:10:14.221767Z",
     "shell.execute_reply": "2025-06-26T22:10:14.221622Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.221607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5cbab",
   "metadata": {},
   "source": [
    "#### U.S.A Advance Retail Sales: Retail Trade and Food Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d07682",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.224423Z",
     "iopub.status.idle": "2025-06-26T22:10:14.224701Z",
     "shell.execute_reply": "2025-06-26T22:10:14.224595Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.224581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_retail['Date']   = pd.to_datetime(df_us_retail['DATE'])\n",
    "df_us_retail = df_us_retail.sort_values('Date')\n",
    "\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_us_retail[['Date','Retail_Sales_Retail_and_Food_Services_USA']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfb761",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.225346Z",
     "iopub.status.idle": "2025-06-26T22:10:14.225915Z",
     "shell.execute_reply": "2025-06-26T22:10:14.225709Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.225683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a9ac9",
   "metadata": {},
   "source": [
    "#### Exchange Rates (China, Mexico, Canada, India, Vietnam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed900a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.230232Z",
     "iopub.status.idle": "2025-06-26T22:10:14.230696Z",
     "shell.execute_reply": "2025-06-26T22:10:14.230508Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.23049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_fx.rename(columns={\n",
    "    'Unnamed: 0': 'Date',\n",
    "    \"('VND_per_USD', 'USDVND=X')\": 'VND_per_USD'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19a385",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.232127Z",
     "iopub.status.idle": "2025-06-26T22:10:14.232532Z",
     "shell.execute_reply": "2025-06-26T22:10:14.232341Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.232324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_fx['Date']   = pd.to_datetime(df_fx['Date'])\n",
    "df_fx = df_fx.sort_values('Date')\n",
    "\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_fx[['Date','CNY_per_USD', 'MXN_per_USD','CAD_per_USD','INR_per_USD','VND_per_USD']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3893577f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.233612Z",
     "iopub.status.idle": "2025-06-26T22:10:14.23393Z",
     "shell.execute_reply": "2025-06-26T22:10:14.233776Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.233761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5457db",
   "metadata": {},
   "source": [
    "#### US External Tax Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02ca28",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.236467Z",
     "iopub.status.idle": "2025-06-26T22:10:14.236755Z",
     "shell.execute_reply": "2025-06-26T22:10:14.236647Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.236632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_tariff.rename(columns={\n",
    "    'Canada': 'US_TAX_Canada',\n",
    "    'China': 'US_TAX_China',\n",
    "    'India': 'US_TAX_India',\n",
    "    'Mexico': 'US_TAX_Mexico',\n",
    "    'Viet Nam': 'US_TAX_Vietnam'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c820d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.237561Z",
     "iopub.status.idle": "2025-06-26T22:10:14.237778Z",
     "shell.execute_reply": "2025-06-26T22:10:14.237684Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.237674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# list of tariff columns\n",
    "tariff_cols = [\n",
    "    'US_TAX_Canada',\n",
    "    'US_TAX_China',\n",
    "    'US_TAX_India',\n",
    "    'US_TAX_Mexico',\n",
    "    'US_TAX_Vietnam'\n",
    "]\n",
    "\n",
    "# 1) prepare the dates\n",
    "df_wm_train['Date']      = pd.to_datetime(df_wm_train['Date'])\n",
    "df_us_tariff['Date']     = pd.date_range(\n",
    "    '2001-01-01',\n",
    "    periods=len(df_us_tariff),\n",
    "    freq='YS'\n",
    ")\n",
    "\n",
    "# 2) sort & merge_asof, then forward-fill tariffs\n",
    "df_merged = (\n",
    "    pd.merge_asof(\n",
    "        df_wm_train.sort_values('Date'),\n",
    "        df_us_tariff[['Date'] + tariff_cols].sort_values('Date'),\n",
    "        on='Date',\n",
    "        direction='backward'\n",
    "    )\n",
    "    .assign(**{col: lambda d, col=col: d[col].ffill() for col in tariff_cols})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67456ecb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.239221Z",
     "iopub.status.idle": "2025-06-26T22:10:14.239557Z",
     "shell.execute_reply": "2025-06-26T22:10:14.239434Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.239419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9125c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.240933Z",
     "iopub.status.idle": "2025-06-26T22:10:14.241264Z",
     "shell.execute_reply": "2025-06-26T22:10:14.241114Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.2411Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23031f73",
   "metadata": {},
   "source": [
    "#### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f24b19",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.245226Z",
     "iopub.status.idle": "2025-06-26T22:10:14.245594Z",
     "shell.execute_reply": "2025-06-26T22:10:14.245425Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.245409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_holiday_impact['Date'] = pd.to_datetime(df_holiday_impact['Date'])\n",
    "\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_holiday_impact[['Date','HolidayImpact']].sort_values('Date'),\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f3912",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.246531Z",
     "iopub.status.idle": "2025-06-26T22:10:14.246856Z",
     "shell.execute_reply": "2025-06-26T22:10:14.246705Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.24669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347f09e6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.249017Z",
     "iopub.status.idle": "2025-06-26T22:10:14.24936Z",
     "shell.execute_reply": "2025-06-26T22:10:14.24922Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.249205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "#  make sure both Date columns are datetime at midnight\n",
    "df_us_holidays['Date'] = pd.to_datetime(df_us_holidays['Date']).dt.normalize()\n",
    "df_merged       ['Date'] = pd.to_datetime(df_merged       ['Date']).dt.normalize()\n",
    "\n",
    "# 2. compute each holiday’s “week_end” Friday (weekday=4)\n",
    "df_us_holidays['week_end'] = (\n",
    "    df_us_holidays['Date']\n",
    "  + pd.to_timedelta((4 - df_us_holidays['Date'].dt.weekday) % 7, unit='D')\n",
    ")\n",
    "\n",
    "#  build lookup: week_end → joined holiday names\n",
    "df_holiday_names = (\n",
    "    df_us_holidays\n",
    "      .groupby('week_end', as_index=False)['Holiday']\n",
    "      .agg(lambda names: ', '.join(names))\n",
    "      .rename(columns={'week_end':'Date', 'Holiday':'Holiday_Name'})\n",
    ")\n",
    "\n",
    "#  merge it in\n",
    "df_merged = df_merged.merge(df_holiday_names, on='Date', how='left')\n",
    "\n",
    "#  for weeks without a holiday, fill in “No Holiday”\n",
    "df_merged['Holiday_Name'] = df_merged['Holiday_Name'].fillna('No Holiday')\n",
    "\n",
    "# recompute your flag if you like:\n",
    "df_merged['IsHoliday'] = df_merged['Holiday_Name'] != 'No Holiday'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b04c5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.251967Z",
     "iopub.status.idle": "2025-06-26T22:10:14.252482Z",
     "shell.execute_reply": "2025-06-26T22:10:14.252171Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.252155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa92fc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.253495Z",
     "iopub.status.idle": "2025-06-26T22:10:14.253861Z",
     "shell.execute_reply": "2025-06-26T22:10:14.253732Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.253712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a784bf",
   "metadata": {},
   "source": [
    "#### Tax Return (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b4d53",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.25735Z",
     "iopub.status.idle": "2025-06-26T22:10:14.257614Z",
     "shell.execute_reply": "2025-06-26T22:10:14.257514Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.257503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_tax_return_train.rename(columns={\n",
    "    'TaxReturnImpact': 'US_Tax_Return'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57759c75",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.259209Z",
     "iopub.status.idle": "2025-06-26T22:10:14.259545Z",
     "shell.execute_reply": "2025-06-26T22:10:14.259414Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.259398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_tax_return_train['Date']   = pd.to_datetime(df_tax_return_train['Date'])\n",
    "df_tax_return_train = df_tax_return_train.sort_values('Date')\n",
    "\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_tax_return_train[['Date','US_Tax_Return']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec11622",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.260013Z",
     "iopub.status.idle": "2025-06-26T22:10:14.260252Z",
     "shell.execute_reply": "2025-06-26T22:10:14.260148Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.260134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6580a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.261942Z",
     "iopub.status.idle": "2025-06-26T22:10:14.26225Z",
     "shell.execute_reply": "2025-06-26T22:10:14.262095Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.262081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaca538",
   "metadata": {},
   "source": [
    "#### Stores Types & Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80754b3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.268294Z",
     "iopub.status.idle": "2025-06-26T22:10:14.268591Z",
     "shell.execute_reply": "2025-06-26T22:10:14.268476Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.268461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_store_types_sizes['Store'] = df_store_types_sizes['Store'].astype(int)\n",
    "df_wm_train['Store'] = df_wm_train['Store'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f75a78",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.270475Z",
     "iopub.status.idle": "2025-06-26T22:10:14.270828Z",
     "shell.execute_reply": "2025-06-26T22:10:14.27067Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.270654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_store_info = df_store_types_sizes.drop_duplicates(subset='Store')\n",
    "\n",
    "main_df = df_wm_train.merge(df_store_info, on='Store', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec39f6f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.272722Z",
     "iopub.status.idle": "2025-06-26T22:10:14.273046Z",
     "shell.execute_reply": "2025-06-26T22:10:14.272903Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.272887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ff2a7",
   "metadata": {},
   "source": [
    "#### Oil Price The U.S. domestic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb65ec",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.277463Z",
     "iopub.status.idle": "2025-06-26T22:10:14.277812Z",
     "shell.execute_reply": "2025-06-26T22:10:14.277662Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.277647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_oil_price['DATE'] = pd.to_datetime(df_us_oil_price['DATE'])\n",
    "df_us_oil_price = df_us_oil_price.set_index('DATE')\n",
    "\n",
    "# Interpolate missing values\n",
    "df_us_oil_price['DCOILWTICO'] = df_us_oil_price['DCOILWTICO'].interpolate(\n",
    "    method='linear',\n",
    "    limit_direction='both',\n",
    "    limit_area='inside'\n",
    ")\n",
    "\n",
    "df_us_oil_price = df_us_oil_price.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8e9fe",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.279371Z",
     "iopub.status.idle": "2025-06-26T22:10:14.279776Z",
     "shell.execute_reply": "2025-06-26T22:10:14.279609Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.279593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_oil_price = df_us_oil_price.rename(columns={'DATE': 'Date'})\n",
    "\n",
    "df_merged = df_wm_train.merge(df_us_oil_price[['Date', 'DCOILWTICO']], on='Date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062736d6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.281256Z",
     "iopub.status.idle": "2025-06-26T22:10:14.281646Z",
     "shell.execute_reply": "2025-06-26T22:10:14.281488Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.281471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1c9c7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.282951Z",
     "iopub.status.idle": "2025-06-26T22:10:14.283294Z",
     "shell.execute_reply": "2025-06-26T22:10:14.283121Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.283111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361e351",
   "metadata": {},
   "source": [
    "#### U.S. ISM Manufacturing PMI & ISM Services PMI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3497e6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.28665Z",
     "iopub.status.idle": "2025-06-26T22:10:14.28692Z",
     "shell.execute_reply": "2025-06-26T22:10:14.286819Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.286808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_ism['Date'] = pd.to_datetime(df_us_ism['Date'])\n",
    "df_us_ism = df_us_ism.set_index('Date')\n",
    "\n",
    "# Interpolate missing values\n",
    "df_us_ism['ISM_Manufacturing_PMI'] = df_us_ism['ISM_Manufacturing_PMI'].interpolate(\n",
    "    method='linear',\n",
    "    limit_direction='both',\n",
    "    limit_area='inside'\n",
    ")\n",
    "df_us_ism['ISM_Services_PMI'] = df_us_ism['ISM_Services_PMI'].interpolate(\n",
    "    method='linear',\n",
    "    limit_direction='both',\n",
    "    limit_area='inside'\n",
    ")\n",
    "\n",
    "\n",
    "df_us_ism = df_us_ism.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51064e60",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.288884Z",
     "iopub.status.idle": "2025-06-26T22:10:14.289556Z",
     "shell.execute_reply": "2025-06-26T22:10:14.289336Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.289313Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_us_ism[['Date','ISM_Manufacturing_PMI','ISM_Services_PMI']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9170d54",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.291347Z",
     "iopub.status.idle": "2025-06-26T22:10:14.291682Z",
     "shell.execute_reply": "2025-06-26T22:10:14.291572Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.291559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945176c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.292731Z",
     "iopub.status.idle": "2025-06-26T22:10:14.293056Z",
     "shell.execute_reply": "2025-06-26T22:10:14.29292Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.292905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b695a",
   "metadata": {},
   "source": [
    "#### US CPI Food & Beverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d615af",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.29684Z",
     "iopub.status.idle": "2025-06-26T22:10:14.297116Z",
     "shell.execute_reply": "2025-06-26T22:10:14.296994Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.296983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_cpi_food = df_us_cpi_food.rename(columns={'DATE': 'Date'})\n",
    "df_us_cpi_food['Date'] = pd.to_datetime(df_us_cpi_food['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48089b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.298954Z",
     "iopub.status.idle": "2025-06-26T22:10:14.299308Z",
     "shell.execute_reply": "2025-06-26T22:10:14.299142Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.299127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_us_cpi_food[['Date','CPI_Food_Beverages']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b428242",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.300369Z",
     "iopub.status.idle": "2025-06-26T22:10:14.300719Z",
     "shell.execute_reply": "2025-06-26T22:10:14.300569Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.300554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5561f4f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.301815Z",
     "iopub.status.idle": "2025-06-26T22:10:14.302148Z",
     "shell.execute_reply": "2025-06-26T22:10:14.302Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.301985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea0b9d",
   "metadata": {},
   "source": [
    "#### US CPI Shelter (Housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e80fe",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.307154Z",
     "iopub.status.idle": "2025-06-26T22:10:14.307545Z",
     "shell.execute_reply": "2025-06-26T22:10:14.307369Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.307353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_cpi_shelter = df_us_cpi_shelter.rename(columns={'DATE': 'Date'})\n",
    "df_us_cpi_shelter['Date'] = pd.to_datetime(df_us_cpi_shelter['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b03902",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.308901Z",
     "iopub.status.idle": "2025-06-26T22:10:14.309264Z",
     "shell.execute_reply": "2025-06-26T22:10:14.309095Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.30908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_us_cpi_shelter[['Date','CPI_Shelter']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f9582",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.310346Z",
     "iopub.status.idle": "2025-06-26T22:10:14.310714Z",
     "shell.execute_reply": "2025-06-26T22:10:14.310557Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.310541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb6f13",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.312299Z",
     "iopub.status.idle": "2025-06-26T22:10:14.312679Z",
     "shell.execute_reply": "2025-06-26T22:10:14.312515Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.312498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e04df",
   "metadata": {},
   "source": [
    "#### US CPI Medical Care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8250753e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.31702Z",
     "iopub.status.idle": "2025-06-26T22:10:14.317378Z",
     "shell.execute_reply": "2025-06-26T22:10:14.317236Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.31722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_cpi_med = df_us_cpi_med.rename(columns={'DATE': 'Date'})\n",
    "df_us_cpi_med['Date'] = pd.to_datetime(df_us_cpi_med['Date'])\n",
    "\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_us_cpi_med[['Date','CPI_Medical_Care']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8fa51",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.318351Z",
     "iopub.status.idle": "2025-06-26T22:10:14.31866Z",
     "shell.execute_reply": "2025-06-26T22:10:14.318551Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.318535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c42970",
   "metadata": {},
   "source": [
    "#### US CPI Transportation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051daf3e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.323246Z",
     "iopub.status.idle": "2025-06-26T22:10:14.323697Z",
     "shell.execute_reply": "2025-06-26T22:10:14.323555Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.323534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_cpi_trans = df_us_cpi_trans.rename(columns={'DATE': 'Date'})\n",
    "df_us_cpi_trans['Date'] = pd.to_datetime(df_us_cpi_trans['Date'])\n",
    "\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_us_cpi_trans[['Date','CPI_Transportation']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d144f9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.324991Z",
     "iopub.status.idle": "2025-06-26T22:10:14.325253Z",
     "shell.execute_reply": "2025-06-26T22:10:14.325139Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.325129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a0202b",
   "metadata": {},
   "source": [
    "#### PCE: US Healthcare Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a9a492",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.330935Z",
     "iopub.status.idle": "2025-06-26T22:10:14.331538Z",
     "shell.execute_reply": "2025-06-26T22:10:14.331277Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.331246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_pce_health = df_us_pce_health.rename(columns={'DATE': 'Date'})\n",
    "df_us_pce_health['Date'] = pd.to_datetime(df_us_pce_health['Date'])\n",
    "\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_us_pce_health[['Date','PCE_Healthcare_Services']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13326b19",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.333101Z",
     "iopub.status.idle": "2025-06-26T22:10:14.333374Z",
     "shell.execute_reply": "2025-06-26T22:10:14.333261Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.33325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8860384",
   "metadata": {},
   "source": [
    "#### US ICSA (Jobless Claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd7cf4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.33762Z",
     "iopub.status.idle": "2025-06-26T22:10:14.338292Z",
     "shell.execute_reply": "2025-06-26T22:10:14.338149Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.338133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_icsa_jobless = df_us_icsa_jobless.rename(columns={'DATE': 'Date'})\n",
    "df_us_icsa_jobless['Date'] = pd.to_datetime(df_us_icsa_jobless['Date'])\n",
    "\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_us_icsa_jobless[['Date','Weekly_Initial_Jobless_Claims']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a643ebc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.339427Z",
     "iopub.status.idle": "2025-06-26T22:10:14.339715Z",
     "shell.execute_reply": "2025-06-26T22:10:14.339603Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.339588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f1d38",
   "metadata": {},
   "source": [
    "#### US Rail , Freight & Carloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb105590",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.34377Z",
     "iopub.status.idle": "2025-06-26T22:10:14.344058Z",
     "shell.execute_reply": "2025-06-26T22:10:14.343953Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.343939Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_us_rail_freight_carloads = df_us_rail_freight_carloads.rename(columns={'DATE': 'Date'})\n",
    "df_us_rail_freight_carloads['Date'] = pd.to_datetime(df_us_rail_freight_carloads['Date'])\n",
    "\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_us_rail_freight_carloads[['Date','Rail_Freight_Carloads','Truck_Tonnage_Index']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8f6d5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.345092Z",
     "iopub.status.idle": "2025-06-26T22:10:14.345363Z",
     "shell.execute_reply": "2025-06-26T22:10:14.345256Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.345246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8b0772",
   "metadata": {},
   "source": [
    "#### EU PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d1001",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.351085Z",
     "iopub.status.idle": "2025-06-26T22:10:14.351682Z",
     "shell.execute_reply": "2025-06-26T22:10:14.351501Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.351484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_eu_pmi['Date'] = pd.to_datetime(df_eu_pmi['Date'])\n",
    "\n",
    "df_merged = pd.merge_asof(\n",
    "    df_wm_train.sort_values('Date'),\n",
    "    df_eu_pmi[['Date','Euro_Manufacturing_PMI']],\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f354341b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.359425Z",
     "iopub.status.idle": "2025-06-26T22:10:14.360017Z",
     "shell.execute_reply": "2025-06-26T22:10:14.359836Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.359815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe9775b",
   "metadata": {},
   "source": [
    "#### Walmart Promotion Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e21d82",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.363219Z",
     "iopub.status.idle": "2025-06-26T22:10:14.363568Z",
     "shell.execute_reply": "2025-06-26T22:10:14.363403Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.363373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_merged = df_wm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec848b46",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.372954Z",
     "iopub.status.idle": "2025-06-26T22:10:14.373277Z",
     "shell.execute_reply": "2025-06-26T22:10:14.373165Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.373152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# — make sure both date columns are true datetimes —\n",
    "df_merged['Date']           = pd.to_datetime(df_merged['Date'])\n",
    "df_walmart_features['Date'] = pd.to_datetime(df_walmart_features['Date'])\n",
    "\n",
    "# — pick just the columns you need from df_walmart_features —\n",
    "md_cols = ['Store','Date','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\n",
    "\n",
    "# — left-merge so every (store,dept,date) in df_merged picks up that store's markdowns for that week —\n",
    "df_out = df_merged.merge(\n",
    "    df_walmart_features[md_cols],\n",
    "    on=['Store','Date'],\n",
    "    how='left'\n",
    ")\n",
    "df_merged = df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90a926",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.374116Z",
     "iopub.status.idle": "2025-06-26T22:10:14.374449Z",
     "shell.execute_reply": "2025-06-26T22:10:14.374315Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.374303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_unique = df_merged.drop_duplicates(subset='Date', keep='first')\n",
    "df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e1eb2b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.3758Z",
     "iopub.status.idle": "2025-06-26T22:10:14.376072Z",
     "shell.execute_reply": "2025-06-26T22:10:14.375947Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.375936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9998a599",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0b5cb",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8eff0d",
   "metadata": {},
   "source": [
    "#### Columns Renaming Reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c828107",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.378307Z",
     "iopub.status.idle": "2025-06-26T22:10:14.37859Z",
     "shell.execute_reply": "2025-06-26T22:10:14.378484Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.378473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# lowercase all columns\n",
    "df_merged.columns = df_merged.columns.str.lower()\n",
    "\n",
    "# define new order in lowercase, with markdowns at the end\n",
    "new_order = [\n",
    "    # identifiers & target\n",
    "    'date', 'store', 'type', 'size', 'dept', 'weekly_sales',\n",
    "\n",
    "    # holiday flags\n",
    "    'isholiday', 'holiday_name', 'holidayimpact',\n",
    "\n",
    "    # tax & return\n",
    "    'us_tax_return',\n",
    "\n",
    "    # market indices\n",
    "    'sp500_weekly_mean_close', 'wmt_weekly_mean_close',\n",
    "\n",
    "    # logistics-stock closes\n",
    "    'arcb_df_logistics_close', 'ait_df_logistics_close', 'ceva_df_logistics_close',\n",
    "    'fdx_df_logistics_close', 'saia_df_logistics_close', 'tfii.to_df_logistics_close',\n",
    "    'xpo_df_logistics_close', 'odfl_df_logistics_close', 'ups_df_logistics_close',\n",
    "    'jbht_df_logistics_close',\n",
    "\n",
    "    # oil price\n",
    "    'dcoilwtico',\n",
    "\n",
    "    # PMI\n",
    "    'china_official_manufacturing_pmi', 'china_official_services_pmi',\n",
    "    'ism_manufacturing_pmi', 'ism_services_pmi', 'euro_manufacturing_pmi',\n",
    "\n",
    "    # sentiment & consumption\n",
    "    'consumer_sentiment_umich', 'us_personal_consumption_expenditures',\n",
    "    'retail_sales_retail_and_food_services_usa',\n",
    "\n",
    "    # interest rates\n",
    "    'us_tbill_3mo_yield', 'us_fed_funds_rate',\n",
    "\n",
    "    # inflation measures\n",
    "    'cpi_food_beverages', 'cpi_shelter', 'cpi_medical_care', 'cpi_transportation',\n",
    "    'pce_healthcare_services',\n",
    "\n",
    "    # labor & freight\n",
    "    'weekly_initial_jobless_claims', 'rail_freight_carloads', 'truck_tonnage_index',\n",
    "\n",
    "    # FX rates\n",
    "    'cny_per_usd', 'mxn_per_usd', 'cad_per_usd', 'inr_per_usd', 'vnd_per_usd',\n",
    "\n",
    "    # trade tariffs\n",
    "    'us_tax_canada', 'us_tax_china', 'us_tax_india', 'us_tax_mexico', 'us_tax_vietnam',\n",
    "\n",
    "    # markdowns\n",
    "    'markdown1', 'markdown2', 'markdown3', 'markdown4', 'markdown5'\n",
    "]\n",
    "\n",
    "# sanity check\n",
    "missing = [col for col in new_order if col not in df_merged.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"These columns are missing from df_merged: {missing}\")\n",
    "\n",
    "# reorder\n",
    "df_merged = df_merged[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf3ed5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.379784Z",
     "iopub.status.idle": "2025-06-26T22:10:14.380111Z",
     "shell.execute_reply": "2025-06-26T22:10:14.379962Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.379946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# lowercase every column name\n",
    "df_merged.columns = df_merged.columns.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb25dfb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.381111Z",
     "iopub.status.idle": "2025-06-26T22:10:14.381466Z",
     "shell.execute_reply": "2025-06-26T22:10:14.381288Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.381273Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686dd941",
   "metadata": {},
   "source": [
    "#### Date Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6d9ca",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.384732Z",
     "iopub.status.idle": "2025-06-26T22:10:14.38501Z",
     "shell.execute_reply": "2025-06-26T22:10:14.384885Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.384871Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Start from your raw df_merged_wm_train\n",
    "df_merged = df_wm_train.copy()\n",
    "\n",
    "# Parse date, extract year & weekofyr, then sort\n",
    "df_merged['date']     = pd.to_datetime(df_merged['date'])\n",
    "df_merged['year']     = df_merged['date'].dt.year\n",
    "df_merged['weekofyr'] = df_merged['date'].dt.isocalendar().week.astype(int)\n",
    "df_merged = df_merged.sort_values(['store','dept','date'])\n",
    "\n",
    "# Build lag features (with NaNs in the first 1/4/52 rows per group)\n",
    "df_merged = df_merged.set_index(['store','dept','date'])\n",
    "for lag in [1,4,52]:\n",
    "    df_merged[f'lag_{lag}'] = df_merged.groupby(level=['store','dept'])['weekly_sales'].shift(lag)\n",
    "df_merged = df_merged.reset_index()\n",
    "\n",
    "# Compute seasonal means by (store,dept,weekofyr,year)\n",
    "seasonal = (\n",
    "    df_merged\n",
    "    .groupby(['store','dept','weekofyr','year'])['weekly_sales']\n",
    "    .mean()\n",
    "    .reset_index(name='seasonal_mean')\n",
    ")\n",
    "\n",
    "# Fit year‐over‐year trend (slope & intercept) for each (store,dept,weekofyr)\n",
    "trend_params = {}\n",
    "for (st, dp, wk), grp in seasonal.groupby(['store','dept','weekofyr']):\n",
    "    yrs   = grp['year'].values\n",
    "    means = grp['seasonal_mean'].values\n",
    "    if len(yrs) >= 2:\n",
    "        m, b = np.polyfit(yrs, means, 1)\n",
    "    else:\n",
    "        # if only one year available, flat trend\n",
    "        m, b = 0.0, means[0]\n",
    "    trend_params[(st,dp,wk)] = (m, b)\n",
    "\n",
    "# Define an imputer that uses seasonal-trend prediction\n",
    "def predict_seasonal(row):\n",
    "    key = (row.store, row.dept, row.weekofyr)\n",
    "    m, b = trend_params.get(key, (0.0, np.nan))\n",
    "    return m * row.year + b\n",
    "\n",
    "# fill any missing lag_* by seasonal-trend\n",
    "for lag in [1,4,52]:\n",
    "    col = f'lag_{lag}'\n",
    "    mask = df_merged[col].isna()\n",
    "    df_merged.loc[mask, col] = df_merged[mask].apply(predict_seasonal, axis=1)\n",
    "\n",
    "# Clip at zero\n",
    "for lag in [1,4,52]:\n",
    "    df_merged[f'lag_{lag}'] = df_merged[f'lag_{lag}'].clip(lower=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979f7a5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.386129Z",
     "iopub.status.idle": "2025-06-26T22:10:14.386371Z",
     "shell.execute_reply": "2025-06-26T22:10:14.38627Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.38626Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# long-term drift   \n",
    "df_merged['year'] = df_merged['date'].dt.year\n",
    "\n",
    "# annual seasonality on a weekly grid\n",
    "df_merged['weekofyr'] = df_merged['date'].dt.isocalendar().week.astype(int)\n",
    "df_merged['week_sin'] = np.sin(2 * np.pi * df_merged['weekofyr'] / 52)\n",
    "df_merged['week_cos'] = np.cos(2 * np.pi * df_merged['weekofyr'] / 52)\n",
    "\n",
    "# continuous trend\n",
    "df_merged['date_ordinal'] = df_merged['date'].map(pd.Timestamp.toordinal)\n",
    "\n",
    "# drop the raw date if you like\n",
    "df_merged = df_merged.drop(columns=['date'])\n",
    "\n",
    "df_wm_train = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9434b23f-d8f0-4355-be2b-a21b052a3d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T22:38:50.924103Z",
     "iopub.status.busy": "2025-06-26T22:38:50.923796Z",
     "iopub.status.idle": "2025-06-26T22:38:54.404406Z",
     "shell.execute_reply": "2025-06-26T22:38:54.403606Z",
     "shell.execute_reply.started": "2025-06-26T22:38:50.924082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "#Kaggle\n",
    "df_wm_train = pd.read_csv('/kaggle/input/df-wm-train01-kaggle/df_wm_train01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19cd38-892a-4c6e-bec6-238ff01c5a4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T22:39:20.242456Z",
     "iopub.status.busy": "2025-06-26T22:39:20.241688Z",
     "iopub.status.idle": "2025-06-26T22:39:20.345353Z",
     "shell.execute_reply": "2025-06-26T22:39:20.344344Z",
     "shell.execute_reply.started": "2025-06-26T22:39:20.242429Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541cf7dd",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f78e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T22:39:14.068023Z",
     "iopub.status.busy": "2025-06-26T22:39:14.06773Z",
     "iopub.status.idle": "2025-06-26T22:39:14.100297Z",
     "shell.execute_reply": "2025-06-26T22:39:14.099135Z",
     "shell.execute_reply.started": "2025-06-26T22:39:14.067995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "type_map = {'A': 1, 'B': 2, 'C': 3}\n",
    "holiday_map = {False: 0, True: 1}\n",
    "\n",
    "df_wm_train['type']      = df_wm_train['type'].map(type_map)\n",
    "df_wm_train['isholiday'] = df_wm_train['isholiday'].map(holiday_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2283700-7b2c-40f0-b4ba-7aabcf346712",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T22:39:31.594538Z",
     "iopub.status.busy": "2025-06-26T22:39:31.594236Z",
     "iopub.status.idle": "2025-06-26T22:39:54.983052Z",
     "shell.execute_reply": "2025-06-26T22:39:54.982293Z",
     "shell.execute_reply.started": "2025-06-26T22:39:31.594517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "#Kaggle\n",
    "df_wm_train.to_csv(\"/kaggle/working/df_wm_train02.csv\", index=False)\n",
    "df_wm_train = pd.read_csv('/kaggle/working/df_wm_train02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece482c-3f10-401c-9aa8-a7a9d64761a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T22:40:33.397502Z",
     "iopub.status.busy": "2025-06-26T22:40:33.397204Z",
     "iopub.status.idle": "2025-06-26T22:40:33.479647Z",
     "shell.execute_reply": "2025-06-26T22:40:33.47874Z",
     "shell.execute_reply.started": "2025-06-26T22:40:33.397479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1ee67",
   "metadata": {},
   "source": [
    "## Data Viz EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e19b8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.393049Z",
     "iopub.status.idle": "2025-06-26T22:10:14.393369Z",
     "shell.execute_reply": "2025-06-26T22:10:14.393222Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.393207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d113781b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.394301Z",
     "iopub.status.idle": "2025-06-26T22:10:14.39466Z",
     "shell.execute_reply": "2025-06-26T22:10:14.394501Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.394486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "plt.figure()\n",
    "df_wm_train.groupby('year')['weekly_sales'].mean().plot()\n",
    "plt.title(\"Avg Weekly Sales Over Time\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.xlabel(\"Date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e045f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.395777Z",
     "iopub.status.idle": "2025-06-26T22:10:14.396206Z",
     "shell.execute_reply": "2025-06-26T22:10:14.395949Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.395933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "plt.figure(figsize=(14,4))\n",
    "for i, lag in enumerate(['lag_1','lag_4','lag_52'], 1):\n",
    "    plt.subplot(1,3,i)\n",
    "    sns.scatterplot(x=lag, y='weekly_sales', data=df_wm_train, alpha=0.3)\n",
    "    plt.title(f\"{lag} vs Weekly Sales\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323edc89",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.398076Z",
     "iopub.status.idle": "2025-06-26T22:10:14.398445Z",
     "shell.execute_reply": "2025-06-26T22:10:14.398265Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.398249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "order = df_wm_train.groupby('holiday_name')['weekly_sales'].median().sort_values(ascending=False).index\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    x='holiday_name',\n",
    "    y='weekly_sales',\n",
    "    data=df_wm_train,\n",
    "    order=order,\n",
    "    width=0.6\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.tick_params(axis='x', pad=12)\n",
    "plt.title(\"Sales by Holiday/Special Event (sorted by median sales)\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Weekly Sales\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d955296",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.399773Z",
     "iopub.status.idle": "2025-06-26T22:10:14.40012Z",
     "shell.execute_reply": "2025-06-26T22:10:14.39997Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.399955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(\n",
    "    x='weekofyr', y='weekly_sales',\n",
    "    hue='year', estimator='mean',\n",
    "    data=df_wm_train, palette=\"tab10\"\n",
    ")\n",
    "plt.title(\"Average Sales by Week of Year, by Year\")\n",
    "plt.xlabel(\"Week of Year\")\n",
    "plt.ylabel(\"Avg Weekly Sales\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05037fa6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.401522Z",
     "iopub.status.idle": "2025-06-26T22:10:14.401757Z",
     "shell.execute_reply": "2025-06-26T22:10:14.401657Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.401647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "plt.figure(figsize=(14,5))\n",
    "subset = df_wm_train[df_wm_train['store'].isin([1,2,3,4,9,25,45])] \n",
    "sns.lineplot(\n",
    "    x='weekofyr', y='weekly_sales',\n",
    "    hue='store', data=subset,\n",
    "    estimator='mean', ci=None\n",
    ")\n",
    "plt.title(\"Sales Over Time for Sample Stores\")\n",
    "plt.xlabel(\"Week of Year\")\n",
    "plt.ylabel(\"Weekly Sales\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e29501c",
   "metadata": {},
   "source": [
    "## Data Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b38d9b",
   "metadata": {},
   "source": [
    "#### Drop columns Correlation >= 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98aa215",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.403158Z",
     "iopub.status.idle": "2025-06-26T22:10:14.403428Z",
     "shell.execute_reply": "2025-06-26T22:10:14.403302Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.403293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train_pruned = df_wm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcee736",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.404696Z",
     "iopub.status.idle": "2025-06-26T22:10:14.40494Z",
     "shell.execute_reply": "2025-06-26T22:10:14.404839Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.404828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# absolute correlation of your numeric features\n",
    "df_num = df_wm_train_pruned.select_dtypes(include=[np.number])\n",
    "corr = df_num.corr().abs()\n",
    "\n",
    "# upper‐triangle mask\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# find all pairs with |r|>0.8\n",
    "thresh = 0.9\n",
    "high_corr = (\n",
    "    corr.where(~mask)               \n",
    "        .stack()                    \n",
    "        .loc[lambda s: s > thresh] \n",
    "        .sort_values(ascending=False)\n",
    ")\n",
    "high_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478732ad",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.405565Z",
     "iopub.status.idle": "2025-06-26T22:10:14.406013Z",
     "shell.execute_reply": "2025-06-26T22:10:14.405697Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.405683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "target_corr = corr['weekly_sales'].sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591780c3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.407358Z",
     "iopub.status.idle": "2025-06-26T22:10:14.407638Z",
     "shell.execute_reply": "2025-06-26T22:10:14.407535Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.407516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "us_tax_mexico → keep us_tax_vietnam\n",
    "(Identical tariff signals.)\n",
    "\n",
    "weekofyr → keep week_sin/week_cos\n",
    "(Raw week number is redundant—cyclical encoding is sufficient.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606970db",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.408583Z",
     "iopub.status.idle": "2025-06-26T22:10:14.408868Z",
     "shell.execute_reply": "2025-06-26T22:10:14.408726Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.408712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "to_drop = [\n",
    "    'us_tax_mexico',\n",
    "    'weekofyr',\n",
    "]\n",
    "\n",
    "df_wm_train_pruned = df_wm_train.drop(columns=to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8399b60f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.410292Z",
     "iopub.status.idle": "2025-06-26T22:10:14.410675Z",
     "shell.execute_reply": "2025-06-26T22:10:14.410537Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.410516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_wm_train_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d867b77",
   "metadata": {},
   "source": [
    "## ML Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c5ced",
   "metadata": {},
   "source": [
    "### 1st selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda32584",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d68cdd-6835-443b-bd86-483c7971cec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T02:02:05.529937Z",
     "iopub.status.busy": "2025-06-27T02:02:05.529201Z",
     "iopub.status.idle": "2025-06-27T02:02:09.846203Z",
     "shell.execute_reply": "2025-06-27T02:02:09.845413Z",
     "shell.execute_reply.started": "2025-06-27T02:02:05.529907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "#Kaggle\n",
    "df_wm_train_pruned = pd.read_csv('/kaggle/input/df-wm-train02/df_wm_train02.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ecf7f1-2bd2-46b5-8c56-691b76b42703",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#VsCode\n",
    "df_wm_train_pruned = pd.read_csv('csv_files/ml_train_data/df_wm_train02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7d960",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T02:02:10.021342Z",
     "iopub.status.busy": "2025-06-27T02:02:10.020836Z",
     "iopub.status.idle": "2025-06-27T02:02:10.137988Z",
     "shell.execute_reply": "2025-06-27T02:02:10.137395Z",
     "shell.execute_reply.started": "2025-06-27T02:02:10.021315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_merged = df_wm_train_pruned\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34fafe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T02:02:11.454462Z",
     "iopub.status.busy": "2025-06-27T02:02:11.453851Z",
     "iopub.status.idle": "2025-06-27T02:02:11.866044Z",
     "shell.execute_reply": "2025-06-27T02:02:11.865426Z",
     "shell.execute_reply.started": "2025-06-27T02:02:11.454436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_merged['date'] = df_wm_train_pruned['date_ordinal'].apply(lambda x: datetime.fromordinal(int(x)))\n",
    "\n",
    "# Verify the resulting date range\n",
    "min_date = df_merged['date'].min()\n",
    "max_date = df_merged['date'].max()\n",
    "\n",
    "print(f\"First date in series: {min_date.date()}\")\n",
    "print(f\"Last date in series: {max_date.date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed184ad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T02:02:15.031484Z",
     "iopub.status.busy": "2025-06-27T02:02:15.030654Z",
     "iopub.status.idle": "2025-06-27T02:02:15.221158Z",
     "shell.execute_reply": "2025-06-27T02:02:15.220413Z",
     "shell.execute_reply.started": "2025-06-27T02:02:15.031447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df_merged.copy()\n",
    "\n",
    "# cutoff as 52 weeks before the last date\n",
    "last_date = df['date'].max()\n",
    "cutoff   = last_date - pd.Timedelta(weeks=52)\n",
    "\n",
    "# split\n",
    "train = df[df['date'] <= cutoff]\n",
    "test  = df[df['date'] >  cutoff]\n",
    "\n",
    "print(f\"Training set: {train['date'].min().date()} → {train['date'].max().date()} ({len(train)} rows)\")\n",
    "print(f\"Test set:     {test['date'].min().date()} → {test['date'].max().date()} ({len(test)} rows)\")\n",
    "\n",
    "\n",
    "# What share of rows are in TRAIN vs TEST?\n",
    "\n",
    "total_rows  = len(train) + len(test)\n",
    "train_pct   = len(train) / total_rows * 100\n",
    "test_pct    = len(test)  / total_rows * 100\n",
    "\n",
    "\n",
    "print(f\"Rows in training set : {len(train):,}  ({train_pct:5.1f} %)\")\n",
    "print(f\"Rows in test set     : {len(test):,}  ({test_pct:5.1f} %)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f09b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T02:02:17.449303Z",
     "iopub.status.busy": "2025-06-27T02:02:17.449024Z",
     "iopub.status.idle": "2025-06-27T02:02:17.519193Z",
     "shell.execute_reply": "2025-06-27T02:02:17.518405Z",
     "shell.execute_reply.started": "2025-06-27T02:02:17.449283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=['weekly_sales', 'date', 'holiday_name'])\n",
    "y_train = train['weekly_sales']\n",
    "\n",
    "X_test  = test.drop(columns=['weekly_sales', 'date', 'holiday_name'])\n",
    "y_test  = test['weekly_sales']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d414cc",
   "metadata": {},
   "source": [
    "#### Custome Evaluation metric (WMAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f20f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T02:02:21.643566Z",
     "iopub.status.busy": "2025-06-27T02:02:21.643066Z",
     "iopub.status.idle": "2025-06-27T02:02:21.648403Z",
     "shell.execute_reply": "2025-06-27T02:02:21.647691Z",
     "shell.execute_reply.started": "2025-06-27T02:02:21.643541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ish = train['isholiday']   # 0/1 Series indexed same as X_train, y_train\n",
    "\n",
    "def weight_fn(idx_labels):\n",
    "    \"\"\"\n",
    "    idx_labels: an Index of row‐labels from y_true\n",
    "    returns an array of 5s and 1s matching those labels.\n",
    "    \"\"\"\n",
    "    # .loc uses the actual labels (dates + store/dept multi‐index, if any)\n",
    "    return np.where(ish.loc[idx_labels] == 1, 5, 1)\n",
    "\n",
    "# Two‐arg WMAE that ModelTrainer expects\n",
    "def wmae_custom(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: pd.Series\n",
    "    y_pred: np.ndarray (same length)\n",
    "    \"\"\"\n",
    "    w = weight_fn(y_true.index)\n",
    "    # now compute weighted MAE\n",
    "    return (w * np.abs(y_true - y_pred)).sum() / w.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e57b8",
   "metadata": {},
   "source": [
    "#### LightGBM Hyper Parameters Search . CV Estimate (used to choose hyper-parameters faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373a3d8",
   "metadata": {},
   "source": [
    "##### LightGBM Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d25d820",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.423477Z",
     "iopub.status.idle": "2025-06-26T22:10:14.423808Z",
     "shell.execute_reply": "2025-06-26T22:10:14.423645Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.42363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "def lgb_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\",  lgb.LGBMRegressor(**params, random_state=7, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = lgb_pipeline_factory, \n",
    "    search        = 'random',\n",
    "    param_grid    = {\n",
    "        'num_leaves':       [31, 50, 100, 150, 200],\n",
    "        'max_depth':        [-1, 5, 10, 15, 20],\n",
    "        'learning_rate':    [0.01, 0.05, 0.1],\n",
    "        'n_estimators':     [100, 300, 500],\n",
    "        'feature_fraction': [0.6, 0.8, 1.0],\n",
    "        'bagging_fraction': [0.6, 0.8, 1.0],\n",
    "        'bagging_freq':     [0, 5, 10],\n",
    "        'min_child_samples':[5, 10, 20, 50],\n",
    "        # new regularization / split-gain params:\n",
    "        'min_split_gain':   [0, 0.1, 0.5, 1.0],\n",
    "        'lambda_l1':        [0, 0.1, 0.5, 1.0],\n",
    "        'lambda_l2':        [0, 0.1, 0.5, 1.0],\n",
    "    },\n",
    "    n_iter           = 50,\n",
    "    cv_splitter      = TimeSeriesSplit(n_splits=2),\n",
    "    custom_metrics   = {'wmae': wmae_custom},\n",
    "    log_path         = 'csv_files/ml_train_data/lgbm_random_wmae01.csv',\n",
    "    model_name       = 'LGBM_WMAE',\n",
    "    problem_type     = 'reg',\n",
    "    n_jobs           = -1,\n",
    "    random_state     = 7\n",
    ")\n",
    "\n",
    "\n",
    "# Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\" Best params:\", best_params)\n",
    "print(\" Best WMAE:  \", best_score)\n",
    "\n",
    "# explicitly close the CSV handle\n",
    "trainer.csv_file.close()\n",
    "# drop the trainer object and force garbage collection\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925be82",
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-26T22:16:29.763Z",
     "iopub.status.busy": "2025-06-26T22:10:14.425126Z",
     "iopub.status.idle": "2025-06-26T22:10:14.425582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "lgbm_random_wmae01 = pd.read_csv('csv_files/ml_train_data/lgbm_random_wmae01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9c30b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.430086Z",
     "iopub.status.idle": "2025-06-26T22:10:14.430333Z",
     "shell.execute_reply": "2025-06-26T22:10:14.430229Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.430219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df = lgbm_random_wmae01.copy()             \n",
    "\n",
    "params_df = (\n",
    "    df['param_json']\n",
    "    .apply(lambda s: json.loads(s) if pd.notnull(s) else {})  \n",
    "    .apply(pd.Series)                                         \n",
    ")\n",
    "df = pd.concat([df, params_df], axis=1)\n",
    "\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name', 'param_json',            \n",
    "          'mae', 'rmse', 'r2', metric, '__source__'}\n",
    "\n",
    "param_cols = [c for c in df.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyper-params\n",
    "X = df[param_cols].select_dtypes(include=[np.number])\n",
    "y = df[metric]\n",
    "\n",
    "# guard against “empty X” (all NaNs / non-numeric)\n",
    "if X.shape[1] == 0:\n",
    "    raise ValueError(\"No numeric hyper-parameter columns left after filtering!\")\n",
    "\n",
    "# urrogate model + analyses \n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial-dependence plots \n",
    "top = importances.sort_values(ascending=False).index[:]\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "for ax in axes[len(top):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7150b09",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.431215Z",
     "iopub.status.idle": "2025-06-26T22:10:14.431482Z",
     "shell.execute_reply": "2025-06-26T22:10:14.431347Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.431337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "def lgb_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\",  lgb.LGBMRegressor(**params, random_state=7, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = lgb_pipeline_factory, \n",
    "    search        = 'random',\n",
    "    param_grid    = {\n",
    "        'num_leaves':       [150],\n",
    "        'max_depth':        [0, 100, 200],\n",
    "        'learning_rate':    [0.1],\n",
    "        'n_estimators':     [500],\n",
    "        'feature_fraction': [1],\n",
    "        'bagging_fraction': [1],\n",
    "        'bagging_freq':     [0, 20, 40],\n",
    "        'min_child_samples':[1],\n",
    "        # new regularization / split-gain params:\n",
    "        'min_split_gain':   [3,4,5],\n",
    "        'lambda_l1':        [0.1],\n",
    "        'lambda_l2':        [3,4,5],\n",
    "    },\n",
    "    n_iter           = 100,\n",
    "    cv_splitter      = TimeSeriesSplit(n_splits=2),\n",
    "    custom_metrics   = {'wmae': wmae_custom},\n",
    "    log_path         = 'csv_files/ml_train_data/lgbm_random_wmae03.csv',\n",
    "    model_name       = 'LGBM_WMAE',\n",
    "    problem_type     = 'reg',\n",
    "    n_jobs           = -1,\n",
    "    random_state     = 7\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\" Best params:\", best_params)\n",
    "print(\" Best WMAE:  \", best_score)\n",
    "\n",
    "# explicitly close the CSV handle\n",
    "trainer.csv_file.close()\n",
    "# drop the trainer object and force garbage collection\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4413578",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.433164Z",
     "iopub.status.idle": "2025-06-26T22:10:14.43367Z",
     "shell.execute_reply": "2025-06-26T22:10:14.433552Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.433539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "lgbm_random_wmae02 = pd.read_csv('csv_files/ml_train_data/lgbm_random_wmae02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2bbfcb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.436195Z",
     "iopub.status.idle": "2025-06-26T22:10:14.436538Z",
     "shell.execute_reply": "2025-06-26T22:10:14.436367Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.436353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# load & explode both CSVs if you haven’t yet —\n",
    "paths = [\n",
    "    'csv_files/ml_train_data/lgbm_random_wmae01.csv',\n",
    "    'csv_files/ml_train_data/lgbm_random_wmae02.csv',\n",
    "]\n",
    "dfs = []\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    params = df['param_json'].apply(json.loads).apply(pd.Series)\n",
    "    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n",
    "    df['__source__'] = path.split('/')[-1]\n",
    "    dfs.append(df)\n",
    "df_all = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49786d56",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.437247Z",
     "iopub.status.idle": "2025-06-26T22:10:14.437582Z",
     "shell.execute_reply": "2025-06-26T22:10:14.437427Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.437412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# choose metric & hyper‐param cols, drop any non‐numeric ones —\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name','mae','rmse','r2', metric, '__source__'}\n",
    "param_cols = [c for c in df_all.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyperparams\n",
    "X = df_all[param_cols].select_dtypes(include=[np.number])\n",
    "y = df_all[metric]\n",
    "\n",
    "# train surrogate —\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# RF importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# permutation importances\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial dependence plots for top-3 features, all in one figure —\n",
    "top = importances.sort_values(ascending=False).index[:].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "# turn off the last (unused) subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70bccdb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.443575Z",
     "iopub.status.idle": "2025-06-26T22:10:14.444116Z",
     "shell.execute_reply": "2025-06-26T22:10:14.443833Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.443816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "def lgb_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\",  lgb.LGBMRegressor(**params, random_state=7, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = lgb_pipeline_factory, \n",
    "    search        = 'random',\n",
    "    param_grid    = {\n",
    "        'num_leaves':       [150],\n",
    "        'max_depth':        [0],\n",
    "        'learning_rate':    [0.1, 0.2, 0.3, 0.4],\n",
    "        'n_estimators':     [500,600,700,800],\n",
    "        'feature_fraction': [1],\n",
    "        'bagging_fraction': [1],\n",
    "        'bagging_freq':     [0],\n",
    "        'min_child_samples':[1],\n",
    "        # new regularization / split-gain params:\n",
    "        'min_split_gain':   [3],\n",
    "        'lambda_l1':        [0.1],\n",
    "        'lambda_l2':        [4, 6, 7],\n",
    "    },\n",
    "    n_iter           = 100,\n",
    "    cv_splitter      = TimeSeriesSplit(n_splits=2),\n",
    "    custom_metrics   = {'wmae': wmae_custom},\n",
    "    log_path         = 'csv_files/ml_train_data/lgbm_random_wmae03.csv',\n",
    "    model_name       = 'LGBM_WMAE',\n",
    "    problem_type     = 'reg',\n",
    "    n_jobs           = -1,\n",
    "    random_state     = 7\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\" Best params:\", best_params)\n",
    "print(\" Best WMAE:  \", best_score)\n",
    "\n",
    "# explicitly close the CSV handle\n",
    "trainer.csv_file.close()\n",
    "# drop the trainer object and force garbage collection\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469c2a9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.450693Z",
     "iopub.status.idle": "2025-06-26T22:10:14.451096Z",
     "shell.execute_reply": "2025-06-26T22:10:14.450926Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.45091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# load & explode both CSVs if you haven’t yet —\n",
    "paths = [\n",
    "    'csv_files/ml_train_data/lgbm_random_wmae01.csv',\n",
    "    'csv_files/ml_train_data/lgbm_random_wmae02.csv',\n",
    "    'csv_files/ml_train_data/lgbm_random_wmae03.csv',\n",
    "]\n",
    "dfs = []\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    params = df['param_json'].apply(json.loads).apply(pd.Series)\n",
    "    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n",
    "    df['__source__'] = path.split('/')[-1]\n",
    "    dfs.append(df)\n",
    "df_all = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6365f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.455226Z",
     "iopub.status.idle": "2025-06-26T22:10:14.455565Z",
     "shell.execute_reply": "2025-06-26T22:10:14.455444Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.455428Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# choose metric & hyper‐param cols, drop any non‐numeric ones —\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name','mae','rmse','r2', metric, '__source__'}\n",
    "param_cols = [c for c in df_all.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyperparams\n",
    "X = df_all[param_cols].select_dtypes(include=[np.number])\n",
    "y = df_all[metric]\n",
    "\n",
    "# train surrogate —\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# RF importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# permutation importances\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial dependence plots for top-3 features, all in one figure —\n",
    "top = importances.sort_values(ascending=False).index[:].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "# turn off the last (unused) subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ddffb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.456607Z",
     "iopub.status.idle": "2025-06-26T22:10:14.457098Z",
     "shell.execute_reply": "2025-06-26T22:10:14.456978Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.456962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "def lgb_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\",  lgb.LGBMRegressor(**params, random_state=7, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = lgb_pipeline_factory, \n",
    "    search        = 'random',\n",
    "    param_grid    = {\n",
    "        'num_leaves':       [150],\n",
    "        'max_depth':        [0],\n",
    "        'learning_rate':    [0.1],\n",
    "        'n_estimators':     [500],\n",
    "        'feature_fraction': [1],\n",
    "        'bagging_fraction': [1],\n",
    "        'bagging_freq':     [0],\n",
    "        'min_child_samples':[0],\n",
    "        # new regularization / split-gain params:\n",
    "        'min_split_gain':   [3],\n",
    "        'lambda_l1':        [0.1],\n",
    "        'lambda_l2':        [6, 8, 9, 10, 11, 12],\n",
    "    },\n",
    "    n_iter           = 100,\n",
    "    cv_splitter      = TimeSeriesSplit(n_splits=2),\n",
    "    custom_metrics   = {'wmae': wmae_custom},\n",
    "    log_path         = 'csv_files/ml_train_data/lgbm_random_wmae04.csv',\n",
    "    model_name       = 'LGBM_WMAE',\n",
    "    problem_type     = 'reg',\n",
    "    n_jobs           = -1,\n",
    "    random_state     = 7\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\" Best params:\", best_params)\n",
    "print(\" Best WMAE:  \", best_score)\n",
    "\n",
    "# explicitly close the CSV handle\n",
    "trainer.csv_file.close()\n",
    "# drop the trainer object and force garbage collection\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a383ae4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.457915Z",
     "iopub.status.idle": "2025-06-26T22:10:14.458165Z",
     "shell.execute_reply": "2025-06-26T22:10:14.458038Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.458029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# load & explode both CSVs if you haven’t yet —\n",
    "paths = [\n",
    "    'csv_files/ml_train_data/lgbm_random_wmae01.csv',\n",
    "    'csv_files/ml_train_data/lgbm_random_wmae02.csv',\n",
    "    'csv_files/ml_train_data/lgbm_random_wmae03.csv',\n",
    "    'csv_files/ml_train_data/lgbm_random_wmae04.csv',\n",
    "\n",
    "]\n",
    "dfs = []\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    params = df['param_json'].apply(json.loads).apply(pd.Series)\n",
    "    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n",
    "    df['__source__'] = path.split('/')[-1]\n",
    "    dfs.append(df)\n",
    "df_all = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884f00c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.461264Z",
     "iopub.status.idle": "2025-06-26T22:10:14.461817Z",
     "shell.execute_reply": "2025-06-26T22:10:14.461563Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.461423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# choose metric & hyper‐param cols, drop any non‐numeric ones —\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name','mae','rmse','r2', metric, '__source__'}\n",
    "param_cols = [c for c in df_all.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyperparams\n",
    "X = df_all[param_cols].select_dtypes(include=[np.number])\n",
    "y = df_all[metric]\n",
    "\n",
    "# train surrogate —\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# RF importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# permutation importances\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial dependence plots for top-3 features, all in one figure —\n",
    "top = importances.sort_values(ascending=False).index[:].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "# turn off the last (unused) subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8462a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.469962Z",
     "iopub.status.idle": "2025-06-26T22:10:14.470285Z",
     "shell.execute_reply": "2025-06-26T22:10:14.470152Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.47014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_lgbm_random_04 = df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3ecbe",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.47125Z",
     "iopub.status.idle": "2025-06-26T22:10:14.471607Z",
     "shell.execute_reply": "2025-06-26T22:10:14.47146Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.471445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_lgbm_random_04.to_csv('csv_files/ml_train_data/df_lgbm_random_04.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2d254",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.473344Z",
     "iopub.status.idle": "2025-06-26T22:10:14.473702Z",
     "shell.execute_reply": "2025-06-26T22:10:14.47356Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.473544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_lgbm_random_04 = pd.read_csv('csv_files/ml_train_data/df_lgbm_random_04.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4c8a1",
   "metadata": {},
   "source": [
    "##### LightGBM Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49673cc4",
   "metadata": {},
   "source": [
    "Get best params from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db62ec",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.474662Z",
     "iopub.status.idle": "2025-06-26T22:10:14.476294Z",
     "shell.execute_reply": "2025-06-26T22:10:14.474841Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.474824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df = df_lgbm_random_04.copy()\n",
    "\n",
    "# pick the row with the lowest WMAE\n",
    "best_row = df.loc[df['wmae'].idxmin()]\n",
    "\n",
    "# keep only the hyper-parameter columns\n",
    "non_params = {'model_name', 'mae', 'rmse', 'r2', 'wmae', '__source__'}\n",
    "param_series = best_row.drop(labels=non_params)\n",
    "\n",
    "# convert to a clean dict, turning floats like 500.0 → 500\n",
    "best_params = {\n",
    "    k: (int(v) if isinstance(v, (int, float)) and not math.isnan(v) and v.is_integer() else v)\n",
    "    for k, v in param_series.items()\n",
    "    if pd.notnull(v)                           # skip NaNs\n",
    "}\n",
    "\n",
    "\n",
    "best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de16f4cf",
   "metadata": {},
   "source": [
    "Train best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c0e4a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.477465Z",
     "iopub.status.idle": "2025-06-26T22:10:14.477799Z",
     "shell.execute_reply": "2025-06-26T22:10:14.477648Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.477633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Rebuild your best‐found model\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", lgb.LGBMRegressor(**best_params, random_state=7, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# 1) Fit on the entire training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# 2) Predict on the test set\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# 3) Compute standard metrics\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "# 4) Compute WMAE (holiday weeks weight=5, others=1)\n",
    "#    assumes you still have `test` DataFrame with 'holiday_name'\n",
    "weights = np.where(test['holiday_name'].notnull(), 5, 1)\n",
    "wmae    = (weights * np.abs(y_test - y_pred)).sum() / weights.sum()\n",
    "\n",
    "# 5) Print them all\n",
    "print(f\"Test MAE:   {mae:.4f}\")\n",
    "print(f\"Test RMSE:  {rmse:.4f}\")\n",
    "print(f\"Test R²:    {r2:.4f}\")\n",
    "print(f\"Test WMAE:  {wmae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd62ad4",
   "metadata": {},
   "source": [
    "Check what features were used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1bdb6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.479022Z",
     "iopub.status.idle": "2025-06-26T22:10:14.479325Z",
     "shell.execute_reply": "2025-06-26T22:10:14.479199Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.479184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# grab the fitted LGBM from your pipeline\n",
    "model = pipe.named_steps['model']\n",
    "\n",
    "# make a Series of the importances\n",
    "fi = pd.Series(\n",
    "    model.feature_importances_,\n",
    "    index = X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# print the top 10\n",
    "print(\"🌳 LightGBM split-gain importances:\")\n",
    "print(fi)\n",
    "\n",
    "# number of times the feature was used by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fdb39a",
   "metadata": {},
   "source": [
    "##### LGBM Retrain wihout not used features (Best Model to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43ec05",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.480261Z",
     "iopub.status.idle": "2025-06-26T22:10:14.480595Z",
     "shell.execute_reply": "2025-06-26T22:10:14.480444Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.480427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "cutoff  = df_merged.date.max() - pd.Timedelta(weeks=52)\n",
    "train   = df_merged[df_merged.date <= cutoff]\n",
    "test    = df_merged[df_merged.date >  cutoff]\n",
    "\n",
    "drop = [\n",
    "    'weekly_sales','date','holiday_name','year',\n",
    "    'markdown5','markdown1','markdown2','markdown3','markdown4',\n",
    "    'us_tax_india','us_tax_vietnam','us_tax_china','us_tax_canada'\n",
    "]\n",
    "\n",
    "X_train, y_train = train.drop(columns=drop), train.weekly_sales\n",
    "X_test,  y_test  = test .drop(columns=drop),  test .weekly_sales\n",
    "\n",
    "\n",
    "# Fit model, keep only used features\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\",  lgb.LGBMRegressor(**best_params, random_state=7, n_jobs=-1))\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "used_feats = X_train.columns[pipe.named_steps['model'].feature_importances_ > 0]\n",
    "pipe.fit(X_train[used_feats], y_train)\n",
    "\n",
    "\n",
    "# Predict & compute metrics\n",
    "\n",
    "y_pred  = pipe.predict(X_test[used_feats])\n",
    "\n",
    "mae     = mean_absolute_error(y_test, y_pred)\n",
    "rmse    = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2      = r2_score(y_test, y_pred)\n",
    "weights = np.where(test['holiday_name'].notnull(), 5, 1)\n",
    "wmae    = (weights * np.abs(y_test - y_pred)).sum() / weights.sum()\n",
    "\n",
    "\n",
    "row = {\n",
    "    \"model_name\": 'LGBM01',\n",
    "    **best_params,\n",
    "    \"mae\":  mae,\n",
    "    \"rmse\": rmse,\n",
    "    \"r2\":   r2,\n",
    "    \"wmae\": wmae\n",
    "}\n",
    "df_best_models = pd.DataFrame([row])\n",
    "\n",
    "df_best_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217948b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.481803Z",
     "iopub.status.idle": "2025-06-26T22:10:14.482103Z",
     "shell.execute_reply": "2025-06-26T22:10:14.481986Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.481971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_best_models.to_csv('csv_files/ml_train_data/df_best_models.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903370b0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.483188Z",
     "iopub.status.idle": "2025-06-26T22:10:14.484009Z",
     "shell.execute_reply": "2025-06-26T22:10:14.483868Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.483848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_best_models = pd.read_csv('csv_files/ml_train_data/df_best_models.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca00b9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.486215Z",
     "iopub.status.idle": "2025-06-26T22:10:14.486538Z",
     "shell.execute_reply": "2025-06-26T22:10:14.486361Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.486346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# to merge new best model : \n",
    "results_df = pd.concat([results_df, df_best_models], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f153ce4",
   "metadata": {},
   "source": [
    "### XGBoostRegressor Hyper Parameters Search / CV Estimate (used to choose hyper-parameters faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37252fb",
   "metadata": {},
   "source": [
    "##### XGBoost Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2709d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.487456Z",
     "iopub.status.idle": "2025-06-26T22:10:14.487765Z",
     "shell.execute_reply": "2025-06-26T22:10:14.487656Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.487642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Define an XGBoost pipeline factory\n",
    "def xgb_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\",  XGBRegressor(**params, random_state=7, n_jobs=-1, verbosity=1))\n",
    "    ])\n",
    "\n",
    "# Set up the random search over a balanced grid of XGBoost hyper-params\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = xgb_pipeline_factory, \n",
    "    search        = 'random',\n",
    "    param_grid    = {\n",
    "        'n_estimators':      [700,800,900],\n",
    "        'max_depth':         [10, 11, 12, 13],\n",
    "        'learning_rate':     [0.2, 0.3, 0.4, 0.5],\n",
    "        'subsample':         [1],\n",
    "        'colsample_bytree':  [0.7, 0.8, 0.9],\n",
    "        'gamma':             [0.01, 0.05 ,0.1],\n",
    "        'min_child_weight':  [1, 5, 10, 20],\n",
    "        'reg_alpha':         [1, 2, 3],\n",
    "        'reg_lambda':        [0.8, 1.0, 1.2],\n",
    "    },\n",
    "    n_iter           = 100,\n",
    "    cv_splitter      = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics   = {'wmae': wmae_custom},\n",
    "    log_path         = 'csv_files/ml_train_data/xgb_random_wmae01.csv',\n",
    "    model_name       = 'XGB_WMAE',\n",
    "    problem_type     = 'reg',\n",
    "    n_jobs           = -1,\n",
    "    random_state     = 7\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best params:\", best_params)\n",
    "print(\" Best WMAE:  \", best_score)\n",
    "\n",
    "# Clean up\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced5b6a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.488747Z",
     "iopub.status.idle": "2025-06-26T22:10:14.488997Z",
     "shell.execute_reply": "2025-06-26T22:10:14.488893Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.488882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "xgb_random_wmae01 = pd.read_csv('csv_files/ml_train_data/xgb_random_wmae01.csv')\n",
    "xgb_random_wmae01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff092b55",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.490684Z",
     "iopub.status.idle": "2025-06-26T22:10:14.491156Z",
     "shell.execute_reply": "2025-06-26T22:10:14.4909Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.490884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df = xgb_random_wmae01.copy()             \n",
    "\n",
    "params_df = (\n",
    "    df['param_json']\n",
    "    .apply(lambda s: json.loads(s) if pd.notnull(s) else {})  \n",
    "    .apply(pd.Series)                                         \n",
    ")\n",
    "df = pd.concat([df, params_df], axis=1)\n",
    "\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name', 'param_json',            \n",
    "          'mae', 'rmse', 'r2', metric, '__source__'}\n",
    "\n",
    "param_cols = [c for c in df.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyper-params\n",
    "X = df[param_cols].select_dtypes(include=[np.number])\n",
    "y = df[metric]\n",
    "\n",
    "# guard against “empty X” (all NaNs / non-numeric)\n",
    "if X.shape[1] == 0:\n",
    "    raise ValueError(\"No numeric hyper-parameter columns left after filtering!\")\n",
    "\n",
    "# urrogate model + analyses \n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial-dependence plots \n",
    "top = importances.sort_values(ascending=False).index[:]\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "for ax in axes[len(top):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a322dd",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.493591Z",
     "iopub.status.idle": "2025-06-26T22:10:14.493858Z",
     "shell.execute_reply": "2025-06-26T22:10:14.493754Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.493742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Define an XGBoost pipeline factory\n",
    "def xgb_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\",  XGBRegressor(**params, random_state=7, n_jobs=-1, verbosity=1))\n",
    "    ])\n",
    "\n",
    "# Set up the random search over a balanced grid of XGBoost hyper-params\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = xgb_pipeline_factory, \n",
    "    search        = 'random',\n",
    "    param_grid    = {\n",
    "        'n_estimators':      [200, 500, 700],\n",
    "        'max_depth':         [2,5,8,10],\n",
    "        'learning_rate':     [0, 0.05, 0.1, 0.2],\n",
    "        'subsample':         [0, 0.5, 1],\n",
    "        'colsample_bytree':  [0, 0.5, 0.8, 1],\n",
    "        'gamma':             [0 ,0.001],\n",
    "        'min_child_weight':  [0, 0.05, 0.1, 1],\n",
    "        'reg_alpha':         [0, 0.5, 5],\n",
    "        'reg_lambda':        [1.0],\n",
    "    },\n",
    "    n_iter           = 180,\n",
    "    cv_splitter      = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics   = {'wmae': wmae_custom},\n",
    "    log_path         = 'csv_files/ml_train_data/xgb_random_wmae02.csv',\n",
    "    model_name       = 'XGB_WMAE',\n",
    "    problem_type     = 'reg',\n",
    "    n_jobs           = -1,\n",
    "    random_state     = 7\n",
    ")\n",
    "\n",
    "# 3) Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best params:\", best_params)\n",
    "print(\" Best WMAE:  \", best_score)\n",
    "\n",
    "# 4) Clean up\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426646f6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.495171Z",
     "iopub.status.idle": "2025-06-26T22:10:14.495566Z",
     "shell.execute_reply": "2025-06-26T22:10:14.495382Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.495367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# load & explode both CSVs if you haven’t yet —\n",
    "paths = [\n",
    "    'csv_files/ml_train_data/xgb_random_wmae01.csv',\n",
    "    'csv_files/ml_train_data/xgb_random_wmae02.csv',\n",
    "]\n",
    "dfs = []\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    params = df['param_json'].apply(json.loads).apply(pd.Series)\n",
    "    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n",
    "    df['__source__'] = path.split('/')[-1]\n",
    "    dfs.append(df)\n",
    "df_all = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5faf2b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.497021Z",
     "iopub.status.idle": "2025-06-26T22:10:14.497345Z",
     "shell.execute_reply": "2025-06-26T22:10:14.497237Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.497225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# choose metric & hyper‐param cols, drop any non‐numeric ones —\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name','mae','rmse','r2', metric, '__source__'}\n",
    "param_cols = [c for c in df_all.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyperparams\n",
    "X = df_all[param_cols].select_dtypes(include=[np.number])\n",
    "y = df_all[metric]\n",
    "\n",
    "# train surrogate —\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# RF importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# permutation importances\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial dependence plots for top-3 features, all in one figure —\n",
    "top = importances.sort_values(ascending=False).index[:].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "# turn off the last (unused) subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5241d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.49831Z",
     "iopub.status.idle": "2025-06-26T22:10:14.498704Z",
     "shell.execute_reply": "2025-06-26T22:10:14.49853Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.498515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_xgb_random_02 = df_all\n",
    "df_xgb_random_02.to_csv('csv_files/ml_train_data/df_xgb_random_02.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d25595",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.499757Z",
     "iopub.status.idle": "2025-06-26T22:10:14.500111Z",
     "shell.execute_reply": "2025-06-26T22:10:14.49996Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.499945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_xgb_random_02 = pd.read_csv('csv_files/ml_train_data/df_xgb_random_02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44af16",
   "metadata": {},
   "source": [
    "##### XGBoost Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61509a0",
   "metadata": {},
   "source": [
    "Get Best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14129844",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.501176Z",
     "iopub.status.idle": "2025-06-26T22:10:14.501941Z",
     "shell.execute_reply": "2025-06-26T22:10:14.501774Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.501756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df = df_xgb_random_02.copy()\n",
    "\n",
    "# pick the row with the lowest WMAE\n",
    "best_row = df.loc[df['wmae'].idxmin()]\n",
    "\n",
    "# keep only the hyper-parameter columns\n",
    "non_params = {'model_name', 'mae', 'rmse', 'r2', 'wmae', '__source__'}\n",
    "param_series = best_row.drop(labels=non_params)\n",
    "\n",
    "# convert to a clean dict, turning floats like 500.0 → 500\n",
    "best_params = {\n",
    "    k: (int(v) if isinstance(v, (int, float)) and not math.isnan(v) and v.is_integer() else v)\n",
    "    for k, v in param_series.items()\n",
    "    if pd.notnull(v)                           # skip NaNs\n",
    "}\n",
    "\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dbe50e",
   "metadata": {},
   "source": [
    "Train Model with Best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d1cff",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.503328Z",
     "iopub.status.idle": "2025-06-26T22:10:14.503696Z",
     "shell.execute_reply": "2025-06-26T22:10:14.503543Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.503527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Rebuild your best‐found model\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", XGBRegressor(**best_params, random_state=7, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# 1) Fit on the entire training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# 2) Predict on the test set\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# 3) Compute standard metrics\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "# 4) Compute WMAE (holiday weeks weight=5, others=1)\n",
    "#    assumes you still have `test` DataFrame with 'holiday_name'\n",
    "weights = np.where(test['holiday_name'].notnull(), 5, 1)\n",
    "wmae    = (weights * np.abs(y_test - y_pred)).sum() / weights.sum()\n",
    "\n",
    "# 5) Print them all\n",
    "print(f\"Test MAE:   {mae:.4f}\")\n",
    "print(f\"Test RMSE:  {rmse:.4f}\")\n",
    "print(f\"Test R²:    {r2:.4f}\")\n",
    "print(f\"Test WMAE:  {wmae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6d13f",
   "metadata": {},
   "source": [
    "Check what features were most used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a3cc0e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.505112Z",
     "iopub.status.idle": "2025-06-26T22:10:14.50556Z",
     "shell.execute_reply": "2025-06-26T22:10:14.505364Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.505346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# grab the fitted LGBM from your pipeline\n",
    "model = pipe.named_steps['model']\n",
    "\n",
    "# make a Series of the importances\n",
    "fi = pd.Series(\n",
    "    model.feature_importances_,\n",
    "    index = X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# print the top 10\n",
    "print(\"🌳 XGBoost split-gain importances:\")\n",
    "print(fi)\n",
    "\n",
    "# number of times the feature was used by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762892c9",
   "metadata": {},
   "source": [
    "##### XGBOOST Retrain wihout not used features (Best Model to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2081d449",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.506703Z",
     "iopub.status.idle": "2025-06-26T22:10:14.507018Z",
     "shell.execute_reply": "2025-06-26T22:10:14.506886Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.506871Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "cutoff  = df_merged.date.max() - pd.Timedelta(weeks=52)\n",
    "train   = df_merged[df_merged.date <= cutoff]\n",
    "test    = df_merged[df_merged.date >  cutoff]\n",
    "\n",
    "drop = [\n",
    "    'weekly_sales','date','holiday_name','pce_healthcare_services'\n",
    "]\n",
    "\n",
    "X_train, y_train = train.drop(columns=drop), train.weekly_sales\n",
    "X_test,  y_test  = test .drop(columns=drop),  test .weekly_sales\n",
    "\n",
    "\n",
    "# Fit model, keep only used features\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\",  XGBRegressor(**best_params, random_state=7, n_jobs=-1))\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "used_feats = X_train.columns[pipe.named_steps['model'].feature_importances_ > 0]\n",
    "pipe.fit(X_train[used_feats], y_train)\n",
    "\n",
    "\n",
    "# Predict & compute metrics\n",
    "\n",
    "y_pred  = pipe.predict(X_test[used_feats])\n",
    "\n",
    "mae     = mean_absolute_error(y_test, y_pred)\n",
    "rmse    = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2      = r2_score(y_test, y_pred)\n",
    "weights = np.where(test['holiday_name'].notnull(), 5, 1)\n",
    "wmae    = (weights * np.abs(y_test - y_pred)).sum() / weights.sum()\n",
    "\n",
    "\n",
    "row = {\n",
    "    \"model_name\": 'XGBOOST01',\n",
    "    **best_params,\n",
    "    \"mae\":  mae,\n",
    "    \"rmse\": rmse,\n",
    "    \"r2\":   r2,\n",
    "    \"wmae\": wmae\n",
    "}\n",
    "df_best_xgboost = pd.DataFrame([row])\n",
    "\n",
    "df_best_xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613d446",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.508079Z",
     "iopub.status.idle": "2025-06-26T22:10:14.508367Z",
     "shell.execute_reply": "2025-06-26T22:10:14.508223Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.50821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "# to merge new best model : \n",
    "results_df = pd.concat([df_best_xgboost, df_best_models], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8361eaa",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.509364Z",
     "iopub.status.idle": "2025-06-26T22:10:14.509706Z",
     "shell.execute_reply": "2025-06-26T22:10:14.509582Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.509565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_best_models = results_df\n",
    "df_best_models.to_csv('csv_files/ml_train_data/df_best_models.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665cb3f8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.511123Z",
     "iopub.status.idle": "2025-06-26T22:10:14.511498Z",
     "shell.execute_reply": "2025-06-26T22:10:14.511324Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.511282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_best_models = pd.read_csv('csv_files/ml_train_data/df_best_models.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47381d1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.512678Z",
     "iopub.status.idle": "2025-06-26T22:10:14.512989Z",
     "shell.execute_reply": "2025-06-26T22:10:14.512862Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.512848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6243d52a",
   "metadata": {},
   "source": [
    "### CatBoost  Hyper Parameters Search / CV Estimate (used to choose hyper-parameters faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef14f4d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.514147Z",
     "iopub.status.idle": "2025-06-26T22:10:14.514482Z",
     "shell.execute_reply": "2025-06-26T22:10:14.514328Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.514318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "def cat_pipeline_factory(params):\n",
    "    \"\"\"Return a (scaler → CatBoost) pipeline with the given hyper-params.\"\"\"\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),                # numeric features only; harmless for CatBoost\n",
    "        (\"model\",  CatBoostRegressor(\n",
    "                        **params,\n",
    "                        random_state=7,\n",
    "                        verbose=0,                  # silence per-iteration logging\n",
    "                    )\n",
    "        )\n",
    "    ])\n",
    "\n",
    "# Wide, balanced CatBoost hyper-parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    # core learning-rate / depth / boosting length\n",
    "    'iterations'        : [ 500, 1000, 1500, 2000 ],\n",
    "    'depth'             : [ 4, 6, 8, 10 ],\n",
    "    'learning_rate'     : [ 0.01, 0.03, 0.1 ],\n",
    "    \n",
    "    # regularisation & tree shape\n",
    "    'l2_leaf_reg'       : [ 1, 3, 5, 10 ],\n",
    "    'min_data_in_leaf'  : [ 1, 5, 10, 20 ],\n",
    "    'random_strength'   : [ 0, 1, 5, 10 ],\n",
    "    'bagging_temperature': [ 0, 0.5, 1, 2 ],\n",
    "    \n",
    "    # sampling for rows / columns\n",
    "    'subsample'         : [ 0.6, 0.8, 1.0 ],\n",
    "    'rsm'               : [ 0.6, 0.8, 1.0 ],        # column sample\n",
    "       \n",
    "    # other structural knobs\n",
    "    'grow_policy'       : [ 'SymmetricTree', 'Depthwise', 'Lossguide' ],\n",
    "    'border_count'      : [ 32, 64, 128, 254 ],\n",
    "    'one_hot_max_size'  : [ 2, 5, 10 ],\n",
    "    \n",
    "    # objective variants (optional: restrict to one if you prefer)\n",
    "    'loss_function'     : [ 'MAE', 'RMSE']\n",
    "}\n",
    "\n",
    "# Random search trainer setup\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    X               = X_train,\n",
    "    y               = y_train,\n",
    "    model_factory   = cat_pipeline_factory,\n",
    "    search          = 'random',\n",
    "    param_grid      = param_grid,\n",
    "    n_iter          = 90,                          # ← wide but balanced exploration\n",
    "    cv_splitter     = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics  = {'wmae': wmae_custom},\n",
    "    log_path        = 'csv_files/ml_train_data/cat_random_wmae01.csv',\n",
    "    model_name      = 'CAT_WMAE',\n",
    "    problem_type    = 'reg',\n",
    "    n_jobs          = -1,\n",
    "    random_state    = 7\n",
    ")\n",
    "\n",
    "\n",
    "# Run the search\n",
    "\n",
    "best_params, best_score = trainer.train()\n",
    "\n",
    "print(\" Best WMAE: \", best_score)\n",
    "\n",
    "\n",
    "# Clean up\n",
    "\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49586d0e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.515291Z",
     "iopub.status.idle": "2025-06-26T22:10:14.515612Z",
     "shell.execute_reply": "2025-06-26T22:10:14.515491Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.515476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "cat_random_wmae01 = pd.read_csv('csv_files/ml_train_data/cat_random_wmae01.csv')\n",
    "cat_random_wmae01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ce7993",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-26T22:10:14.516491Z",
     "iopub.status.idle": "2025-06-26T22:10:14.516746Z",
     "shell.execute_reply": "2025-06-26T22:10:14.516645Z",
     "shell.execute_reply.started": "2025-06-26T22:10:14.516633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "df = cat_random_wmae01.copy()             \n",
    "\n",
    "params_df = (\n",
    "    df['param_json']\n",
    "    .apply(lambda s: json.loads(s) if pd.notnull(s) else {})  \n",
    "    .apply(pd.Series)                                         \n",
    ")\n",
    "df = pd.concat([df, params_df], axis=1)\n",
    "\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name', 'param_json',            \n",
    "          'mae', 'rmse', 'r2', metric, '__source__'}\n",
    "\n",
    "param_cols = [c for c in df.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyper-params\n",
    "X = df[param_cols].select_dtypes(include=[np.number])\n",
    "y = df[metric]\n",
    "\n",
    "# guard against “empty X” (all NaNs / non-numeric)\n",
    "if X.shape[1] == 0:\n",
    "    raise ValueError(\"No numeric hyper-parameter columns left after filtering!\")\n",
    "\n",
    "# urrogate model + analyses \n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial-dependence plots \n",
    "top = importances.sort_values(ascending=False).index[:]\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "for ax in axes[len(top):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f0943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T02:00:37.325458Z",
     "iopub.status.busy": "2025-06-27T02:00:37.325055Z",
     "iopub.status.idle": "2025-06-27T02:00:37.333577Z",
     "shell.execute_reply": "2025-06-27T02:00:37.332502Z",
     "shell.execute_reply.started": "2025-06-27T02:00:37.325437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "#VSCode\n",
    "def cat_pipeline_factory(params):\n",
    "    \"\"\"\n",
    "    Build a (StandardScaler → CatBoostRegressor) pipeline.\n",
    "\n",
    "    Adds early_stopping_rounds=200 on top of the hyper-parameters supplied\n",
    "    by the random-search engine.\n",
    "    \"\"\"\n",
    "    params = params.copy()\n",
    "    params.update(\n",
    "        early_stopping_rounds=200   # stop if validation metric hasn't improved\n",
    "                                    # for 200 consecutive trees\n",
    "    )\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),        # safe for purely numeric features\n",
    "        (\"model\", CatBoostRegressor(\n",
    "            **params,                        # hyper-parameters from search\n",
    "            random_state=7,                  # reproducible results\n",
    "            verbose=0                        # keep per-tree logging silent\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# Wide, balanced CatBoost hyper-parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    # core learning-rate / depth / boosting length\n",
    "    'iterations'        : [ 3000, 5000 ],\n",
    "    'depth'             : [10, 12, 14 ],\n",
    "    'learning_rate'     : [0.1, 0.2, 0.3 ],\n",
    "    \n",
    "    # regularisation & tree shape\n",
    "    'l2_leaf_reg'       : [ 3, 17, 20 ],\n",
    "    'min_data_in_leaf'  : [ 1,5],\n",
    "    'random_strength'   : [ 0.1],\n",
    "    'bagging_temperature': [ 0.1],\n",
    "    \n",
    "    # sampling for rows / columns\n",
    "    'subsample'         : [ 0.1, 0.8 ],\n",
    "    'rsm'               : [ 0.9 ],        # column sample\n",
    "       \n",
    "    # other structural knobs\n",
    "    'grow_policy'       : [ 'Depthwise', 'Lossguide' ],\n",
    "    'border_count'      : [ 254, 512, 1024 ],\n",
    "    'one_hot_max_size'  : [5],\n",
    "    \n",
    "    # objective variants (optional: restrict to one if you prefer)\n",
    "    'loss_function'     : [ 'MAE', 'RMSE']\n",
    "}\n",
    "\n",
    "# Random search trainer setup\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    X               = X_train,\n",
    "    y               = y_train,\n",
    "    model_factory   = cat_pipeline_factory,\n",
    "    search          = 'random',\n",
    "    param_grid      = param_grid,\n",
    "    n_iter          = 50,                         \n",
    "    cv_splitter     = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics  = {'wmae': wmae_custom},\n",
    "    log_path        = 'csv_files/ml_train_data/cat_random_wmae02.csv',\n",
    "    model_name      = 'CAT_WMAE',\n",
    "    problem_type    = 'reg',\n",
    "    n_jobs          = -1,\n",
    "    random_state    = 7\n",
    ")\n",
    "\n",
    "\n",
    "# Run the search\n",
    "\n",
    "best_params, best_score = trainer.train()\n",
    "\n",
    "print(\" Best WMAE: \", best_score)\n",
    "\n",
    "\n",
    "# Clean up\n",
    "\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef7815-ba2d-49ca-b80f-a24d0ce040eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T02:04:46.002273Z",
     "iopub.status.busy": "2025-06-27T02:04:46.001507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "#Kaggle\n",
    "def cat_pipeline_factory(params):\n",
    "    params = params.copy()\n",
    "    params.update(\n",
    "        task_type=\"GPU\",\n",
    "        devices=\"0\",\n",
    "        metric_period=20,\n",
    "        early_stopping_rounds=200,\n",
    "        bootstrap_type=\"Bernoulli\"      # lets subsample work\n",
    "    )\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", CatBoostRegressor(\n",
    "            **params,\n",
    "            random_state=7,\n",
    "            verbose=0\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    # core learning-rate / depth / boosting length\n",
    "    \"iterations\"        : [3000, 5000],\n",
    "    \"depth\"             : [10, 12, 14],\n",
    "    \"learning_rate\"     : [0.1, 0.2, 0.3],\n",
    "\n",
    "    # regularisation & tree shape\n",
    "    \"l2_leaf_reg\"       : [3, 17, 20],\n",
    "    \"min_data_in_leaf\"  : [1, 5],\n",
    "    \"random_strength\"   : [0.1],\n",
    "\n",
    "    # sampling for rows\n",
    "    \"subsample\"         : [0.1, 0.8],        # row sampling now works\n",
    "    # rsm removed ──↑\n",
    "\n",
    "    # other structural knobs\n",
    "    \"grow_policy\"       : [\"Depthwise\", \"Lossguide\"],\n",
    "    \"border_count\"      : [254, 512, 1024],\n",
    "    \"one_hot_max_size\"  : [5],\n",
    "\n",
    "    # objective variants\n",
    "    \"loss_function\"     : [\"MAE\", \"RMSE\"],\n",
    "}\n",
    "\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    X               = X_train,\n",
    "    y               = y_train,\n",
    "    model_factory   = cat_pipeline_factory,\n",
    "    search          = \"random\",\n",
    "    param_grid      = param_grid,\n",
    "    n_iter          = 50,\n",
    "    cv_splitter     = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics  = {\"wmae\": wmae_custom},\n",
    "    log_path        = \"/kaggle/working/cat_random_wmae02.csv\",\n",
    "    model_name      = \"CAT_WMAE\",\n",
    "    problem_type    = \"reg\",\n",
    "    n_jobs          = 1,                 # one fit → one GPU\n",
    "    random_state    = 7\n",
    ")\n",
    "\n",
    "\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best WMAE:\", best_score)\n",
    "\n",
    "\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# load & explode both CSVs if you haven’t yet —\n",
    "paths = [\n",
    "    'csv_files/ml_train_data/cat_random_wmae01.csv',\n",
    "    'csv_files/ml_train_data/cat_random_wmae02.csv',\n",
    "]\n",
    "dfs = []\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    params = df['param_json'].apply(json.loads).apply(pd.Series)\n",
    "    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n",
    "    df['__source__'] = path.split('/')[-1]\n",
    "    dfs.append(df)\n",
    "df_all = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# choose metric & hyper‐param cols, drop any non‐numeric ones —\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name','mae','rmse','r2', metric, '__source__'}\n",
    "param_cols = [c for c in df_all.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyperparams\n",
    "X = df_all[param_cols].select_dtypes(include=[np.number])\n",
    "y = df_all[metric]\n",
    "\n",
    "# train surrogate —\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# RF importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# permutation importances\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial dependence plots for top-3 features, all in one figure —\n",
    "top = importances.sort_values(ascending=False).index[:].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "# turn off the last (unused) subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "#Kaggle\n",
    "def cat_pipeline_factory(params):\n",
    "    params = params.copy()\n",
    "    params.update(\n",
    "        task_type=\"GPU\",\n",
    "        devices=\"0\",\n",
    "        metric_period=20,\n",
    "        early_stopping_rounds=200,\n",
    "        bootstrap_type=\"Bernoulli\"      # lets subsample work\n",
    "    )\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", CatBoostRegressor(\n",
    "            **params,\n",
    "            random_state=7,\n",
    "            verbose=0\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    # core learning-rate / depth / boosting length\n",
    "    \"iterations\"        : [2000],\n",
    "    \"depth\"             : [12, 16],\n",
    "    \"learning_rate\"     : [0.3, 0.4, 0.5],\n",
    "\n",
    "    # regularisation & tree shape\n",
    "    \"l2_leaf_reg\"       : [17,30],\n",
    "    \"min_data_in_leaf\"  : [3, 5],\n",
    "    \"random_strength\"   : [5],\n",
    "\n",
    "    # sampling for rows\n",
    "    \"subsample\"         : [0.8],        # row sampling now works\n",
    "    # rsm removed ──↑\n",
    "\n",
    "    # other structural knobs\n",
    "    \"grow_policy\"       : [\"Depthwise\", \"Lossguide\"],\n",
    "    \"border_count\"      : [1024, 2048, 4096],\n",
    "    \"one_hot_max_size\"  : [10,15, 20],\n",
    "\n",
    "    # objective variants\n",
    "    \"loss_function\"     : [\"MAE\", \"RMSE\"],\n",
    "}\n",
    "\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    X               = X_train,\n",
    "    y               = y_train,\n",
    "    model_factory   = cat_pipeline_factory,\n",
    "    search          = \"random\",\n",
    "    param_grid      = param_grid,\n",
    "    n_iter          = 50,\n",
    "    cv_splitter     = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics  = {\"wmae\": wmae_custom},\n",
    "    log_path        = \"/kaggle/working/cat_random_wmae02.csv\",\n",
    "    model_name      = \"CAT_WMAE\",\n",
    "    problem_type    = \"reg\",\n",
    "    n_jobs          = 1,                 # one fit → one GPU\n",
    "    random_state    = 7\n",
    ")\n",
    "\n",
    "\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best WMAE:\", best_score)\n",
    "\n",
    "\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad33ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# load & explode both CSVs if you haven’t yet —\n",
    "paths = [\n",
    "    'csv_files/ml_train_data/cat_random_wmae01.csv',\n",
    "    'csv_files/ml_train_data/cat_random_wmae02.csv',\n",
    "    'csv_files/ml_train_data/cat_random_wmae03.csv',\n",
    "]\n",
    "dfs = []\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    params = df['param_json'].apply(json.loads).apply(pd.Series)\n",
    "    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n",
    "    df['__source__'] = path.split('/')[-1]\n",
    "    dfs.append(df)\n",
    "df_all = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92fde00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# choose metric & hyper‐param cols, drop any non‐numeric ones —\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name','mae','rmse','r2', metric, '__source__'}\n",
    "param_cols = [c for c in df_all.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyperparams\n",
    "X = df_all[param_cols].select_dtypes(include=[np.number])\n",
    "y = df_all[metric]\n",
    "\n",
    "# train surrogate —\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# RF importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# permutation importances\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial dependence plots for top-3 features, all in one figure —\n",
    "top = importances.sort_values(ascending=False).index[:].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "# turn off the last (unused) subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "#Kaggle\n",
    "def cat_pipeline_factory(params):\n",
    "    params = params.copy()\n",
    "    params.update(\n",
    "        task_type=\"GPU\",\n",
    "        devices=\"0\",\n",
    "        metric_period=20,\n",
    "        early_stopping_rounds=200,\n",
    "        bootstrap_type=\"Bernoulli\"      # lets subsample work\n",
    "    )\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", CatBoostRegressor(\n",
    "            **params,\n",
    "            random_state=7,\n",
    "            verbose=0\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    # core learning-rate / depth / boosting length\n",
    "    \"iterations\"        : [6000],\n",
    "    \"depth\"             : [12],\n",
    "    \"learning_rate\"     : [0.4],\n",
    "\n",
    "    # regularisation & tree shape\n",
    "    \"l2_leaf_reg\"       : [15],\n",
    "    \"min_data_in_leaf\"  : [5],\n",
    "    \"random_strength\"   : [0.01, 0.1],\n",
    "\n",
    "    # sampling for rows\n",
    "    \"subsample\"         : [0.6],        # row sampling now works\n",
    "    # rsm removed ──↑\n",
    "\n",
    "    # other structural knobs\n",
    "    \"grow_policy\"       : [\"Depthwise\", \"Lossguide\"],\n",
    "    \"border_count\"      : [4096, 8192, 16384],\n",
    "    \"one_hot_max_size\"  : [20,15,30],\n",
    "\n",
    "    # objective variants\n",
    "    \"loss_function\"     : [\"MAE\", \"RMSE\"],\n",
    "}\n",
    "\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    X               = X_train,\n",
    "    y               = y_train,\n",
    "    model_factory   = cat_pipeline_factory,\n",
    "    search          = \"random\",\n",
    "    param_grid      = param_grid,\n",
    "    n_iter          = 50,\n",
    "    cv_splitter     = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics  = {\"wmae\": wmae_custom},\n",
    "    log_path        = \"/kaggle/working/cat_random_wmae04.csv\",\n",
    "    model_name      = \"CAT_WMAE\",\n",
    "    problem_type    = \"reg\",\n",
    "    n_jobs          = 1,                 # one fit → one GPU\n",
    "    random_state    = 7\n",
    ")\n",
    "\n",
    "\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best WMAE:\", best_score)\n",
    "\n",
    "\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04366c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# load & explode both CSVs if you haven’t yet —\n",
    "paths = [\n",
    "    'csv_files/ml_train_data/cat_random_wmae01.csv',\n",
    "    'csv_files/ml_train_data/cat_random_wmae02.csv',\n",
    "    'csv_files/ml_train_data/cat_random_wmae03.csv',\n",
    "    'csv_files/ml_train_data/cat_random_wmae04.csv',\n",
    "]\n",
    "dfs = []\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    params = df['param_json'].apply(json.loads).apply(pd.Series)\n",
    "    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n",
    "    df['__source__'] = path.split('/')[-1]\n",
    "    dfs.append(df)\n",
    "df_all = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07685b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_all.to_csv('csv_files/ml_train_data/df_cat_random04.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# choose metric & hyper‐param cols, drop any non‐numeric ones —\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name','mae','rmse','r2', metric, '__source__'}\n",
    "param_cols = [c for c in df_all.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyperparams\n",
    "X = df_all[param_cols].select_dtypes(include=[np.number])\n",
    "y = df_all[metric]\n",
    "\n",
    "# train surrogate —\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# RF importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# permutation importances\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial dependence plots for top-3 features, all in one figure —\n",
    "top = importances.sort_values(ascending=False).index[:].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "# turn off the last (unused) subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbde2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "#Kaggle\n",
    "def cat_pipeline_factory(params):\n",
    "    params = params.copy()\n",
    "    params.update(\n",
    "        task_type=\"GPU\",\n",
    "        devices=\"0\",\n",
    "        metric_period=20,\n",
    "        early_stopping_rounds=200,\n",
    "        bootstrap_type=\"Bernoulli\"      # lets subsample work\n",
    "    )\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", CatBoostRegressor(\n",
    "            **params,\n",
    "            random_state=7,\n",
    "            verbose=0\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    # core learning-rate / depth / boosting length\n",
    "    \"iterations\"        : [2000],\n",
    "    \"depth\"             : [12],\n",
    "    \"learning_rate\"     : [0.4],\n",
    "\n",
    "    # regularisation & tree shape\n",
    "    \"l2_leaf_reg\"       : [17],\n",
    "    \"min_data_in_leaf\"  : [5],\n",
    "    \"random_strength\"   : [5],\n",
    "\n",
    "    # sampling for rows\n",
    "    \"subsample\"         : [0.8],        # row sampling now works\n",
    "    # rsm removed ──↑\n",
    "\n",
    "    # other structural knobs\n",
    "    \"grow_policy\"       : [\"Depthwise\", \"Lossguide\"],\n",
    "    \"border_count\"      : [8192],\n",
    "    \"one_hot_max_size\"  : [20],\n",
    "\n",
    "    # objective variants\n",
    "    \"loss_function\"     : [\"MAE\", \"RMSE\"],\n",
    "}\n",
    "\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    X               = X_train,\n",
    "    y               = y_train,\n",
    "    model_factory   = cat_pipeline_factory,\n",
    "    search          = \"random\",\n",
    "    param_grid      = param_grid,\n",
    "    n_iter          = 50,\n",
    "    cv_splitter     = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics  = {\"wmae\": wmae_custom},\n",
    "    log_path        = \"/kaggle/working/cat_random_wmae04.csv\",\n",
    "    model_name      = \"CAT_WMAE\",\n",
    "    problem_type    = \"reg\",\n",
    "    n_jobs          = 1,                 # one fit → one GPU\n",
    "    random_state    = 7\n",
    ")\n",
    "\n",
    "\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best WMAE:\", best_score)\n",
    "\n",
    "\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8e622",
   "metadata": {},
   "source": [
    "##### CatBoost Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8150bc",
   "metadata": {},
   "source": [
    "Get Best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eea3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_cat_random04 = pd.read_csv('csv_files/ml_train_data/df_cat_random04.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "df = df_cat_random04.copy()\n",
    "\n",
    "# pick the row with the lowest WMAE\n",
    "best_row = df.loc[df['wmae'].idxmin()]\n",
    "\n",
    "# keep only the hyper-parameter columns\n",
    "non_params = {'model_name', 'mae', 'rmse', 'r2', 'wmae', '__source__'}\n",
    "param_series = best_row.drop(labels=non_params)\n",
    "\n",
    "# convert to a clean dict, turning floats like 500.0 → 500\n",
    "best_params = {\n",
    "    k: (int(v) if isinstance(v, (int, float)) and not math.isnan(v) and v.is_integer() else v)\n",
    "    for k, v in param_series.items()\n",
    "    if pd.notnull(v)                           # skip NaNs\n",
    "}\n",
    "\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a41d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Rebuild your best‐found model\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", CatBoostRegressor(**best_params, random_state=7))\n",
    "])\n",
    "\n",
    "# 1) Fit on the entire training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# 2) Predict on the test set\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# 3) Compute standard metrics\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "# 4) Compute WMAE (holiday weeks weight=5, others=1)\n",
    "#    assumes you still have `test` DataFrame with 'holiday_name'\n",
    "weights = np.where(test['holiday_name'].notnull(), 5, 1)\n",
    "wmae    = (weights * np.abs(y_test - y_pred)).sum() / weights.sum()\n",
    "\n",
    "# 5) Print them all\n",
    "print(f\"Test MAE:   {mae:.4f}\")\n",
    "print(f\"Test RMSE:  {rmse:.4f}\")\n",
    "print(f\"Test R²:    {r2:.4f}\")\n",
    "print(f\"Test WMAE:  {wmae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# grab the fitted LGBM from your pipeline\n",
    "model = pipe.named_steps['model']\n",
    "\n",
    "# make a Series of the importances\n",
    "fi = pd.Series(\n",
    "    model.feature_importances_,\n",
    "    index = X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# print the top 10\n",
    "print(\"🌳 CatBoost split-gain importances:\")\n",
    "display(fi)\n",
    "\n",
    "# number of times the feature was used by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "cutoff  = df_merged.date.max() - pd.Timedelta(weeks=52)\n",
    "train   = df_merged[df_merged.date <= cutoff]\n",
    "test    = df_merged[df_merged.date >  cutoff]\n",
    "\n",
    "drop = [\n",
    "    'weekly_sales','date','holiday_name','markdown1', 'markdown2', 'markdown3', 'markdown4', 'markdown5'\n",
    "]\n",
    "\n",
    "X_train, y_train = train.drop(columns=drop), train.weekly_sales\n",
    "X_test,  y_test  = test .drop(columns=drop),  test .weekly_sales\n",
    "\n",
    "\n",
    "# Fit model, keep only used features\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\",  CatBoostRegressor(**best_params, random_state=7))\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "used_feats = X_train.columns[pipe.named_steps['model'].feature_importances_ > 0]\n",
    "pipe.fit(X_train[used_feats], y_train)\n",
    "\n",
    "\n",
    "# Predict & compute metrics\n",
    "\n",
    "y_pred  = pipe.predict(X_test[used_feats])\n",
    "\n",
    "mae     = mean_absolute_error(y_test, y_pred)\n",
    "rmse    = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2      = r2_score(y_test, y_pred)\n",
    "weights = np.where(test['holiday_name'].notnull(), 5, 1)\n",
    "wmae    = (weights * np.abs(y_test - y_pred)).sum() / weights.sum()\n",
    "\n",
    "\n",
    "row = {\n",
    "    \"model_name\": 'CatBoost01',\n",
    "    **best_params,\n",
    "    \"mae\":  mae,\n",
    "    \"rmse\": rmse,\n",
    "    \"r2\":   r2,\n",
    "    \"wmae\": wmae\n",
    "}\n",
    "df_best_catboost = pd.DataFrame([row])\n",
    "\n",
    "df_best_catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_best_models.to_csv('csv_files/ml_train_data/df_best_models.csv', index=None)\n",
    "results_df = pd.concat([df_best_catboost, df_best_models], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab424110",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_best_models = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_best_models.to_csv('csv_files/ml_train_data/df_best_models.csv', index=None)\n",
    "df_best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de487076",
   "metadata": {},
   "source": [
    "### Random Forest Regressor Hyper Parameters Search / CV Estimate (used to choose hyper-parameters faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Define a Random Forest pipeline factory\n",
    "def rf_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", RandomForestRegressor(**params, random_state=7, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "# Updated RF hyperparameter grid (dropping 'auto' and using 1.0 instead)\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = rf_pipeline_factory, \n",
    "    search        = 'random',\n",
    "    param_grid    = {\n",
    "        'n_estimators':       [100, 200, 300, 400, 500],\n",
    "        'max_depth':          [None, 10, 20, 30, 40],\n",
    "        'max_features':       [1.0, 'sqrt', 'log2', 0.5],  # replaced 'auto' with 1.0\n",
    "        'min_samples_split':  [2, 5, 10],\n",
    "        'min_samples_leaf':   [1, 2, 4],\n",
    "        'bootstrap':          [True, False]\n",
    "    },\n",
    "    n_iter           = 100,\n",
    "    cv_splitter      = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics   = {'wmae': wmae_custom},\n",
    "    log_path         = 'csv_files/ml_train_data/rf_random_wmae01.csv',\n",
    "    model_name       = 'RF_WMAE',\n",
    "    problem_type     = 'reg',\n",
    "    n_jobs           = -1,\n",
    "    random_state     = 7\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best params:\", best_params)\n",
    "print(\" Best WMAE:  \", best_score)\n",
    "\n",
    "# Clean up\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_random_wmae01 = pd.read_csv('csv_files/ml_train_data/rf_random_wmae01.csv')\n",
    "print(rf_random_wmae01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1117cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "df = rf_random_wmae01.copy()             \n",
    "\n",
    "params_df = (\n",
    "    df['param_json']\n",
    "    .apply(lambda s: json.loads(s) if pd.notnull(s) else {})  \n",
    "    .apply(pd.Series)                                         \n",
    ")\n",
    "df = pd.concat([df, params_df], axis=1)\n",
    "\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name', 'param_json',            \n",
    "          'mae', 'rmse', 'r2', metric, '__source__'}\n",
    "\n",
    "param_cols = [c for c in df.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyper-params\n",
    "X = df[param_cols].select_dtypes(include=[np.number])\n",
    "y = df[metric]\n",
    "\n",
    "# guard against “empty X” (all NaNs / non-numeric)\n",
    "if X.shape[1] == 0:\n",
    "    raise ValueError(\"No numeric hyper-parameter columns left after filtering!\")\n",
    "\n",
    "# urrogate model + analyses \n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial-dependence plots \n",
    "top = importances.sort_values(ascending=False).index[:]\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "for ax in axes[len(top):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e292ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Define a Random Forest pipeline factory\n",
    "def rf_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", RandomForestRegressor(**params, random_state=7, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "# Updated RF hyperparameter grid (dropping 'auto' and using 1.0 instead)\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = rf_pipeline_factory, \n",
    "    search        = 'random',\n",
    "    param_grid    = {\n",
    "        'n_estimators':       [200, 400],\n",
    "        'max_depth':          [20, 40, 60],\n",
    "        'max_features':       [1.0, 0.5], \n",
    "        'min_samples_split':  [0.1,0.5, 2],\n",
    "        'min_samples_leaf':   [4, 8, 16],\n",
    "        'bootstrap':          [True, False]\n",
    "    },\n",
    "    n_iter           = 100,\n",
    "    cv_splitter      = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics   = {'wmae': wmae_custom},\n",
    "    log_path         = 'csv_files/ml_train_data/rf_random_wmae02.csv',\n",
    "    model_name       = 'RF_WMAE',\n",
    "    problem_type     = 'reg',\n",
    "    n_jobs           = -1,\n",
    "    random_state     = 7\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best params:\", best_params)\n",
    "print(\" Best WMAE:  \", best_score)\n",
    "\n",
    "# Clean up\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# load & explode both CSVs if you haven’t yet —\n",
    "paths = [\n",
    "    'csv_files/ml_train_data/rf_random_wmae01.csv',\n",
    "    'csv_files/ml_train_data/rf_random_wmae02.csv',\n",
    "]\n",
    "dfs = []\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    params = df['param_json'].apply(json.loads).apply(pd.Series)\n",
    "    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n",
    "    df['__source__'] = path.split('/')[-1]\n",
    "    dfs.append(df)\n",
    "df_all = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45193a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# choose metric & hyper‐param cols, drop any non‐numeric ones —\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name','mae','rmse','r2', metric, '__source__'}\n",
    "param_cols = [c for c in df_all.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyperparams\n",
    "X = df_all[param_cols].select_dtypes(include=[np.number])\n",
    "y = df_all[metric]\n",
    "\n",
    "# train surrogate —\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# RF importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# permutation importances\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial dependence plots for top-3 features, all in one figure —\n",
    "top = importances.sort_values(ascending=False).index[:].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "# turn off the last (unused) subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Define a Random Forest pipeline factory\n",
    "def rf_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", RandomForestRegressor(**params, random_state=7, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "# Updated RF hyperparameter grid (dropping 'auto' and using 1.0 instead)\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = rf_pipeline_factory, \n",
    "    search        = 'random',\n",
    "    param_grid    = {\n",
    "        'n_estimators':       [200, 400],\n",
    "        'max_depth':          [20],\n",
    "        'max_features':       [1.0, 0.5], \n",
    "        'min_samples_split':  [2],\n",
    "        'min_samples_leaf':   [4],\n",
    "        'bootstrap':          [False]\n",
    "    },\n",
    "    n_iter           = 100,\n",
    "    cv_splitter      = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics   = {'wmae': wmae_custom},\n",
    "    log_path         = 'csv_files/ml_train_data/rf_random_wmae03.csv',\n",
    "    model_name       = 'RF_WMAE',\n",
    "    problem_type     = 'reg',\n",
    "    n_jobs           = -1,\n",
    "    random_state     = 7\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best params:\", best_params)\n",
    "print(\" Best WMAE:  \", best_score)\n",
    "\n",
    "# Clean up\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de545d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# load & explode both CSVs if you haven’t yet —\n",
    "paths = [\n",
    "    'csv_files/ml_train_data/rf_random_wmae01.csv',\n",
    "    'csv_files/ml_train_data/rf_random_wmae02.csv',\n",
    "    'csv_files/ml_train_data/rf_random_wmae03.csv',\n",
    "]\n",
    "dfs = []\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    params = df['param_json'].apply(json.loads).apply(pd.Series)\n",
    "    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n",
    "    df['__source__'] = path.split('/')[-1]\n",
    "    dfs.append(df)\n",
    "df_all = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# choose metric & hyper‐param cols, drop any non‐numeric ones —\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name','mae','rmse','r2', metric, '__source__'}\n",
    "param_cols = [c for c in df_all.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyperparams\n",
    "X = df_all[param_cols].select_dtypes(include=[np.number])\n",
    "y = df_all[metric]\n",
    "\n",
    "# train surrogate —\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# RF importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# permutation importances\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial dependence plots for top-3 features, all in one figure —\n",
    "top = importances.sort_values(ascending=False).index[:].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "# turn off the last (unused) subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837700cf",
   "metadata": {},
   "source": [
    "##### Random Forest Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb18396",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "df_all.to_csv('csv_files/ml_train_data/df_rf_random03.csv', index=None)\n",
    "df = pd.read_csv('csv_files/ml_train_data/df_rf_random03.csv')\n",
    "# pick the row with the lowest WMAE\n",
    "best_row = df.loc[df['wmae'].idxmin()]\n",
    "\n",
    "# keep only the hyper-parameter columns\n",
    "non_params = {'model_name', 'mae', 'rmse', 'r2', 'wmae', '__source__'}\n",
    "param_series = best_row.drop(labels=non_params)\n",
    "\n",
    "# convert to a clean dict, turning floats like 500.0 → 500\n",
    "best_params = {\n",
    "    k: (int(v) if isinstance(v, (int, float)) and not math.isnan(v) and v.is_integer() else v)\n",
    "    for k, v in param_series.items()\n",
    "    if pd.notnull(v)                           # skip NaNs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35fb74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "best_row = df.loc[df['wmae'].idxmin()]\n",
    "\n",
    "# Drop non‐hyperparameter cols\n",
    "non_params = {'model_name','mae','rmse','r2','wmae','__source__'}\n",
    "param_series = best_row.drop(labels=non_params)\n",
    "\n",
    "# Clean & cast params\n",
    "best_params = {}\n",
    "for k, v in param_series.items():\n",
    "    if pd.isnull(v):\n",
    "        continue\n",
    "    # try to interpret strings as numbers\n",
    "    if isinstance(v, str):\n",
    "        try:\n",
    "            num = float(v)\n",
    "            # if it's an integer value, cast to int\n",
    "            v = int(num) if num.is_integer() else num\n",
    "        except ValueError:\n",
    "            # not a number → leave as string\n",
    "            pass\n",
    "    # if it's a float that is whole, cast to int\n",
    "    if isinstance(v, float) and v.is_integer():\n",
    "        v = int(v)\n",
    "    best_params[k] = v\n",
    "\n",
    "print(\"Reconstructed params:\", best_params)\n",
    "\n",
    "# Rebuild & fit your pipeline\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", RandomForestRegressor(**best_params, random_state=7, n_jobs=-1))\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = pipe.predict(X_test)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "# Compute WMAE (holiday weeks weight=5)\n",
    "weights = np.where(test['holiday_name'].notnull(), 5, 1)\n",
    "wmae    = (weights * np.abs(y_test - y_pred)).sum() / weights.sum()\n",
    "\n",
    "print(f\"Test MAE:   {mae:.4f}\")\n",
    "print(f\"Test RMSE:  {rmse:.4f}\")\n",
    "print(f\"Test R²:    {r2:.4f}\")\n",
    "print(f\"Test WMAE:  {wmae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# grab the fitted RF from your pipeline\n",
    "model = pipe.named_steps['model']\n",
    "\n",
    "# make a Series of the importances\n",
    "fi = pd.Series(\n",
    "    model.feature_importances_,\n",
    "    index = X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# print the top 10\n",
    "print(\"🌳 RF split-gain importances:\")\n",
    "display(fi)\n",
    "\n",
    "# number of times the feature was used by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "cutoff  = df_merged.date.max() - pd.Timedelta(weeks=52)\n",
    "train   = df_merged[df_merged.date <= cutoff]\n",
    "test    = df_merged[df_merged.date >  cutoff]\n",
    "\n",
    "drop = [\n",
    "    'weekly_sales','date','holiday_name'\n",
    "]\n",
    "\n",
    "X_train, y_train = train.drop(columns=drop), train.weekly_sales\n",
    "X_test,  y_test  = test .drop(columns=drop),  test .weekly_sales\n",
    "\n",
    "\n",
    "# Fit model, keep only used features\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\",  RandomForestRegressor(**best_params, random_state=7))\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "used_feats = X_train.columns[pipe.named_steps['model'].feature_importances_ > 0]\n",
    "pipe.fit(X_train[used_feats], y_train)\n",
    "\n",
    "\n",
    "# Predict & compute metrics\n",
    "\n",
    "y_pred  = pipe.predict(X_test[used_feats])\n",
    "\n",
    "mae     = mean_absolute_error(y_test, y_pred)\n",
    "rmse    = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2      = r2_score(y_test, y_pred)\n",
    "weights = np.where(test['holiday_name'].notnull(), 5, 1)\n",
    "wmae    = (weights * np.abs(y_test - y_pred)).sum() / weights.sum()\n",
    "\n",
    "\n",
    "row = {\n",
    "    \"model_name\": 'RandomForest01',\n",
    "    **best_params,\n",
    "    \"mae\":  mae,\n",
    "    \"rmse\": rmse,\n",
    "    \"r2\":   r2,\n",
    "    \"wmae\": wmae\n",
    "}\n",
    "df_best_randomforest = pd.DataFrame([row])\n",
    "\n",
    "df_best_randomforest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16031a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# to merge new best model : \n",
    "df_best_models = pd.read_csv('csv_files/ml_train_data/df_best_models.csv')\n",
    "results_df = pd.concat([df_best_randomforest, df_best_models], ignore_index=True)\n",
    "df_best_models = results_df\n",
    "df_best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c2927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_models.to_csv('csv_files/ml_train_data/df_best_models.csv', index=None)\n",
    "df_best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262e7937",
   "metadata": {},
   "source": [
    "### Histogram-Based Gradient Boosting  Hyper Parameters Search / CV Estimate (used to choose hyper-parameters faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Pipeline factory for Histogram-Based Gradient Boosting\n",
    "def hgb_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", HistGradientBoostingRegressor(**params, random_state=7))\n",
    "    ])\n",
    "\n",
    "# 2) Build a “wide” param‐grid, but drop 'poisson' to avoid the non-negativity requirement error\n",
    "hgb_param_grid = {\n",
    "    # loss functions (no more 'poisson' for now)\n",
    "    \"loss\":                 [\"squared_error\", \"absolute_error\"],\n",
    "    # number of boosting iterations\n",
    "    \"max_iter\":             [100, 200, 300, 500],\n",
    "    # step size shrinkage\n",
    "    \"learning_rate\":        [0.01, 0.05, 0.1, 0.2],\n",
    "    # maximum number of leaves per tree\n",
    "    \"max_leaf_nodes\":       [31, 63, 127, None],\n",
    "    # maximum depth of each tree (None = unlimited, bounded by leaf‐nodes)\n",
    "    \"max_depth\":            [None, 3, 5, 10],\n",
    "    # minimum number of samples in each leaf\n",
    "    \"min_samples_leaf\":     [1, 10, 20, 50],\n",
    "    # regularization strength (L2 penalty)\n",
    "    \"l2_regularization\":    [0.0, 1.0, 10.0],\n",
    "    # number of bins used to bucket continuous features\n",
    "    \"max_bins\":             [100, 150, 255],\n",
    "    # tolerance for early stopping (only applies if early_stopping=True)\n",
    "    \"tol\":                  [1e-7, 1e-5, 1e-3],\n",
    "    # number of rounds with no improvement to wait before stopping\n",
    "    \"n_iter_no_change\":     [5, 10, 20],\n",
    "    # whether to use early stopping on a validation split\n",
    "    \"early_stopping\":       [True, False],\n",
    "    # fraction of data to use for the internal validation set (if early_stopping=True)\n",
    "    \"validation_fraction\":  [0.05, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "# Set up the ModelTrainer for Randomized Search\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = hgb_pipeline_factory,\n",
    "    search        = 'random',\n",
    "    param_grid    = hgb_param_grid,\n",
    "    n_iter        = 50,\n",
    "    cv_splitter   = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics= {'wmae': wmae_custom},\n",
    "    log_path      = 'csv_files/ml_train_data/hgb_random_wmae01.csv',\n",
    "    model_name    = 'HGB_WMAE',\n",
    "    problem_type  = 'reg',\n",
    "    n_jobs        = -1,\n",
    "    random_state  = 7\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best params:\", best_params)\n",
    "print(\"Best WMAE:  \", best_score)\n",
    "\n",
    "# Clean up\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "hgb_random_wmae01 = pd.read_csv('csv_files/ml_train_data/hgb_random_wmae01.csv')\n",
    "hgb_random_wmae01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4342ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "df = hgb_random_wmae01.copy()             \n",
    "\n",
    "params_df = (\n",
    "    df['param_json']\n",
    "    .apply(lambda s: json.loads(s) if pd.notnull(s) else {})  \n",
    "    .apply(pd.Series)                                         \n",
    ")\n",
    "df = pd.concat([df, params_df], axis=1)\n",
    "\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name', 'param_json',            \n",
    "          'mae', 'rmse', 'r2', metric, '__source__'}\n",
    "\n",
    "param_cols = [c for c in df.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyper-params\n",
    "X = df[param_cols].select_dtypes(include=[np.number])\n",
    "y = df[metric]\n",
    "\n",
    "# guard against “empty X” (all NaNs / non-numeric)\n",
    "if X.shape[1] == 0:\n",
    "    raise ValueError(\"No numeric hyper-parameter columns left after filtering!\")\n",
    "\n",
    "# urrogate model + analyses \n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial-dependence plots \n",
    "top = importances.sort_values(ascending=False).index[:]\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "for ax in axes[len(top):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b88c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Pipeline factory for Histogram-Based Gradient Boosting\n",
    "def hgb_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", HistGradientBoostingRegressor(**params, random_state=7))\n",
    "    ])\n",
    "\n",
    "# 2) Build a “wide” param‐grid, but drop 'poisson' to avoid the non-negativity requirement error\n",
    "hgb_param_grid = {\n",
    "    # loss functions (no more 'poisson' for now)\n",
    "    \"loss\":                 [\"absolute_error\"],\n",
    "    # number of boosting iterations\n",
    "    \"max_iter\":             [300, 500, 1000],\n",
    "    # step size shrinkage\n",
    "    \"learning_rate\":        [0.05,0.01, 0.4],\n",
    "    # maximum number of leaves per tree\n",
    "    \"max_leaf_nodes\":       [None],\n",
    "    # maximum depth of each tree (None = unlimited, bounded by leaf‐nodes)\n",
    "    \"max_depth\":            [5, 10],\n",
    "    # minimum number of samples in each leaf\n",
    "    \"min_samples_leaf\":     [20,100],\n",
    "    # regularization strength (L2 penalty)\n",
    "    \"l2_regularization\":    [10.0, 20.0],\n",
    "    # number of bins used to bucket continuous features\n",
    "    \"max_bins\":             [255],\n",
    "    # tolerance for early stopping (only applies if early_stopping=True)\n",
    "    \"tol\":                  [1e-3, 1e-2],\n",
    "    # number of rounds with no improvement to wait before stopping\n",
    "    \"n_iter_no_change\":     [2, 5],\n",
    "    # whether to use early stopping on a validation split\n",
    "    \"early_stopping\":       [False],\n",
    "    # fraction of data to use for the internal validation set (if early_stopping=True)\n",
    "    \"validation_fraction\":  [0.05, 0.2],\n",
    "}\n",
    "\n",
    "# Set up the ModelTrainer for Randomized Search\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = hgb_pipeline_factory,\n",
    "    search        = 'random',\n",
    "    param_grid    = hgb_param_grid,\n",
    "    n_iter        = 50,\n",
    "    cv_splitter   = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics= {'wmae': wmae_custom},\n",
    "    log_path      = 'csv_files/ml_train_data/hgb_random_wmae02.csv',\n",
    "    model_name    = 'HGB_WMAE',\n",
    "    problem_type  = 'reg',\n",
    "    n_jobs        = -1,\n",
    "    random_state  = 7\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best params:\", best_params)\n",
    "print(\"Best WMAE:  \", best_score)\n",
    "\n",
    "# Clean up\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee5aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load & explode both CSVs if you haven’t yet —\n",
    "paths = [\n",
    "    'csv_files/ml_train_data/hgb_random_wmae01.csv',\n",
    "    'csv_files/ml_train_data/hgb_random_wmae02.csv',\n",
    "]\n",
    "dfs = []\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    params = df['param_json'].apply(json.loads).apply(pd.Series)\n",
    "    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n",
    "    df['__source__'] = path.split('/')[-1]\n",
    "    dfs.append(df)\n",
    "df_all = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# choose metric & hyper‐param cols, drop any non‐numeric ones —\n",
    "metric = 'wmae'\n",
    "ignore = {'model_name','mae','rmse','r2', metric, '__source__'}\n",
    "param_cols = [c for c in df_all.columns if c not in ignore]\n",
    "\n",
    "# keep only numeric hyperparams\n",
    "X = df_all[param_cols].select_dtypes(include=[np.number])\n",
    "y = df_all[metric]\n",
    "\n",
    "# train surrogate —\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# RF importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# permutation importances\n",
    "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=X.columns)\n",
    "print(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# partial dependence plots for top-3 features, all in one figure —\n",
    "top = importances.sort_values(ascending=False).index[:].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat in zip(axes, top):\n",
    "    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n",
    "    ax.set_title(f\"PDP: {feat}\")\n",
    "\n",
    "# turn off the last (unused) subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11cee78",
   "metadata": {},
   "source": [
    "#### HBGB Halving Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "ba1f0ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HGB_WMAE halving (300 iters):   0%|          | 0/2 [00:09<?, ?combo/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[263], line 39\u001b[0m\n\u001b[0;32m      9\u001b[0m hgb_param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m:                 [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute_error\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m:             [\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m1000\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_fraction\u001b[39m\u001b[38;5;124m\"\u001b[39m:  [\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.2\u001b[39m],\n\u001b[0;32m     22\u001b[0m }\n\u001b[0;32m     24\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(\n\u001b[0;32m     25\u001b[0m     X             \u001b[38;5;241m=\u001b[39m X_train,\n\u001b[0;32m     26\u001b[0m     y             \u001b[38;5;241m=\u001b[39m y_train,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     random_state  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[0;32m     38\u001b[0m )\n\u001b[1;32m---> 39\u001b[0m best_params, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Run the halving search\u001b[39;00m\n\u001b[0;32m     43\u001b[0m best_params, best_score \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\utils\\trainer.py:130\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# handle halving search separately\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhalving\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_halving\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_list)\n\u001b[0;32m    133\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_list,\n\u001b[0;32m    134\u001b[0m                 desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m search\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    135\u001b[0m                 unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\utils\\trainer.py:245\u001b[0m, in \u001b[0;36mModelTrainer._train_halving\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m X_val, y_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39miloc[val_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[0;32m    244\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_factory(params_full)\n\u001b[1;32m--> 245\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m    247\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics[prim](y_val, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\sklearn\\pipeline.py:661\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    656\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    657\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    658\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    659\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    660\u001b[0m         )\n\u001b[1;32m--> 661\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:954\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, X_val, y_val, sample_weight_val)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trees_per_iteration_):\n\u001b[0;32m    935\u001b[0m     grower \u001b[38;5;241m=\u001b[39m TreeGrower(\n\u001b[0;32m    936\u001b[0m         X_binned\u001b[38;5;241m=\u001b[39mX_binned_train,\n\u001b[0;32m    937\u001b[0m         gradients\u001b[38;5;241m=\u001b[39mg_view[:, k],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    952\u001b[0m         n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m    953\u001b[0m     )\n\u001b[1;32m--> 954\u001b[0m     \u001b[43mgrower\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m     acc_apply_split_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m grower\u001b[38;5;241m.\u001b[39mtotal_apply_split_time\n\u001b[0;32m    957\u001b[0m     acc_find_split_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m grower\u001b[38;5;241m.\u001b[39mtotal_find_split_time\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py:387\u001b[0m, in \u001b[0;36mTreeGrower.grow\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Grow the tree, from root to leaves.\"\"\"\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplittable_nodes:\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_shrinkage()\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py:638\u001b[0m, in \u001b[0;36mTreeGrower.split_next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_best_split_and_push(left_child_node)\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_split_right:\n\u001b[1;32m--> 638\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_best_split_and_push\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_child_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_find_split_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m tic\n\u001b[0;32m    641\u001b[0m \u001b[38;5;66;03m# Release memory used by histograms as they are no longer needed\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;66;03m# for leaf nodes since they won't be split.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py:471\u001b[0m, in \u001b[0;36mTreeGrower._compute_best_split_and_push\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compute_best_split_and_push\u001b[39m(\u001b[38;5;28mself\u001b[39m, node):\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the best possible split (SplitInfo) of a given node.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m    Also push it in the heap of splittable nodes if gain isn't zero.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    (min_hessians_to_split, min_gain_to_split, min_samples_leaf)\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     node\u001b[38;5;241m.\u001b[39msplit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_node_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistograms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistograms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43msum_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43msum_hessians\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum_hessians\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallowed_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallowed_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39msplit_info\u001b[38;5;241m.\u001b[39mgain \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# no valid split\u001b[39;00m\n\u001b[0;32m    483\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize_leaf(node)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Pipeline factory for Histogram-Based Gradient Boosting\n",
    "def hgb_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", HistGradientBoostingRegressor(**params, random_state=7))\n",
    "    ])\n",
    "\n",
    "# 2) Build a “wide” param‐grid (same as before)\n",
    "hgb_param_grid = {\n",
    "    \"loss\":                 [\"absolute_error\"],\n",
    "    \"max_iter\":             [300, 500, 1000],\n",
    "    \"learning_rate\":        [0.05, 0.01, 0.4],\n",
    "    \"max_leaf_nodes\":       [None],\n",
    "    \"max_depth\":            [5, 10],\n",
    "    \"min_samples_leaf\":     [20, 100],\n",
    "    \"l2_regularization\":    [10.0, 20.0],\n",
    "    \"max_bins\":             [255],\n",
    "    \"tol\":                  [1e-3, 1e-2],\n",
    "    \"n_iter_no_change\":     [2, 5],\n",
    "    \"early_stopping\":       [True],\n",
    "    \"validation_fraction\":  [0.05, 0.2],\n",
    "}\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = hgb_pipeline_factory,\n",
    "    search        = 'halving',      # now recognized!\n",
    "    param_grid    = hgb_param_grid,\n",
    "    n_iter        = 2,             # ignored by halving\n",
    "    cv_splitter   = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics= {'wmae': wmae_custom},\n",
    "    log_path      = 'csv_files/ml_train_data/hgb_halving_wmae03.csv',\n",
    "    model_name    = 'HGB_WMAE',\n",
    "    problem_type  = 'reg',\n",
    "    n_jobs        = -1,\n",
    "    random_state  = 7\n",
    ")\n",
    "best_params, best_score = trainer.train()\n",
    "\n",
    "\n",
    "# Run the halving search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best params:\", best_params)\n",
    "print(\"Best WMAE:  \", best_score)\n",
    "\n",
    "# Clean up\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "bbf12acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\estif\\AppData\\Local\\Temp\\ipykernel_26856\\3725908297.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all = pd.concat(dfs, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load & explode both CSVs if you haven’t yet —\n",
    "paths = [\n",
    "    'csv_files/ml_train_data/hgb_random_wmae01.csv',\n",
    "    'csv_files/ml_train_data/hgb_random_wmae02.csv',\n",
    "    'csv_files/ml_train_data/hgb_halving_wmae03.csv',\n",
    "]\n",
    "dfs = []\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    params = df['param_json'].apply(json.loads).apply(pd.Series)\n",
    "    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n",
    "    df['__source__'] = path.split('/')[-1]\n",
    "    dfs.append(df)\n",
    "df_all = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "cf1e23e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "param_json",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mae",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rmse",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "r2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "wmae",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "981221e0-4957-432b-a49c-f22ce8e85ab9",
       "rows": [],
       "shape": {
        "columns": 6,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>param_json</th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "      <th>wmae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_name, param_json, mae, rmse, r2, wmae]\n",
       "Index: []"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgb_halving_wmae03 = pd.read_csv('csv_files/ml_train_data/hgb_halving_wmae03.csv')\n",
    "hgb_halving_wmae03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc8038",
   "metadata": {},
   "source": [
    "### ExtraTreesRegressor  Hyper Parameters Search / CV Estimate (used to choose hyper-parameters faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "3871ae07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ET_WMAE halving (100 iters):   0%|          | 0/20 [00:05<?, ?combo/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[262], line 42\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 3) Run halving with the updated grid\u001b[39;00m\n\u001b[0;32m     27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(\n\u001b[0;32m     28\u001b[0m     X             \u001b[38;5;241m=\u001b[39m X_train,\n\u001b[0;32m     29\u001b[0m     y             \u001b[38;5;241m=\u001b[39m y_train,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     random_state  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[0;32m     40\u001b[0m )\n\u001b[1;32m---> 42\u001b[0m best_params, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest WMAE:  \u001b[39m\u001b[38;5;124m\"\u001b[39m, best_score)\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\utils\\trainer.py:130\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# handle halving search separately\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhalving\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_halving\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_list)\n\u001b[0;32m    133\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_list,\n\u001b[0;32m    134\u001b[0m                 desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m search\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    135\u001b[0m                 unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\utils\\trainer.py:245\u001b[0m, in \u001b[0;36mModelTrainer._train_halving\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m X_val, y_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39miloc[val_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[0;32m    244\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_factory(params_full)\n\u001b[1;32m--> 245\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m    247\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics[prim](y_val, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\sklearn\\pipeline.py:661\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    656\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    657\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    658\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    659\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    660\u001b[0m         )\n\u001b[1;32m--> 661\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    475\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    478\u001b[0m ]\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 486\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\estif\\Desktop\\DataAnalyst\\_DataCourse_IronHack\\Quests\\ml_walmart_price\\venv\\lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1) Pipeline factory that remaps 'max_iter'→'n_estimators'\n",
    "def et_pipeline_factory(params):\n",
    "    p = params.copy()\n",
    "    if \"max_iter\" in p:\n",
    "        p[\"n_estimators\"] = p.pop(\"max_iter\")\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", ExtraTreesRegressor(**p, random_state=7, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "# 2) Halving‐friendly grid (no 'poisson')\n",
    "et_param_grid_halving = {\n",
    "    \"max_iter\":             [100, 200, 300, 500, 800],  # resource budgets\n",
    "    \"criterion\":            [\"squared_error\", \"absolute_error\"],  # dropped 'poisson'\n",
    "    \"max_depth\":            [None, 10, 20, 30, 40],\n",
    "    \"max_features\":         [\"sqrt\", \"log2\", 0.5, 1.0],\n",
    "    \"min_samples_split\":    [2, 5, 10, 0.01, 0.05, 0.1],\n",
    "    \"min_samples_leaf\":     [1, 2, 4, 10, 20, 50],\n",
    "    \"min_weight_fraction_leaf\": [0.0, 0.01, 0.05],\n",
    "    \"max_leaf_nodes\":       [None, 10, 30, 50, 100],\n",
    "    \"min_impurity_decrease\":[0.0, 1e-7, 1e-5],\n",
    "    \"ccp_alpha\":            [0.0, 0.001, 0.01, 0.1],\n",
    "    \"bootstrap\":            [True, False]\n",
    "}\n",
    "\n",
    "# 3) Run halving with the updated grid\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = et_pipeline_factory,\n",
    "    search        = 'halving',                   \n",
    "    param_grid    = et_param_grid_halving,      \n",
    "    cv_splitter   = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics= {'wmae': wmae_custom},\n",
    "    log_path      = 'csv_files/ml_train_data/et_halving_wmae01.csv',\n",
    "    model_name    = 'ET_WMAE',\n",
    "    problem_type  = 'reg',\n",
    "    n_jobs        = -1,\n",
    "    random_state  = 7\n",
    ")\n",
    "\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best params:\", best_params)\n",
    "print(\"Best WMAE:  \", best_score)\n",
    "\n",
    "# Clean up\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081828e",
   "metadata": {},
   "source": [
    "### NGBoost (Natural Gradient Boosting)  Hyper Parameters Search / CV Estimate (used to choose hyper-parameters faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.distns import Normal, Laplace, StudentT\n",
    "\n",
    "# 1) Pipeline factory for NGBoost\n",
    "def ngb_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", NGBRegressor(**params, random_state=7))\n",
    "    ])\n",
    "\n",
    "# 2) A broad hyperparameter grid for randomized search\n",
    "ngb_param_grid = {\n",
    "    # number of boosting rounds\n",
    "    \"n_estimators\":            [100, 200, 500, 1000],\n",
    "    # shrinkage / step size\n",
    "    \"learning_rate\":           [0.01, 0.05, 0.1, 0.2],\n",
    "    # choice of predictive distribution\n",
    "    \"Dist\":                    [Normal, Laplace, StudentT],\n",
    "    # whether to use natural gradient updates\n",
    "    \"natural_gradient\":        [True, False],\n",
    "    # fraction of data to subsample for each update\n",
    "    \"minibatch_frac\":          [0.5, 1.0],\n",
    "    # fraction of features to subsample for each tree\n",
    "    \"col_sample\":              [0.5, 1.0],\n",
    "    # stopping tolerance (None disables early stopping)\n",
    "    \"tol\":                     [None, 1e-4, 1e-3],\n",
    "    # tree‐specific parameters passed to the default base learner\n",
    "    \"Base__max_depth\":         [3, 5, 10],\n",
    "    \"Base__min_samples_leaf\":  [1, 5, 10],\n",
    "}\n",
    "\n",
    "# 3) Set up the ModelTrainer for randomized search\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = ngb_pipeline_factory,\n",
    "    search        = 'random',\n",
    "    param_grid    = ngb_param_grid,\n",
    "    n_iter        = 100,\n",
    "    cv_splitter   = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics= {'wmae': wmae_custom},\n",
    "    log_path      = 'csv_files/ml_train_data/ngb_random_wmae.csv',\n",
    "    model_name    = 'NGB_WMAE',\n",
    "    problem_type  = 'reg',\n",
    "    n_jobs        = -1,           # NGBoost itself parallelizes tree fits internally\n",
    "    random_state  = 7\n",
    ")\n",
    "\n",
    "# 4) Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best params:\", best_params)\n",
    "print(\" Best WMAE:  \", best_score)\n",
    "\n",
    "# 5) Clean up\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699df851",
   "metadata": {},
   "source": [
    "### HuberRegressor (Robust Linear Model)  Hyper Parameters Search / CV Estimate (used to choose hyper-parameters faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cdd5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# 1) Pipeline factory for HuberRegressor\n",
    "def huber_pipeline_factory(params):\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", HuberRegressor(**params))\n",
    "    ])\n",
    "\n",
    "# 2) Broad hyperparameter grid for randomized search\n",
    "huber_param_grid = {\n",
    "    # The epsilon parameter for the Huber loss threshold\n",
    "    \"epsilon\":          [1.1, 1.35, 1.5, 1.75, 2.0],\n",
    "    # Regularization strength (inverse of C in SVM terms)\n",
    "    \"alpha\":            [1e-4, 1e-3, 1e-2, 1e-1, 1.0],\n",
    "    # Maximum number of iterations for the solver\n",
    "    \"max_iter\":         [100, 500, 1000],\n",
    "    # Tolerance for optimization convergence\n",
    "    \"tol\":              [1e-4, 1e-3, 1e-2],\n",
    "    # Whether to calculate the intercept for this model\n",
    "    \"fit_intercept\":    [True, False],\n",
    "    # Whether to reuse the solution of the previous call to fit as initialization\n",
    "    \"warm_start\":       [False, True]\n",
    "}\n",
    "\n",
    "# 3) Set up the ModelTrainer for randomized search\n",
    "trainer = ModelTrainer(\n",
    "    X             = X_train,\n",
    "    y             = y_train,\n",
    "    model_factory = huber_pipeline_factory,\n",
    "    search        = 'random',\n",
    "    param_grid    = huber_param_grid,\n",
    "    n_iter        = 100,\n",
    "    cv_splitter   = TimeSeriesSplit(n_splits=3),\n",
    "    custom_metrics= {'wmae': wmae_custom},\n",
    "    log_path      = 'csv_files/ml_train_data/huber_random_wmae.csv',\n",
    "    model_name    = 'HUBER_WMAE',\n",
    "    problem_type  = 'reg',\n",
    "    n_jobs        = -1,       # parallelism handled by sklearn where applicable\n",
    "    random_state  = 7\n",
    ")\n",
    "\n",
    "# 4) Run the search\n",
    "best_params, best_score = trainer.train()\n",
    "print(\"Best params:\", best_params)\n",
    "print(\"Best WMAE:   \", best_score)\n",
    "\n",
    "# 5) Clean up\n",
    "trainer.csv_file.close()\n",
    "del trainer\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fb80d",
   "metadata": {},
   "source": [
    "### Meta Ensemble  Hyper Parameters Search / CV Estimate (used to choose hyper-parameters faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "# Import your base‐learner classes:\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# 1) Load your tuned‐params CSV (index should be model_name)\n",
    "best_params_df = pd.read_csv('best_params.csv', index_col='model_name')\n",
    "\n",
    "# 2) Map CSV names → model classes (add new entries here as you add models)\n",
    "model_mapping = {\n",
    "    'CatBoost01': CatBoostRegressor,\n",
    "    'XGBOOST01':  XGBRegressor,\n",
    "    'LGBM01':     LGBMRegressor,\n",
    "    'RF01':       RandomForestRegressor,\n",
    "    'ET01':       ExtraTreesRegressor,\n",
    "    'HGB01':      HistGradientBoostingRegressor,\n",
    "    'NGB01':      NGBRegressor,\n",
    "    'HUBER01':    HuberRegressor,\n",
    "    # 'NEWMODEL':  YourNewRegressorClass,\n",
    "}\n",
    "\n",
    "# 3) Instantiate each base learner with its best params\n",
    "base_estimators = []\n",
    "for name, row in best_params_df.iterrows():\n",
    "    cls = model_mapping.get(name)\n",
    "    if cls is None:\n",
    "        # placeholder slot for future models\n",
    "        continue\n",
    "    # drop any NaNs from your CSV\n",
    "    params = {k: v for k, v in row.to_dict().items() if pd.notna(v)}\n",
    "    # many of these regressors accept random_state; pass it if they do\n",
    "    if 'random_state' in cls().get_params():\n",
    "        params['random_state'] = 7\n",
    "    # Some need n_jobs\n",
    "    if 'n_jobs' in cls().get_params():\n",
    "        params['n_jobs'] = -1\n",
    "    estimator = cls(**params)\n",
    "    base_estimators.append((name.lower(), estimator))\n",
    "\n",
    "# 4) Build the stacking regressor\n",
    "stack = StackingRegressor(\n",
    "    estimators     = base_estimators,\n",
    "    final_estimator= RidgeCV(alphas=[0.1, 1.0, 10.0]),   # simple meta‐learner\n",
    "    cv             = TimeSeriesSplit(n_splits=3),\n",
    "    passthrough    = False,\n",
    "    n_jobs         = -1,\n",
    "    verbose        = 1\n",
    ")\n",
    "\n",
    "# 5) Random‐search **only** the meta‐learner params\n",
    "meta_param_dist = {\n",
    "    # RidgeCV’s alpha grid to search over \n",
    "    \"final_estimator__alphas\": [\n",
    "        [0.01, 0.1, 1.0],\n",
    "        [0.1, 1.0, 10.0],\n",
    "        [1.0, 10.0, 100.0]\n",
    "    ],\n",
    "    # whether to feed original features alongside base‐predictions\n",
    "    \"passthrough\": [True, False],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator        = stack,\n",
    "    param_distributions = meta_param_dist,\n",
    "    n_iter           = 20,\n",
    "    cv               = TimeSeriesSplit(n_splits=3),\n",
    "    scoring          = make_scorer(wmae_custom, greater_is_better=False),\n",
    "    n_jobs           = -1,\n",
    "    random_state     = 7,\n",
    "    verbose          = 2\n",
    ")\n",
    "\n",
    "# 6) Fit & evaluate\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best meta‐params:\", search.best_params_)\n",
    "print(\"Stacked WMAE:\", -search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8777adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7749575,
     "sourceId": 12295466,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7750250,
     "sourceId": 12296537,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
