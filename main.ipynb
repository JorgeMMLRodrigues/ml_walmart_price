{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12295466,"sourceType":"datasetVersion","datasetId":7749575},{"sourceId":12296537,"sourceType":"datasetVersion","datasetId":7750250}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jorgemmlrodrigues/walmart-price-pred?scriptVersionId=247716419\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"id":"6100c02c","cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/jorgemmlrodrigues/walmart-price-pred?scriptVersionId=247661249\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"id":"6701bea9","cell_type":"code","source":"\n#Kaggle\n!git -C /kaggle/working clone --depth 1 https://github.com/JorgeMMLRodrigues/ml_walmart_price.git || \\\n git -C /kaggle/working/ml_walmart_price pull    \n!pip install -q -r /kaggle/working/ml_walmart_price/requirements.txt\n\nimport sys, os\nos.chdir(\"/kaggle/working/ml_walmart_price\")\nsys.path.insert(0, os.getcwd())","metadata":{"execution":{"iopub.status.busy":"2025-06-27T18:25:20.249057Z","iopub.execute_input":"2025-06-27T18:25:20.249354Z","iopub.status.idle":"2025-06-27T18:25:30.32731Z","shell.execute_reply.started":"2025-06-27T18:25:20.249332Z","shell.execute_reply":"2025-06-27T18:25:30.326258Z"},"trusted":true},"outputs":[{"name":"stdout","text":"fatal: destination path 'ml_walmart_price' already exists and is not an empty directory.\nAlready up to date.\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[33mWARNING: akracer 0.0.13 does not provide the extra 'py-mini-racer'\u001b[0m\u001b[33m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for jsonpath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"id":"295d466e-5ee6-4c6d-ba0e-e5007bdbc98d","cell_type":"code","source":"import sys, os\nfrom utils.trainer import ModelTrainer\n\nimport pandas as pd\nimport numpy as np\nfrom dotenv import load_dotenv\nimport random, time, inspect,time,json,os,requests,sys, math\nfrom pathlib import Path\n\nfrom datetime import date, datetime, timedelta\nfrom dateutil.easter import easter\n\nfrom tqdm.notebook import tqdm\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\n\nimport yfinance as yf\n\nimport akshare as ak\n\nfrom pandas_datareader.data import DataReader\nfrom pandas_datareader import wb\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, f1_score, roc_auc_score\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nfrom sklearn.linear_model import RidgeCV, Lasso,ElasticNetCV\nfrom sklearn.model_selection import TimeSeriesSplit,ParameterGrid,GridSearchCV\nimport lightgbm as lgb\n\nfrom sklearn.inspection import PartialDependenceDisplay,permutation_importance\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e83caba5","cell_type":"code","source":"# skip cells:\nfrom IPython.core.magic import register_cell_magic\n\n@register_cell_magic\ndef skip(line, cell):\n\n    return","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b6ecb136","cell_type":"markdown","source":"# Data","metadata":{}},{"id":"73590342","cell_type":"markdown","source":"## Data Exploration","metadata":{}},{"id":"aca79ac9","cell_type":"code","source":"%%skip\ndf_walmart_train = pd.read_csv('csv_files/walmart_data/train.csv')\ndf_walmart_test = pd.read_csv('csv_files/walmart_data/test.csv')\ndf_walmart_features = pd.read_csv('csv_files/walmart_data/features.csv')\ndf_walmart_stores = pd.read_csv('csv_files/walmart_data/stores.csv')\n\ndf_walmart_train[\"Date\"] = pd.to_datetime(df_walmart_train[\"Date\"], errors=\"raise\")\ndf_walmart_test[\"Date\"] = pd.to_datetime(df_walmart_test[\"Date\"], errors=\"raise\")\ndf_walmart_features[\"Date\"] = pd.to_datetime(df_walmart_features[\"Date\"], errors=\"raise\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5231e068","cell_type":"code","source":"%%skip\ndf_walmart_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a576dec3","cell_type":"code","source":"%%skip\ndf_walmart_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9fd3115b","cell_type":"code","source":"%%skip\ndf_walmart_features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ea6bce62","cell_type":"markdown","source":"## Data Gathering","metadata":{}},{"id":"d4045cd2","cell_type":"markdown","source":"#### Sp500","metadata":{}},{"id":"a568aad2","cell_type":"code","source":"%%skip\n# SP500 Index\nSTART_DATE = \"2000-01-01\"\nEND_DATE   = date.today().isoformat()\n\n\ndaily = yf.download(\n    \"^GSPC\",           # S&P 500 index ticker\n    start=START_DATE,\n    end=END_DATE,\n    interval=\"1d\",\n    auto_adjust=True,\n    progress=False,\n)\n\nif isinstance(daily.columns, pd.MultiIndex):\n    lvl0 = daily.columns.get_level_values(0)\n    if \"Close\" in lvl0:\n        close_prices = daily.xs(\"Close\", axis=1, level=0)\n    elif \"Adj Close\" in lvl0:\n        close_prices = daily.xs(\"Adj Close\", axis=1, level=0)\n    else:\n        raise KeyError(\"Neither 'Close' nor 'Adj Close' found in data\")\nelse:\n    if \"Close\" in daily.columns:\n        close_prices = daily[\"Close\"]\n    else:\n        close_prices = daily[\"Adj Close\"]\n\n\nweekly_mean_close = close_prices.resample(\"W-FRI\").mean()\n\nif isinstance(weekly_mean_close, pd.Series):\n    df_sp500 = weekly_mean_close.to_frame(name=\"SPX_Weekly_Mean_Close\")\nelse:\n    df_sp500 = weekly_mean_close.copy()\n    df_sp500.columns = [\"SPX_Weekly_Mean_Close\"]\n\n\nOUTFILE = \"csv_files/idea_csv/sp500_weekly_mean_close.csv\"\ndf_sp500.to_csv(OUTFILE)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6cf15d9d","cell_type":"code","source":"%%skip\ndf_sp500 = pd.read_csv(\"csv_files/idea_csv/sp500_weekly_mean_close.csv\")\ndf_sp500","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"adb5f51d","cell_type":"markdown","source":"#### Walmart Stock Price","metadata":{}},{"id":"f2f4ab5a","cell_type":"code","source":"%%skip\nSTART_DATE = \"2000-01-01\"\nEND_DATE   = date.today().isoformat()\n\n\ndaily = yf.download(\n    \"WMT\",           \n    start=START_DATE,\n    end=END_DATE,\n    interval=\"1d\",\n    auto_adjust=True, \n    progress=False,\n)\n\n\nif isinstance(daily.columns, pd.MultiIndex):\n    lvl0 = daily.columns.get_level_values(0)\n    if \"Close\" in lvl0:\n        close_prices = daily.xs(\"Close\", axis=1, level=0)\n    elif \"Adj Close\" in lvl0:\n        close_prices = daily.xs(\"Adj Close\", axis=1, level=0)\n    else:\n        raise KeyError(\"Neither 'Close' nor 'Adj Close' found in data\")\nelse:\n    if \"Close\" in daily.columns:\n        close_prices = daily[\"Close\"]\n    else:\n        close_prices = daily[\"Adj Close\"]\n\n\nweekly_mean_close = close_prices.resample(\"W-FRI\").mean()\n\n\nif isinstance(weekly_mean_close, pd.Series):\n    df_walmart_stock = weekly_mean_close.to_frame(name=\"WMT_Weekly_Mean_Close\")\nelse:\n    df_walmart_stock = weekly_mean_close.copy()\n    df_walmart_stock.columns = [\"WMT_Weekly_Mean_Close\"]\n\n\nOUTFILE = \"csv_files/idea_csv/wmt_weekly_mean_close.csv\"\ndf_walmart_stock.to_csv(OUTFILE)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"00c9a073","cell_type":"code","source":"%%skip\ndf_walmart_stock = pd.read_csv(\"csv_files/idea_csv/wmt_weekly_mean_close.csv\")\ndf_walmart_stock","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"26393546","cell_type":"markdown","source":"#### External Logistic companies Walmart","metadata":{}},{"id":"813a9bff","cell_type":"code","source":"%%skip\n#   - ARCB: ArcBest Corporation (ABF Logistics / ArcBest Freight)\n#   - AIT: AIT Worldwide Logistics\n#   - CEVA: CEVA Logistics\n#   - DPW.DE: Deutsche Post (DHL Freight / DHL Supply Chain) on XETRA\n#   - FDX: FedEx Corporation (FedEx Freight)\n#   - SAIA: Saia, Inc. (Saia Motor Freight Line)\n#   - TFII.TO: TFI International (TForce Freight) on TSX\n#   - XPO: XPO Logistics, Inc.\n#   - ODFL: Old Dominion Freight Line, Inc.\n#   - UPS: United Parcel Service, Inc.\n#   - JBHT: J.B. Hunt Transport Services, Inc.\n# Note: Some private carriers (Estes Express, R+L Carriers) are not publicly traded.\n\nTICKERS = [\n    \"ARCB\", \"AIT\", \"CEVA\", \"DPW.DE\", \"FDX\",\n    \"SAIA\", \"TFII.TO\", \"XPO\", \"ODFL\", \"UPS\", \"JBHT\"\n]\n\nSTART_DATE = \"2000-01-01\"\nEND_DATE   = date.today().isoformat()\n\ndaily = yf.download(\n    TICKERS,\n    start=START_DATE,\n    end=END_DATE,\n    interval=\"1d\",\n    auto_adjust=True,\n    group_by=\"ticker\",\n    threads=True,\n    progress=True,\n)\n\nclose = pd.DataFrame()\nfor sym in TICKERS:\n    try:\n        series = daily[sym][\"Close\"]\n        close[sym] = series\n    except Exception:\n        print(f\"Skipping {sym!r}: no data available or ticker invalid\")\n\n\nbefore = close.shape[1]\nclose = close.dropna(axis=1, how=\"all\")\nafter = close.shape[1]\nprint(f\"Dropped {before-after} tickers; {after} tickers remain for analysis\")\n\ndf_logistics = close.resample(\"W-FRI\").mean().round(4)\n\ndf_logistics.columns = [f\"{sym}_df_logistics_Close\" for sym in df_logistics.columns]\n\nOUTFILE = \"csv_files/idea_csv/logistics_df_logistics_close.csv\"\ndf_logistics.to_csv(OUTFILE)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d1238b66","cell_type":"code","source":"%%skip\ndf_logistics = pd.read_csv(\"csv_files/idea_csv/logistics_df_logistics_close.csv\")\ndf_logistics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"876333ca","cell_type":"markdown","source":"#### Official China PMI (Caixin PMI only starts in 2014)\n","metadata":{}},{"id":"4c9f73f2","cell_type":"code","source":"%%skip\n# All AkShare functions containing \"pmi\"\n\ndf_official = ak.macro_china_pmi()\n\nprint(\"Columns in df_official:\", df_official.columns.tolist())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3d78a461","cell_type":"code","source":"%%skip\n# 1) Fetch the official China PMI data\ndf_official = ak.macro_china_pmi()\n\n# 2) Rename Chinese column names to English\ndf_official = df_official.rename(columns={\n    \"月份\": \"Date\",\n    \"制造业-指数\": \"Official_Manufacturing_PMI\",\n    \"制造业-同比增长\": \"Official_Manufacturing_PMI_YoY\",\n    \"非制造业-指数\": \"Official_Services_PMI\",\n    \"非制造业-同比增长\": \"Official_Services_PMI_YoY\",\n})\n\n# 3) Clean and parse the 'Date' column (\"YYYY年MM月份\" → \"YYYY-MM\")\ndf_official[\"Date\"] = (\n    df_official[\"Date\"]\n      .str.replace(\"年\", \"-\", regex=False)\n      .str.replace(\"月份\", \"\", regex=False)\n)\ndf_official[\"Date\"] = pd.to_datetime(df_official[\"Date\"], format=\"%Y-%m\")\n\n# 4) Set Date as the index and sort\ndf_official = df_official.set_index(\"Date\").sort_index()\n\n# 5) Subset to the period 2009-01-01 through 2014-12-31\ndf_pmi_china = df_official.loc[\"2009-01-01\":\"2014-12-31\"]\ndf_pmi_china.to_csv(\"csv_files/idea_csv/df_pmi_china.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"352ea0e7","cell_type":"code","source":"%%skip\ndf_pmi_china = pd.read_csv(\"csv_files/idea_csv/df_pmi_china.csv\")\ndf_pmi_china","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1824d1ce","cell_type":"markdown","source":"#### PCE USA (Personal Consumption Expenditures)\n","metadata":{}},{"id":"b5091fcc","cell_type":"code","source":"%%skip\nSTART_DATE = \"2000-01-01\"\nEND_DATE   = date.today().isoformat()\n\ndf_pce = DataReader(\"PCE\", \"fred\", start=START_DATE, end=END_DATE)\n\ndf_pce.columns = [\"Personal_Consumption_Expenditures\"]\n\n# 4) Save and quick sanity-check\nOUTFILE = \"csv_files/idea_csv/personal_consumption_expenditures.csv\"\ndf_pce.to_csv(OUTFILE)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f5374042","cell_type":"code","source":"%%skip\ndf_pce = pd.read_csv(\"csv_files/idea_csv/personal_consumption_expenditures.csv\")\ndf_pce","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c091ad03","cell_type":"markdown","source":"#### Interest Rates USA (Fed Funds Rate & Tbill 3 Months Yield)","metadata":{}},{"id":"06d04fe0","cell_type":"code","source":"%%skip\nSTART = \"2001-06-01\"               \nEND   = date.today().isoformat()\n\n\nfed  = DataReader(\"FEDFUNDS\", \"fred\", START, END)\ntbill = DataReader(\"DGS3MO\",  \"fred\", START, END)\n\ndf_interest_rates = pd.concat([fed, tbill], axis=1).rename(columns={\n    \"FEDFUNDS\": \"Fed_Funds_Rate\",\n    \"DGS3MO\":   \"TBill_3mo_Yield\",\n})\ndf_interest_rates = df_interest_rates.resample(\"W-FRI\").mean()\n\nOUTFILE = \"csv_files/idea_csv/df_interest_rates.csv\"\ndf_interest_rates.to_csv(OUTFILE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"41da9b40","cell_type":"code","source":"%%skip\nf_interest_rates = pd.read_csv(\"csv_files/idea_csv/df_interest_rates.csv\")\ndf_interest_rates","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8907aa88","cell_type":"markdown","source":"#### CCI USA (Consumer Confidence Index) from University of Michigan","metadata":{}},{"id":"d8eff796","cell_type":"code","source":"%%skip\nSTART_DATE = \"2001-06-17\"\nEND_DATE   = date.today().isoformat()\n\ndf_us_cci = DataReader(\"UMCSENT\", \"fred\", START_DATE, END_DATE)\ndf_us_cci.columns = [\"Consumer_Sentiment_UMich\"]\n\nOUTFILE = \"csv_files/idea_csv/consumer_confidence_index.csv\"\ndf_us_cci.to_csv(OUTFILE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"51523471","cell_type":"code","source":"%%skip\ndf_us_cci = pd.read_csv(\"csv_files/idea_csv/consumer_confidence_index.csv\")\ndf_us_cci","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9939e092","cell_type":"markdown","source":"#### U.S.A Advance Retail Sales: Retail Trade and Food Services","metadata":{}},{"id":"f03ba7e2","cell_type":"code","source":"%%skip\nSTART_DATE = \"2000-01-01\"\nEND_DATE   = date.today().isoformat()\n\n# RSAFS = Advance Retail Sales: Retail Trade and Food Services (Millions of Dollars, SA)\ndf_us_retail = DataReader(\"RSAFS\", \"fred\", START_DATE, END_DATE)\n\ndf_us_retail.columns = [\"Retail_Sales_Retail_and_Food_Services_USA\"]\n\nOUTFILE = \"csv_files/idea_csv/usa_retail_sales.csv\"\ndf_us_retail.to_csv(OUTFILE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a51ef437","cell_type":"code","source":"%%skip\ndf_us_retail = pd.read_csv(\"csv_files/idea_csv/usa_retail_sales.csv\")\ndf_us_retail","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"556249e1","cell_type":"markdown","source":"#### Exchange Rates (China, Mexico, Canada, India, Vietnam)\n","metadata":{}},{"id":"eac1803e","cell_type":"code","source":"%%skip\nSTART = \"2001-06-17\"                 \nEND   = date.today().isoformat()\n\n#    China (CNY per USD): DEXCHUS  \n#    Mexico (MXN per USD): DEXMXUS  \n#    Canada (CAD per USD): DEXCAUS  \n#    India (INR per USD): DEXINUS  \ncny = DataReader(\"DEXCHUS\", \"fred\", START, END)\nmxn = DataReader(\"DEXMXUS\", \"fred\", START, END)\ncad = DataReader(\"DEXCAUS\", \"fred\", START, END)\ninr = DataReader(\"DEXINUS\", \"fred\", START, END)\n\n# Vietnam via yfinance\nvn_df = yf.download(\n    \"USDVND=X\",\n    start=START,\n    end=END,\n    progress=False\n)\nvn = vn_df[[\"Close\"]].rename(columns={\"Close\": \"VND_per_USD\"})\n\n\nfx = pd.concat([cny, mxn, cad, inr, vn], axis=1).rename(columns={\n    \"DEXCHUS\": \"CNY_per_USD\",\n    \"DEXMXUS\": \"MXN_per_USD\",\n    \"DEXCAUS\": \"CAD_per_USD\",\n    \"DEXINUS\": \"INR_per_USD\"\n})\n\ndf_fx = fx.resample(\"W-FRI\").mean().dropna(how=\"all\").round(4)\n\n\nOUTFILE = \"csv_files/idea_csv/foreign_exchange.csv\"\ndf_fx.to_csv(OUTFILE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"08e87636","cell_type":"code","source":"%%skip\ndf_fx = pd.read_csv(\"csv_files/idea_csv/foreign_exchange.csv\")\ndf_fx","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c96b6f4e","cell_type":"markdown","source":"#### US External Tax Rate","metadata":{}},{"id":"3c1f4120","cell_type":"code","source":"%%skip\nINDICATOR   = \"TM.TAX.MRCH.WM.AR.ZS\"  # Tariff rate %\nCOUNTRIES   = [\"CN\", \"IN\", \"MX\", \"CA\", \"VN\"]\nSTART_YEAR  = 2000\nEND_YEAR    = date.today().year\n\n# from World Bank\ndf_us_tariff = wb.download(\n    indicator=INDICATOR,\n    country=COUNTRIES,\n    start=START_YEAR,\n    end=END_YEAR\n)\n\ndf_us_tariff = df_us_tariff.reset_index().pivot(index=\"year\", columns=\"country\", values=INDICATOR)\n\ndf_us_tariff = df_us_tariff.rename(columns={\n    \"CN\": \"China_Applied_Tariff_%\", \n    \"IN\": \"India_Applied_Tariff_%\", \n    \"MX\": \"Mexico_Applied_Tariff_%\", \n    \"CA\": \"Canada_Applied_Tariff_%\", \n    \"VN\": \"Vietnam_Applied_Tariff_%\"\n})\n\n\nOUTFILE = \"csv_files/idea_csv/external_tax_rates.csv\"\ndf_us_tariff.to_csv(OUTFILE, index_label=\"Year\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7ffeee62","cell_type":"code","source":"%%skip\ndf_us_tariff = pd.read_csv(\"csv_files/idea_csv/external_tax_rates.csv\")\ndf_us_tariff","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a5f8530b","cell_type":"markdown","source":"#### Holidays","metadata":{}},{"id":"b56fb08e","cell_type":"code","source":"%%skip\ndef nth_weekday(year, month, weekday, n):\n    \"\"\"\n    Return the date of the nth occurrence of the given weekday\n    in the specified month and year.\n    weekday: Monday=0, Sunday=6\n    \"\"\"\n    d = date(year, month, 1)\n    count = 0\n    while True:\n        if d.weekday() == weekday:\n            count += 1\n            if count == n:\n                return d\n        d += timedelta(days=1)\n\n# Super Bowl dates per your list\nsuper_bowl_day = {2010: 12, 2011: 11, 2012: 10, 2013: 8}\n\nyears = range(2010, 2014)\nrecords = []\n\nfor year in years:\n    # Fixed‐date holidays\n    records += [\n        {\"Date\": pd.Timestamp(date(year, 2, 14)),  \"Holiday\": \"Valentine's Day\"},\n        {\"Date\": pd.Timestamp(date(year, 3, 17)),  \"Holiday\": \"St. Patrick's Day\"},\n        {\"Date\": pd.Timestamp(date(year, 7, 4)),   \"Holiday\": \"Independence Day\"},\n        {\"Date\": pd.Timestamp(date(year,10,31)),   \"Holiday\": \"Halloween\"},\n        {\"Date\": pd.Timestamp(date(year,12,24)),   \"Holiday\": \"Christmas Eve\"},\n        {\"Date\": pd.Timestamp(date(year,12,25)),   \"Holiday\": \"Christmas Day\"},\n        {\"Date\": pd.Timestamp(date(year,12,31)),   \"Holiday\": \"New Year's Eve\"},\n        # Super Bowl\n        {\"Date\": pd.Timestamp(date(year, 2, super_bowl_day[year])), \"Holiday\": \"Super Bowl\"},\n    ]\n    \n    # Presidents' Day: 3rd Monday in February\n    pd_day = nth_weekday(year, 2, 0, 3)\n    records.append({\"Date\": pd.Timestamp(pd_day), \"Holiday\": \"Presidents' Day\"})\n    \n    # Mother's Day: 2nd Sunday in May\n    md = nth_weekday(year, 5, 6, 2)\n    records.append({\"Date\": pd.Timestamp(md), \"Holiday\": \"Mother's Day\"})\n    \n    # Father's Day: 3rd Sunday in June\n    fd = nth_weekday(year, 6, 6, 3)\n    records.append({\"Date\": pd.Timestamp(fd), \"Holiday\": \"Father's Day\"})\n    \n    # Memorial Day: last Monday in May\n    d_mem = date(year, 5, 31)\n    while d_mem.weekday() != 0:  # 0 = Monday\n        d_mem -= timedelta(days=1)\n    records.append({\"Date\": pd.Timestamp(d_mem), \"Holiday\": \"Memorial Day\"})\n    \n    # Labor Day: 1st Monday in September\n    ld = nth_weekday(year, 9, 0, 1)\n    records.append({\"Date\": pd.Timestamp(ld), \"Holiday\": \"Labor Day\"})\n    \n    # Good Friday & Easter\n    eas = easter(year)\n    gf = eas - timedelta(days=2)\n    records.append({\"Date\": pd.Timestamp(gf), \"Holiday\": \"Good Friday\"})\n    records.append({\"Date\": pd.Timestamp(eas), \"Holiday\": \"Easter Sunday\"})\n    \n    # Daylight Saving Time\n    dst_start = nth_weekday(year, 3, 6, 2)   # 2nd Sunday in March\n    dst_end   = nth_weekday(year,11, 6, 1)   # 1st Sunday in November\n    records.append({\"Date\": pd.Timestamp(dst_start), \"Holiday\": \"DST Start\"})\n    records.append({\"Date\": pd.Timestamp(dst_end),   \"Holiday\": \"DST End\"})\n    \n    # Thanksgiving & related\n    th = nth_weekday(year, 11, 3, 4)  # 4th Thu in Nov\n    records.append({\"Date\": pd.Timestamp(th), \"Holiday\": \"Thanksgiving\"})\n    records.append({\"Date\": pd.Timestamp(th + timedelta(days=1)), \"Holiday\": \"Black Friday\"})\n    records.append({\"Date\": pd.Timestamp(th + timedelta(days=2)), \"Holiday\": \"Small Business Saturday\"})\n    records.append({\"Date\": pd.Timestamp(th + timedelta(days=4)), \"Holiday\": \"Cyber Monday\"})\n    # Super Saturday: last Saturday before Christmas Eve\n    d2 = date(year, 12, 24) - timedelta(days=1)\n    while d2.weekday() != 5: d2 -= timedelta(days=1)\n    records.append({\"Date\": pd.Timestamp(d2), \"Holiday\": \"Super Saturday\"})\n    \n    # Green Monday: 2nd Monday in December\n    gm = nth_weekday(year, 12, 0, 2)\n    records.append({\"Date\": pd.Timestamp(gm), \"Holiday\": \"Green Monday\"})\n    \n    # 2012‐only events\n    if year == 2012:\n        records.append({\"Date\": pd.Timestamp(date(2012, 7, 27)), \"Holiday\": \"Olympics Opening\"})\n        records.append({\"Date\": pd.Timestamp(date(2012,11, 6)), \"Holiday\": \"Presidential Election\"})\n\n# Build the DataFrame\ndf_us_holidays = (\n    pd.DataFrame(records)\n      .drop_duplicates(subset=\"Date\")      # in case any collide\n      .sort_values(\"Date\")\n      .reset_index(drop=True)\n)\n\n# Save or merge as needed\ndf_us_holidays.to_csv(\"csv_files/idea_csv/df_us_holidays.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3611fc71","cell_type":"code","source":"%%skip\ndf_us_holidays = pd.read_csv(\"csv_files/idea_csv/df_us_holidays.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d10bbba2","cell_type":"code","source":"%%skip\n#  parâmetros\nramp_up_days   = 42\nramp_down_days = 14\nwindow_days    = ramp_up_days + ramp_down_days    \n\n# dados de entrada \ndf_us_holidays['Date'] = pd.to_datetime(df_us_holidays['Date']).dt.normalize()\nholidays = df_us_holidays['Date'].tolist()\n\ndf_holiday_impact = df_walmart_train[['Date']].copy()\ndf_holiday_impact['Date'] = pd.to_datetime(df_holiday_impact['Date']).dt.normalize()\ndf_holiday_impact['HolidayImpact'] = 0.0\n\n#  função vectorizada para um único feriado\ndef add_one_holiday(peak_date):\n    start = peak_date - timedelta(days=ramp_up_days)\n    end   = peak_date + timedelta(days=ramp_down_days)\n\n    mask = (df_holiday_impact['Date'] >= start) & (df_holiday_impact['Date'] <= end)\n    if not mask.any():   \n        return\n\n    diff = (df_holiday_impact.loc[mask, 'Date'] - peak_date).dt.days.to_numpy()\n\n    # parte esquerda (ramp-up: diff ∈ [-14, 0])\n    up_mask   = diff <= 0\n    x_up      = (ramp_up_days + diff[up_mask]) / ramp_up_days          # 0→1\n    weights   = np.zeros_like(diff, dtype=float)\n    weights[up_mask] = 0.5 * (1 - np.cos(np.pi * x_up))\n\n    # parte direita (ramp-down: diff ∈ (0, 42])\n    down_mask = diff > 0\n    x_down    = diff[down_mask] / ramp_down_days                       # 0→1\n    weights[down_mask] = 0.5 * (1 + np.cos(np.pi * x_down))\n\n    # soma ao total\n    df_holiday_impact.loc[mask, 'HolidayImpact'] += weights\n\n#  corre todos os feriados \nfor hday in holidays:\n    add_one_holiday(hday)\n\ndf_holiday_impact['HolidayImpact'] = df_holiday_impact['HolidayImpact']\n\ndf_holiday_impact.to_csv('csv_files/idea_csv/df_holiday_impact.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"85a722ea","cell_type":"code","source":"%%skip\ndf_holiday_impact = pd.read_csv('csv_files/idea_csv/df_holiday_impact.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"53a0ce63","cell_type":"markdown","source":"#### Tax Return","metadata":{}},{"id":"ec547b96","cell_type":"markdown","source":"Train","metadata":{}},{"id":"1bf355ee","cell_type":"code","source":"%%skip\ndf_tax_return_train = df_walmart_train[[\"Date\"]].copy()\ndf_tax_return_train\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a428b117","cell_type":"code","source":"%%skip\nramp_up_days   = 14 \nramp_down_days = 42  \n\ndef filing_deadline(year):\n    \"\"\"\n    IRS filing deadline April 15, bumped to Monday if on a weekend.\n    \"\"\"\n    d = date(year, 4, 15)\n    while d.weekday() >= 5:  # \n        d += timedelta(days=1)\n    return pd.Timestamp(d)\n\n\ndef tax_return_weight(ts, ramp_up=ramp_up_days, ramp_down=ramp_down_days):\n    \"\"\"\n    Smooth raised-cosine weight:\n      • 0 before (deadline - ramp_up)\n      • ramps up from 0→1 over `ramp_up` days\n      • ramps down from 1→0 over `ramp_down` days\n      • 0 after (deadline + ramp_down)\n    \"\"\"\n    ts   = pd.Timestamp(ts).normalize()\n    peak = filing_deadline(ts.year)\n    start = peak - timedelta(days=ramp_up)\n    end   = peak + timedelta(days=ramp_down)\n\n    if ts < start or ts > end:\n        return 0.0\n\n    if ts <= peak:\n        # fraction of ramp-up completed [0…1]\n        x = (ts - start).days / ramp_up\n        # raised‐cosine from 0→1\n        return 0.5 * (1 - np.cos(np.pi * x))\n    else:\n        # fraction of ramp-down completed [0…1]\n        x = (ts - peak).days / ramp_down\n        # raised‐cosine from 1→0\n        return 0.5 * (1 + np.cos(np.pi * x))\n\n\ndf_tax_return_train = df_walmart_train[['Date']].copy()\ndf_tax_return_train['TaxReturnImpact'] = (\n    df_tax_return_train['Date']\n      .dt.normalize()\n      .map(tax_return_weight)\n)\n\ndf_tax_return_train.to_csv('csv_files/idea_csv/df_tax_return_train.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c3fafe35","cell_type":"code","source":"%%skip\ndf_unique = df_tax_return_train.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1dfd1022","cell_type":"code","source":"%%skip\ndf_tax_return_train = pd.read_csv('csv_files/idea_csv/df_tax_return_train.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"24f4f2ca","cell_type":"markdown","source":"Test","metadata":{}},{"id":"31cd2010","cell_type":"code","source":"%%skip\nramp_up_days   = 14 \nramp_down_days = 42  \n\ndef filing_deadline(year):\n    \"\"\"\n    IRS filing deadline April 15, bumped to Monday if on a weekend.\n    \"\"\"\n    d = date(year, 4, 15)\n    while d.weekday() >= 5:\n        d += timedelta(days=1)\n    return pd.Timestamp(d)\n\n# ─── SMOOTH WEIGHT FUNCTION ───────────────────────────────\ndef tax_return_weight(ts, ramp_up=ramp_up_days, ramp_down=ramp_down_days):\n    \"\"\"\n    Smooth raised-cosine weight:\n      • 0 before (deadline - ramp_up)\n      • ramps up from 0→1 over `ramp_up` days\n      • ramps down from 1→0 over `ramp_down` days\n      • 0 after (deadline + ramp_down)\n    \"\"\"\n    ts   = pd.Timestamp(ts).normalize()\n    peak = filing_deadline(ts.year)\n    start = peak - timedelta(days=ramp_up)\n    end   = peak + timedelta(days=ramp_down)\n\n    if ts < start or ts > end:\n        return 0.0\n\n    if ts <= peak:\n        # fraction of ramp-up completed [0…1]\n        x = (ts - start).days / ramp_up\n        # raised‐cosine from 0→1\n        return 0.5 * (1 - np.cos(np.pi * x))\n    else:\n        # fraction of ramp-down completed [0…1]\n        x = (ts - peak).days / ramp_down\n        # raised‐cosine from 1→0\n        return 0.5 * (1 + np.cos(np.pi * x))\n\ndf_tax_return_test = df_walmart_test[['Date']].copy()\ndf_tax_return_test['TaxReturnImpact'] = (\n    df_tax_return_test['Date']\n      .dt.normalize()\n      .map(tax_return_weight)\n)\n\ndf_tax_return_test.to_csv('csv_files/idea_csv/df_tax_return_test.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"de2931b9","cell_type":"code","source":"%%skip\ndf_tax_return_test = pd.read_csv('csv_files/idea_csv/df_tax_return_test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d51bedab","cell_type":"markdown","source":"#### Stores Types & Sizes","metadata":{}},{"id":"06cccb9d","cell_type":"code","source":"%%skip\ndf_walmart_train['Store']  = df_walmart_train['Store'].astype(str)\ndf_walmart_stores['Store'] = df_walmart_stores['Store'].astype(str)\n\n# Merge the store metadata into your training DataFrame\ndf_store_types_sizes = df_walmart_train.merge(\n    df_walmart_stores,     \n    on='Store',             \n    how='left',       \n    validate='many_to_one'  \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"797265fe","cell_type":"code","source":"%%skip\ndf_store_types_sizes = df_store_types_sizes.loc[:, [\"Store\",\"Type\",\"Size\"]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"94ef73b4","cell_type":"markdown","source":"#### Oil Price The U.S. domestic","metadata":{}},{"id":"e79e31d2","cell_type":"code","source":"%%skip\nstart = \"2010-02-05\"\nend   = date.today().isoformat()\n \ndf_us_oil_price = DataReader(\"DCOILWTICO\", \"fred\", start, end)\n\nwti_weekly = df_us_oil_price.resample(\"W-FRI\").mean().rename(\n    columns={\"DCOILWTICO\":\"WTI_Weekly_Mean_Price\"}\n)\ndf_us_oil_price.to_csv('csv_files/idea_csv/df_us_oil_price.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6c47a64b","cell_type":"code","source":"%%skip\ndf_us_oil_price = pd.read_csv('csv_files/idea_csv/df_us_oil_price.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d78f1794","cell_type":"markdown","source":"#### U.S. ISM Manufacturing PMI & ISM Services PMI\n","metadata":{}},{"id":"4b0ba176","cell_type":"code","source":"%%skip\n# Get columns names\ndf_man = ak.macro_usa_ism_pmi()\n\nprint(\"Columns in df_man:\", df_man.columns.tolist())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dcad5ebe","cell_type":"code","source":"%%skip\nSTART, END = \"2009-01-01\", \"2014-12-31\"\n\ndf_man = ak.macro_usa_ism_pmi()\n\ndf_man = df_man.rename(columns={\n    \"日期\": \"Date\",\n    \"今值\": \"ISM_Manufacturing_PMI\"\n})\ndf_man[\"Date\"] = pd.to_datetime(df_man[\"Date\"], format=\"%Y-%m\")\ndf_man = (\n    df_man.set_index(\"Date\")[[\"ISM_Manufacturing_PMI\"]]\n    .sort_index()\n    .loc[START:END]\n)\n\ndf_svc = ak.macro_usa_ism_non_pmi()\n\ndf_svc = df_svc.rename(columns={\n    \"日期\": \"Date\",\n    \"今值\": \"ISM_Services_PMI\"\n})\ndf_svc[\"Date\"] = pd.to_datetime(df_svc[\"Date\"], format=\"%Y-%m\")\ndf_svc = (\n    df_svc.set_index(\"Date\")[[\"ISM_Services_PMI\"]]\n    .sort_index()\n    .loc[START:END]\n)\n\ndf_us_ism = df_man.join(df_svc, how=\"outer\")\n\ndf_us_ism.to_csv('csv_files/idea_csv/df_us_ism.csv')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bdcf123d","cell_type":"code","source":"%%skip\ndf_us_ism = pd.read_csv('csv_files/idea_csv/df_us_ism.csv')\ndf_us_ism","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"21e29c85","cell_type":"markdown","source":"#### US CPI Food & Beverages\n","metadata":{}},{"id":"d43223c8","cell_type":"code","source":"%%skip\nSTART, END = \"2009-01-01\", \"2014-12-31\"\n\ndf_us_cpi_food = DataReader(\"CPIFABSL\", \"fred\", START, END)\n\ndf_us_cpi_food.rename(columns={\"CPIFABSL\": \"CPI_Food_Beverages\"}, inplace=True)\n\ndf_us_cpi_food = (\n    df_us_cpi_food[\"CPI_Food_Beverages\"]\n    .resample(\"W-FRI\")\n    .ffill()\n    .to_frame()\n)\ndf_us_cpi_food.to_csv('csv_files/idea_csv/df_us_cpi_food.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"574fba58","cell_type":"code","source":"%%skip\ndf_us_cpi_food = pd.read_csv('csv_files/idea_csv/df_us_cpi_food.csv')\ndf_us_cpi_food","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6a76d45a","cell_type":"markdown","source":"#### US CPI Shelter (Housing)","metadata":{}},{"id":"2b7d7eb2","cell_type":"code","source":"%%skip\nSTART, END = \"2009-01-01\", \"2014-12-31\"\n\ndf_us_cpi_shelter = DataReader(\"CUSR0000SAH1\", \"fred\", START, END)\n\ndf_us_cpi_shelter.rename(columns={\"CUSR0000SAH1\": \"CPI_Shelter\"}, inplace=True)\n\ndf_us_cpi_shelter = (\n    df_us_cpi_shelter[\"CPI_Shelter\"]\n      .resample(\"W-FRI\")\n      .ffill()            \n      .to_frame()       \n)\n\ndf_us_cpi_shelter.to_csv('csv_files/idea_csv/df_us_cpi_shelter.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ae792c64","cell_type":"code","source":"%%skip\ndf_us_cpi_shelter = pd.read_csv('csv_files/idea_csv/df_us_cpi_shelter.csv')\ndf_us_cpi_shelter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b84e0424","cell_type":"markdown","source":"#### US CPI Medical Care\n","metadata":{}},{"id":"83ddcd95","cell_type":"code","source":"%%skip\nSTART, END = \"2009-01-01\", \"2014-12-31\"\n\ndf_us_cpi_med = DataReader(\"CPIMEDSL\", \"fred\", START, END)\n\ndf_us_cpi_med.rename(columns={\"CPIMEDSL\": \"CPI_Medical_Care\"}, inplace=True)\n\ndf_us_cpi_med = (\n    df_us_cpi_med[\"CPI_Medical_Care\"]\n      .resample(\"W-FRI\")   # calendar‐weeks ending Fridays\n      .ffill()             # carry each month’s CPI forward until the next release\n      .to_frame()\n)\n\ndf_us_cpi_med.to_csv('csv_files/idea_csv/df_us_cpi_med.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a8a5cbec","cell_type":"code","source":"%%skip\ndf_us_cpi_med = pd.read_csv('csv_files/idea_csv/df_us_cpi_med.csv')\ndf_us_cpi_med","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4fd3d247","cell_type":"markdown","source":"#### US CPI Transportation\n","metadata":{}},{"id":"e1d82656","cell_type":"code","source":"%%skip\nSTART, END = \"2009-01-01\", \"2014-12-31\"\n\ndf_us_cpi_trans = DataReader(\"CPITRNSL\", \"fred\", START, END)\n\ndf_us_cpi_trans.rename(columns={\"CPITRNSL\": \"CPI_Transportation\"}, inplace=True)\n\ndf_us_cpi_trans = (\n    df_us_cpi_trans[\"CPI_Transportation\"]\n      .resample(\"W-FRI\")\n      .ffill()\n      .to_frame()\n)\n\ndf_us_cpi_trans.to_csv('csv_files/idea_csv/df_us_cpi_trans.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5095ed62","cell_type":"code","source":"%%skip\ndf_us_cpi_trans = pd.read_csv('csv_files/idea_csv/df_us_cpi_trans.csv')\ndf_us_cpi_trans","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"66507566","cell_type":"markdown","source":"#### PCE: US Healthcare Services\n","metadata":{}},{"id":"4b7d3784","cell_type":"code","source":"%%skip\nSTART, END = \"2009-01-01\", \"2014-12-31\"\n\ndf_us_pce_health = DataReader(\"DHLCRC1Q027SBEA\", \"fred\", START, END)\n\ndf_us_pce_health.rename(columns={\"DHLCRC1Q027SBEA\": \"PCE_Healthcare_Services\"}, inplace=True)\n\ndf_us_pce_health = (\n    df_us_pce_health[\"PCE_Healthcare_Services\"]\n      .resample(\"W-FRI\")   \n      .ffill()      \n      .to_frame()\n)\n\n\ndf_us_pce_health.to_csv('csv_files/idea_csv/df_us_pce_health.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7751d199","cell_type":"code","source":"%%skip\ndf_us_pce_health = pd.read_csv('csv_files/idea_csv/df_us_pce_health.csv')\ndf_us_pce_health","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d1d4fd13","cell_type":"markdown","source":"#### US ICSA (Jobless Claims)","metadata":{}},{"id":"601cfc27","cell_type":"code","source":"%%skip\nSTART, END = \"2009-01-01\", \"2014-12-31\"\n\n\ndf_us_icsa_jobless = DataReader(\"ICSA\", \"fred\", START, END)\n\ndf_us_icsa_jobless.rename(columns={\"ICSA\": \"Weekly_Initial_Jobless_Claims\"}, inplace=True)\n\ndf_us_icsa_jobless = (\n    df_us_icsa_jobless[\"Weekly_Initial_Jobless_Claims\"]\n      .resample(\"W-FRI\")  \n      .ffill()           \n      .to_frame()\n)\n\ndf_us_icsa_jobless.to_csv('csv_files/idea_csv/df_us_icsa_jobless.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bb3cc0dc","cell_type":"code","source":"%%skip\ndf_us_icsa_jobless = pd.read_csv('csv_files/idea_csv/df_us_icsa_jobless.csv')\ndf_us_icsa_jobless","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5eaab7ab","cell_type":"markdown","source":"#### US Rail , Freight & Carloads","metadata":{}},{"id":"1e18106c","cell_type":"code","source":"%%skip\nSTART, END = \"2009-01-01\", \"2014-12-31\"\n\nrail = DataReader(\"RAILFRTCARLOADS\", \"fred\", START, END)\nrail.rename(columns={\"RAILFRTCARLOADS\": \"Rail_Freight_Carloads\"}, inplace=True)\n\nrail_weekly = (\n    rail[\"Rail_Freight_Carloads\"]\n        .resample(\"W-FRI\")\n        .ffill()\n        .to_frame()\n)\n\ntruck = DataReader(\"TRUCKD11\", \"fred\", START, END)\ntruck.rename(columns={\"TRUCKD11\": \"Truck_Tonnage_Index\"}, inplace=True)\n\ntruck_weekly = (\n    truck[\"Truck_Tonnage_Index\"]\n         .resample(\"W-FRI\")\n         .ffill()\n         .to_frame()\n)\n\ndf_us_rail_freight_carloads = rail_weekly.join(truck_weekly, how=\"outer\")\n\ndf_us_rail_freight_carloads.to_csv('csv_files/idea_csv/df_us_rail_freight_carloads.csv')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"db603ad4","cell_type":"code","source":"%%skip\ndf_us_rail_freight_carloads = pd.read_csv('csv_files/idea_csv/df_us_rail_freight_carloads.csv')\ndf_us_rail_freight_carloads","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e0987caa","cell_type":"markdown","source":"#### EU PMI","metadata":{}},{"id":"6433ffd1","cell_type":"code","source":"%%skip\ndf_eu = ak.macro_euro_manufacturing_pmi()\n\nprint(df_eu.columns.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"92723d01","cell_type":"code","source":"%%skip\nSTART, END = \"2009-01-01\", \"2014-12-31\"\n\ndf_eu_pmi = ak.macro_euro_manufacturing_pmi()\n\ndf_eu_pmi = df_eu_pmi.rename(columns={\n    \"日期\": \"Date\",\n    \"今值\":  \"Euro_Manufacturing_PMI\"\n})\ndf_eu_pmi[\"Date\"] = pd.to_datetime(df_eu_pmi[\"Date\"], format=\"%Y-%m\")\n\ndf_eu_pmi = (\n    df_eu_pmi.set_index(\"Date\")[[\"Euro_Manufacturing_PMI\"]]\n              .sort_index()\n              .loc[START:END]\n)\n\ndf_eu_pmi = (\n    df_eu_pmi[\"Euro_Manufacturing_PMI\"]\n      .resample(\"W-FRI\")\n      .ffill()\n      .to_frame()\n)\ndf_eu_pmi.to_csv('csv_files/idea_csv/df_eu_pmi.csv')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0842da7a","cell_type":"code","source":"%%skip\ndf_eu_pmi = pd.read_csv('csv_files/idea_csv/df_eu_pmi.csv')\ndf_eu_pmi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f48f0bab","cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"id":"d19ee0d3","cell_type":"code","source":"%%skip\ndf_wm_train = df_walmart_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"750503ef","cell_type":"markdown","source":"### Data Cleaning - Train","metadata":{}},{"id":"654597fd","cell_type":"markdown","source":"#### SP 500","metadata":{}},{"id":"0a154839","cell_type":"code","source":"%%skip\ndf_sp500['Date'] = pd.to_datetime(df_sp500[\"Date\"], errors=\"raise\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4adf2de3","cell_type":"code","source":"%%skip\ndf_sp500.rename(columns={\n    'SPX_Weekly_Mean_Close': 'SP500_Weekly_Mean_Close'\n}, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"048a5345","cell_type":"code","source":"%%skip\ndf_merged = df_wm_train.merge(\n    df_sp500,\n    on=\"Date\",\n    how=\"left\",\n    validate=\"many_to_one\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2f21dc64","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7fd6d60b","cell_type":"markdown","source":"#### Walmart Stock Price","metadata":{}},{"id":"f6f51118","cell_type":"code","source":"%%skip\ndf_walmart_stock['Date'] = pd.to_datetime(df_walmart_stock[\"Date\"], errors=\"raise\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e79b05d6","cell_type":"code","source":"%%skip\ndf_merged = df_wm_train.merge(\n    df_walmart_stock,\n    on=\"Date\",\n    how=\"left\",\n    validate=\"many_to_one\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a8b85d47","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7a8ce570","cell_type":"markdown","source":"#### External Logistic companies Walmart","metadata":{}},{"id":"09f2840c","cell_type":"code","source":"%%skip\ndf_logistics['Date'] = pd.to_datetime(df_logistics[\"Date\"], errors=\"raise\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8e79c62f","cell_type":"code","source":"%%skip\ndf_merged = df_wm_train.merge(\n    df_logistics,\n    on=\"Date\",\n    how=\"left\",\n    validate=\"many_to_one\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"43a70cb4","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dc856701","cell_type":"markdown","source":"#### Official China PMI (Caixin PMI only starts in 2014)","metadata":{}},{"id":"0a0aa189","cell_type":"code","source":"%%skip\ndf_pmi_china.drop(columns=['Official_Manufacturing_PMI_YoY', 'Official_Services_PMI_YoY'], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"511d53b8","cell_type":"code","source":"%%skip\ndf_pmi_china.rename(columns={\n    'Official_Manufacturing_PMI': 'China_Official_Manufacturing_PMI',\n    'Official_Services_PMI': 'China_Official_Services_PMI'\n}, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"85bfa335","cell_type":"code","source":"%%skip\ndf_pmi_china['Date'] = pd.to_datetime(df_pmi_china[\"Date\"], errors=\"raise\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9d829f1a","cell_type":"code","source":"%%skip\ndf_wm_train['YM'] = df_wm_train['Date'].dt.to_period('M')\ndf_pmi_china    ['YM'] = df_pmi_china    ['Date'].dt.to_period('M')\n\ndf_merged = df_wm_train.merge(\n    df_pmi_china[['YM','China_Official_Manufacturing_PMI','China_Official_Services_PMI']],\n    on='YM',\n    how='left'\n).drop(columns='YM')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5035e219","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f55872d6","cell_type":"markdown","source":"#### PCE USA (Personal Consumption Expenditures)","metadata":{}},{"id":"9a6b48f4","cell_type":"code","source":"%%skip\ndf_pce.rename(columns={\n    'Personal_Consumption_Expenditures': 'US_Personal_Consumption_Expenditures'\n}, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cfa5fa96","cell_type":"code","source":"%%skip\ndf_pce['DATE'] = pd.to_datetime(df_pce[\"DATE\"], errors=\"raise\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e53bf8ed","cell_type":"code","source":"%%skip\ndf_wm_train['YM'] = df_wm_train['Date'].dt.to_period('M')\ndf_pce    ['YM'] = df_pce    ['DATE'].dt.to_period('M')\n\ndf_merged = df_wm_train.merge(\n    df_pce[['YM','US_Personal_Consumption_Expenditures']],\n    on='YM',\n    how='left'\n).drop(columns='YM')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b654ea5b","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e063e49c","cell_type":"markdown","source":"#### Interest Rates USA (Fed Funds Rate & Tbill 3 Months Yield)","metadata":{}},{"id":"a49657aa","cell_type":"code","source":"%%skip\ndf_interest_rates['DATE'] = pd.to_datetime(df_interest_rates[\"DATE\"], errors=\"raise\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f88706b9","cell_type":"code","source":"%%skip\ndf_interest_rates.rename(columns={\n    'Fed_Funds_Rate': 'US_Fed_Funds_Rate',\n    'TBill_3mo_Yield': 'US_TBill_3mo_Yield',\n    'DATE': 'Date'\n\n}, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d563b328","cell_type":"code","source":"%%skip\ndf_wm_train = df_wm_train.sort_values('Date')\ndf_interest_rates = df_interest_rates.sort_values('Date')\n\ndf_merged = df_wm_train.merge(\n    df_interest_rates[['Date','US_TBill_3mo_Yield','US_Fed_Funds_Rate']],\n    on='Date',\n    how='left'\n)\n\ndf_merged['US_Fed_Funds_Rate'] = df_merged['US_Fed_Funds_Rate'].ffill()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7f55aee9","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"66f574bc","cell_type":"markdown","source":"#### CCI USA (Consumer Confidence Index) from University of Michigan","metadata":{}},{"id":"2f3db3d5","cell_type":"code","source":"%%skip\ndf_us_cci['Date']   = pd.to_datetime(df_us_cci['DATE'])\ndf_us_cci = df_us_cci.sort_values('Date')\n\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_us_cci[['Date','Consumer_Sentiment_UMich']],\n    on='Date',\n    direction='backward'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5444d26d","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7cc5cbab","cell_type":"markdown","source":"#### U.S.A Advance Retail Sales: Retail Trade and Food Services","metadata":{}},{"id":"98d07682","cell_type":"code","source":"%%skip\ndf_us_retail['Date']   = pd.to_datetime(df_us_retail['DATE'])\ndf_us_retail = df_us_retail.sort_values('Date')\n\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_us_retail[['Date','Retail_Sales_Retail_and_Food_Services_USA']],\n    on='Date',\n    direction='backward'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6bdfb761","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4a8a9ac9","cell_type":"markdown","source":"#### Exchange Rates (China, Mexico, Canada, India, Vietnam)","metadata":{}},{"id":"7fed900a","cell_type":"code","source":"%%skip\ndf_fx.rename(columns={\n    'Unnamed: 0': 'Date',\n    \"('VND_per_USD', 'USDVND=X')\": 'VND_per_USD'\n}, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7a19a385","cell_type":"code","source":"%%skip\ndf_fx['Date']   = pd.to_datetime(df_fx['Date'])\ndf_fx = df_fx.sort_values('Date')\n\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_fx[['Date','CNY_per_USD', 'MXN_per_USD','CAD_per_USD','INR_per_USD','VND_per_USD']],\n    on='Date',\n    direction='backward'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3893577f","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7e5457db","cell_type":"markdown","source":"#### US External Tax Rate","metadata":{}},{"id":"7e02ca28","cell_type":"code","source":"%%skip\ndf_us_tariff.rename(columns={\n    'Canada': 'US_TAX_Canada',\n    'China': 'US_TAX_China',\n    'India': 'US_TAX_India',\n    'Mexico': 'US_TAX_Mexico',\n    'Viet Nam': 'US_TAX_Vietnam'\n}, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"316c820d","cell_type":"code","source":"%%skip\n# list of tariff columns\ntariff_cols = [\n    'US_TAX_Canada',\n    'US_TAX_China',\n    'US_TAX_India',\n    'US_TAX_Mexico',\n    'US_TAX_Vietnam'\n]\n\n# 1) prepare the dates\ndf_wm_train['Date']      = pd.to_datetime(df_wm_train['Date'])\ndf_us_tariff['Date']     = pd.date_range(\n    '2001-01-01',\n    periods=len(df_us_tariff),\n    freq='YS'\n)\n\n# 2) sort & merge_asof, then forward-fill tariffs\ndf_merged = (\n    pd.merge_asof(\n        df_wm_train.sort_values('Date'),\n        df_us_tariff[['Date'] + tariff_cols].sort_values('Date'),\n        on='Date',\n        direction='backward'\n    )\n    .assign(**{col: lambda d, col=col: d[col].ffill() for col in tariff_cols})\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"67456ecb","cell_type":"code","source":"%%skip\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a7d9125c","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"23031f73","cell_type":"markdown","source":"#### Holidays","metadata":{}},{"id":"36f24b19","cell_type":"code","source":"%%skip\ndf_holiday_impact['Date'] = pd.to_datetime(df_holiday_impact['Date'])\n\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_holiday_impact[['Date','HolidayImpact']].sort_values('Date'),\n    on='Date',\n    direction='backward'\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"251f3912","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"347f09e6","cell_type":"code","source":"%%skip\n#  make sure both Date columns are datetime at midnight\ndf_us_holidays['Date'] = pd.to_datetime(df_us_holidays['Date']).dt.normalize()\ndf_merged       ['Date'] = pd.to_datetime(df_merged       ['Date']).dt.normalize()\n\n# 2. compute each holiday’s “week_end” Friday (weekday=4)\ndf_us_holidays['week_end'] = (\n    df_us_holidays['Date']\n  + pd.to_timedelta((4 - df_us_holidays['Date'].dt.weekday) % 7, unit='D')\n)\n\n#  build lookup: week_end → joined holiday names\ndf_holiday_names = (\n    df_us_holidays\n      .groupby('week_end', as_index=False)['Holiday']\n      .agg(lambda names: ', '.join(names))\n      .rename(columns={'week_end':'Date', 'Holiday':'Holiday_Name'})\n)\n\n#  merge it in\ndf_merged = df_merged.merge(df_holiday_names, on='Date', how='left')\n\n#  for weeks without a holiday, fill in “No Holiday”\ndf_merged['Holiday_Name'] = df_merged['Holiday_Name'].fillna('No Holiday')\n\n# recompute your flag if you like:\ndf_merged['IsHoliday'] = df_merged['Holiday_Name'] != 'No Holiday'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"348b04c5","cell_type":"code","source":"%%skip\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0aaa92fc","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"00a784bf","cell_type":"markdown","source":"#### Tax Return (Train)","metadata":{}},{"id":"e31b4d53","cell_type":"code","source":"%%skip\ndf_tax_return_train.rename(columns={\n    'TaxReturnImpact': 'US_Tax_Return'\n}, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"57759c75","cell_type":"code","source":"%%skip\ndf_tax_return_train['Date']   = pd.to_datetime(df_tax_return_train['Date'])\ndf_tax_return_train = df_tax_return_train.sort_values('Date')\n\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_tax_return_train[['Date','US_Tax_Return']],\n    on='Date',\n    direction='backward'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1ec11622","cell_type":"code","source":"%%skip\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d9f6580a","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"efaca538","cell_type":"markdown","source":"#### Stores Types & Sizes","metadata":{}},{"id":"c80754b3","cell_type":"code","source":"%%skip\ndf_store_types_sizes['Store'] = df_store_types_sizes['Store'].astype(int)\ndf_wm_train['Store'] = df_wm_train['Store'].astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d9f75a78","cell_type":"code","source":"%%skip\ndf_store_info = df_store_types_sizes.drop_duplicates(subset='Store')\n\nmain_df = df_wm_train.merge(df_store_info, on='Store', how='left')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1ec39f6f","cell_type":"code","source":"%%skip\ndf_wm_train = main_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"712ff2a7","cell_type":"markdown","source":"#### Oil Price The U.S. domestic","metadata":{}},{"id":"b2fb65ec","cell_type":"code","source":"%%skip\ndf_us_oil_price['DATE'] = pd.to_datetime(df_us_oil_price['DATE'])\ndf_us_oil_price = df_us_oil_price.set_index('DATE')\n\n# Interpolate missing values\ndf_us_oil_price['DCOILWTICO'] = df_us_oil_price['DCOILWTICO'].interpolate(\n    method='linear',\n    limit_direction='both',\n    limit_area='inside'\n)\n\ndf_us_oil_price = df_us_oil_price.reset_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"33c8e9fe","cell_type":"code","source":"%%skip\ndf_us_oil_price = df_us_oil_price.rename(columns={'DATE': 'Date'})\n\ndf_merged = df_wm_train.merge(df_us_oil_price[['Date', 'DCOILWTICO']], on='Date', how='left')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"062736d6","cell_type":"code","source":"%%skip\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b8e1c9c7","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6361e351","cell_type":"markdown","source":"#### U.S. ISM Manufacturing PMI & ISM Services PMI\n","metadata":{}},{"id":"6f3497e6","cell_type":"code","source":"%%skip\ndf_us_ism['Date'] = pd.to_datetime(df_us_ism['Date'])\ndf_us_ism = df_us_ism.set_index('Date')\n\n# Interpolate missing values\ndf_us_ism['ISM_Manufacturing_PMI'] = df_us_ism['ISM_Manufacturing_PMI'].interpolate(\n    method='linear',\n    limit_direction='both',\n    limit_area='inside'\n)\ndf_us_ism['ISM_Services_PMI'] = df_us_ism['ISM_Services_PMI'].interpolate(\n    method='linear',\n    limit_direction='both',\n    limit_area='inside'\n)\n\n\ndf_us_ism = df_us_ism.reset_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"51064e60","cell_type":"code","source":"%%skip\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_us_ism[['Date','ISM_Manufacturing_PMI','ISM_Services_PMI']],\n    on='Date',\n    direction='backward'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f9170d54","cell_type":"code","source":"%%skip\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d945176c","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3b3b695a","cell_type":"markdown","source":"#### US CPI Food & Beverages","metadata":{}},{"id":"75d615af","cell_type":"code","source":"%%skip\ndf_us_cpi_food = df_us_cpi_food.rename(columns={'DATE': 'Date'})\ndf_us_cpi_food['Date'] = pd.to_datetime(df_us_cpi_food['Date'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7d48089b","cell_type":"code","source":"%%skip\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_us_cpi_food[['Date','CPI_Food_Beverages']],\n    on='Date',\n    direction='backward'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4b428242","cell_type":"code","source":"%%skip\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e5561f4f","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d6ea0b9d","cell_type":"markdown","source":"#### US CPI Shelter (Housing)","metadata":{}},{"id":"c31e80fe","cell_type":"code","source":"%%skip\ndf_us_cpi_shelter = df_us_cpi_shelter.rename(columns={'DATE': 'Date'})\ndf_us_cpi_shelter['Date'] = pd.to_datetime(df_us_cpi_shelter['Date'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"06b03902","cell_type":"code","source":"%%skip\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_us_cpi_shelter[['Date','CPI_Shelter']],\n    on='Date',\n    direction='backward'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"437f9582","cell_type":"code","source":"%%skip\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5bcb6f13","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cf0e04df","cell_type":"markdown","source":"#### US CPI Medical Care","metadata":{}},{"id":"8250753e","cell_type":"code","source":"%%skip\ndf_us_cpi_med = df_us_cpi_med.rename(columns={'DATE': 'Date'})\ndf_us_cpi_med['Date'] = pd.to_datetime(df_us_cpi_med['Date'])\n\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_us_cpi_med[['Date','CPI_Medical_Care']],\n    on='Date',\n    direction='backward'\n)\n\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3ea8fa51","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"55c42970","cell_type":"markdown","source":"#### US CPI Transportation\n","metadata":{}},{"id":"051daf3e","cell_type":"code","source":"%%skip\ndf_us_cpi_trans = df_us_cpi_trans.rename(columns={'DATE': 'Date'})\ndf_us_cpi_trans['Date'] = pd.to_datetime(df_us_cpi_trans['Date'])\n\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_us_cpi_trans[['Date','CPI_Transportation']],\n    on='Date',\n    direction='backward'\n)\n\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"24d144f9","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"49a0202b","cell_type":"markdown","source":"#### PCE: US Healthcare Services","metadata":{}},{"id":"a7a9a492","cell_type":"code","source":"%%skip\ndf_us_pce_health = df_us_pce_health.rename(columns={'DATE': 'Date'})\ndf_us_pce_health['Date'] = pd.to_datetime(df_us_pce_health['Date'])\n\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_us_pce_health[['Date','PCE_Healthcare_Services']],\n    on='Date',\n    direction='backward'\n)\n\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"13326b19","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c8860384","cell_type":"markdown","source":"#### US ICSA (Jobless Claims)","metadata":{}},{"id":"25fd7cf4","cell_type":"code","source":"%%skip\ndf_us_icsa_jobless = df_us_icsa_jobless.rename(columns={'DATE': 'Date'})\ndf_us_icsa_jobless['Date'] = pd.to_datetime(df_us_icsa_jobless['Date'])\n\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_us_icsa_jobless[['Date','Weekly_Initial_Jobless_Claims']],\n    on='Date',\n    direction='backward'\n)\n\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9a643ebc","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c55f1d38","cell_type":"markdown","source":"#### US Rail , Freight & Carloads","metadata":{}},{"id":"fb105590","cell_type":"code","source":"%%skip\ndf_us_rail_freight_carloads = df_us_rail_freight_carloads.rename(columns={'DATE': 'Date'})\ndf_us_rail_freight_carloads['Date'] = pd.to_datetime(df_us_rail_freight_carloads['Date'])\n\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_us_rail_freight_carloads[['Date','Rail_Freight_Carloads','Truck_Tonnage_Index']],\n    on='Date',\n    direction='backward'\n)\n\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0aa8f6d5","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1c8b0772","cell_type":"markdown","source":"#### EU PMI","metadata":{}},{"id":"048d1001","cell_type":"code","source":"%%skip\ndf_eu_pmi['Date'] = pd.to_datetime(df_eu_pmi['Date'])\n\ndf_merged = pd.merge_asof(\n    df_wm_train.sort_values('Date'),\n    df_eu_pmi[['Date','Euro_Manufacturing_PMI']],\n    on='Date',\n    direction='backward'\n)\n\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f354341b","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3fe9775b","cell_type":"markdown","source":"#### Walmart Promotion Feature","metadata":{}},{"id":"b3e21d82","cell_type":"code","source":"%%skip\ndf_merged = df_wm_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ec848b46","cell_type":"code","source":"%%skip\n# — make sure both date columns are true datetimes —\ndf_merged['Date']           = pd.to_datetime(df_merged['Date'])\ndf_walmart_features['Date'] = pd.to_datetime(df_walmart_features['Date'])\n\n# — pick just the columns you need from df_walmart_features —\nmd_cols = ['Store','Date','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\n\n# — left-merge so every (store,dept,date) in df_merged picks up that store's markdowns for that week —\ndf_out = df_merged.merge(\n    df_walmart_features[md_cols],\n    on=['Store','Date'],\n    how='left'\n)\ndf_merged = df_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1c90a926","cell_type":"code","source":"%%skip\ndf_unique = df_merged.drop_duplicates(subset='Date', keep='first')\ndf_unique","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e4e1eb2b","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9998a599","cell_type":"markdown","source":"## Data Transformation","metadata":{}},{"id":"afe0b5cb","cell_type":"markdown","source":"### Train","metadata":{}},{"id":"fe8eff0d","cell_type":"markdown","source":"#### Columns Renaming Reorder","metadata":{}},{"id":"6c828107","cell_type":"code","source":"%%skip\n# lowercase all columns\ndf_merged.columns = df_merged.columns.str.lower()\n\n# define new order in lowercase, with markdowns at the end\nnew_order = [\n    # identifiers & target\n    'date', 'store', 'type', 'size', 'dept', 'weekly_sales',\n\n    # holiday flags\n    'isholiday', 'holiday_name', 'holidayimpact',\n\n    # tax & return\n    'us_tax_return',\n\n    # market indices\n    'sp500_weekly_mean_close', 'wmt_weekly_mean_close',\n\n    # logistics-stock closes\n    'arcb_df_logistics_close', 'ait_df_logistics_close', 'ceva_df_logistics_close',\n    'fdx_df_logistics_close', 'saia_df_logistics_close', 'tfii.to_df_logistics_close',\n    'xpo_df_logistics_close', 'odfl_df_logistics_close', 'ups_df_logistics_close',\n    'jbht_df_logistics_close',\n\n    # oil price\n    'dcoilwtico',\n\n    # PMI\n    'china_official_manufacturing_pmi', 'china_official_services_pmi',\n    'ism_manufacturing_pmi', 'ism_services_pmi', 'euro_manufacturing_pmi',\n\n    # sentiment & consumption\n    'consumer_sentiment_umich', 'us_personal_consumption_expenditures',\n    'retail_sales_retail_and_food_services_usa',\n\n    # interest rates\n    'us_tbill_3mo_yield', 'us_fed_funds_rate',\n\n    # inflation measures\n    'cpi_food_beverages', 'cpi_shelter', 'cpi_medical_care', 'cpi_transportation',\n    'pce_healthcare_services',\n\n    # labor & freight\n    'weekly_initial_jobless_claims', 'rail_freight_carloads', 'truck_tonnage_index',\n\n    # FX rates\n    'cny_per_usd', 'mxn_per_usd', 'cad_per_usd', 'inr_per_usd', 'vnd_per_usd',\n\n    # trade tariffs\n    'us_tax_canada', 'us_tax_china', 'us_tax_india', 'us_tax_mexico', 'us_tax_vietnam',\n\n    # markdowns\n    'markdown1', 'markdown2', 'markdown3', 'markdown4', 'markdown5'\n]\n\n# sanity check\nmissing = [col for col in new_order if col not in df_merged.columns]\nif missing:\n    raise KeyError(f\"These columns are missing from df_merged: {missing}\")\n\n# reorder\ndf_merged = df_merged[new_order]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0adf3ed5","cell_type":"code","source":"%%skip\n# lowercase every column name\ndf_merged.columns = df_merged.columns.str.lower()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7cb25dfb","cell_type":"code","source":"%%skip\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"686dd941","cell_type":"markdown","source":"#### Date Column","metadata":{}},{"id":"d7c6d9ca","cell_type":"code","source":"%%skip\n# Start from your raw df_merged_wm_train\ndf_merged = df_wm_train.copy()\n\n# Parse date, extract year & weekofyr, then sort\ndf_merged['date']     = pd.to_datetime(df_merged['date'])\ndf_merged['year']     = df_merged['date'].dt.year\ndf_merged['weekofyr'] = df_merged['date'].dt.isocalendar().week.astype(int)\ndf_merged = df_merged.sort_values(['store','dept','date'])\n\n# Build lag features (with NaNs in the first 1/4/52 rows per group)\ndf_merged = df_merged.set_index(['store','dept','date'])\nfor lag in [1,4,52]:\n    df_merged[f'lag_{lag}'] = df_merged.groupby(level=['store','dept'])['weekly_sales'].shift(lag)\ndf_merged = df_merged.reset_index()\n\n# Compute seasonal means by (store,dept,weekofyr,year)\nseasonal = (\n    df_merged\n    .groupby(['store','dept','weekofyr','year'])['weekly_sales']\n    .mean()\n    .reset_index(name='seasonal_mean')\n)\n\n# Fit year‐over‐year trend (slope & intercept) for each (store,dept,weekofyr)\ntrend_params = {}\nfor (st, dp, wk), grp in seasonal.groupby(['store','dept','weekofyr']):\n    yrs   = grp['year'].values\n    means = grp['seasonal_mean'].values\n    if len(yrs) >= 2:\n        m, b = np.polyfit(yrs, means, 1)\n    else:\n        # if only one year available, flat trend\n        m, b = 0.0, means[0]\n    trend_params[(st,dp,wk)] = (m, b)\n\n# Define an imputer that uses seasonal-trend prediction\ndef predict_seasonal(row):\n    key = (row.store, row.dept, row.weekofyr)\n    m, b = trend_params.get(key, (0.0, np.nan))\n    return m * row.year + b\n\n# fill any missing lag_* by seasonal-trend\nfor lag in [1,4,52]:\n    col = f'lag_{lag}'\n    mask = df_merged[col].isna()\n    df_merged.loc[mask, col] = df_merged[mask].apply(predict_seasonal, axis=1)\n\n# Clip at zero\nfor lag in [1,4,52]:\n    df_merged[f'lag_{lag}'] = df_merged[f'lag_{lag}'].clip(lower=0)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b979f7a5","cell_type":"code","source":"%%skip\n# long-term drift   \ndf_merged['year'] = df_merged['date'].dt.year\n\n# annual seasonality on a weekly grid\ndf_merged['weekofyr'] = df_merged['date'].dt.isocalendar().week.astype(int)\ndf_merged['week_sin'] = np.sin(2 * np.pi * df_merged['weekofyr'] / 52)\ndf_merged['week_cos'] = np.cos(2 * np.pi * df_merged['weekofyr'] / 52)\n\n# continuous trend\ndf_merged['date_ordinal'] = df_merged['date'].map(pd.Timestamp.toordinal)\n\n# drop the raw date if you like\ndf_merged = df_merged.drop(columns=['date'])\n\ndf_wm_train = df_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9434b23f-d8f0-4355-be2b-a21b052a3d15","cell_type":"code","source":"%%skip\n#Kaggle\ndf_wm_train = pd.read_csv('/kaggle/input/df-wm-train01-kaggle/df_wm_train01.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3f19cd38-892a-4c6e-bec6-238ff01c5a4f","cell_type":"code","source":"%%skip\ndf_wm_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"541cf7dd","cell_type":"markdown","source":"#### Encoding","metadata":{}},{"id":"020f78e8","cell_type":"code","source":"%%skip\ntype_map = {'A': 1, 'B': 2, 'C': 3}\nholiday_map = {False: 0, True: 1}\n\ndf_wm_train['type']      = df_wm_train['type'].map(type_map)\ndf_wm_train['isholiday'] = df_wm_train['isholiday'].map(holiday_map)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b2283700-7b2c-40f0-b4ba-7aabcf346712","cell_type":"code","source":"%%skip\n#Kaggle\ndf_wm_train.to_csv(\"/kaggle/working/df_wm_train02.csv\", index=False)\ndf_wm_train = pd.read_csv('/kaggle/working/df_wm_train02.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8ece482c-3f10-401c-9aa8-a7a9d64761a9","cell_type":"code","source":"%%skip\ndf_wm_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"06a1ee67","cell_type":"markdown","source":"## Data Viz EDA","metadata":{}},{"id":"053e19b8","cell_type":"code","source":"%%skip\ndf_wm_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d113781b","cell_type":"code","source":"%%skip\nplt.figure()\ndf_wm_train.groupby('year')['weekly_sales'].mean().plot()\nplt.title(\"Avg Weekly Sales Over Time\")\nplt.ylabel(\"Sales\")\nplt.xlabel(\"Date\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c37e045f","cell_type":"code","source":"%%skip\nplt.figure(figsize=(14,4))\nfor i, lag in enumerate(['lag_1','lag_4','lag_52'], 1):\n    plt.subplot(1,3,i)\n    sns.scatterplot(x=lag, y='weekly_sales', data=df_wm_train, alpha=0.3)\n    plt.title(f\"{lag} vs Weekly Sales\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"323edc89","cell_type":"code","source":"%%skip\norder = df_wm_train.groupby('holiday_name')['weekly_sales'].median().sort_values(ascending=False).index\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16, 8))\n\nax = sns.boxplot(\n    x='holiday_name',\n    y='weekly_sales',\n    data=df_wm_train,\n    order=order,\n    width=0.6\n)\n\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\nax.tick_params(axis='x', pad=12)\nplt.title(\"Sales by Holiday/Special Event (sorted by median sales)\")\nplt.xlabel(\"\")\nplt.ylabel(\"Weekly Sales\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6d955296","cell_type":"code","source":"%%skip\nplt.figure(figsize=(12,6))\nsns.lineplot(\n    x='weekofyr', y='weekly_sales',\n    hue='year', estimator='mean',\n    data=df_wm_train, palette=\"tab10\"\n)\nplt.title(\"Average Sales by Week of Year, by Year\")\nplt.xlabel(\"Week of Year\")\nplt.ylabel(\"Avg Weekly Sales\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"05037fa6","cell_type":"code","source":"%%skip\nplt.figure(figsize=(14,5))\nsubset = df_wm_train[df_wm_train['store'].isin([1,2,3,4,9,25,45])] \nsns.lineplot(\n    x='weekofyr', y='weekly_sales',\n    hue='store', data=subset,\n    estimator='mean', ci=None\n)\nplt.title(\"Sales Over Time for Sample Stores\")\nplt.xlabel(\"Week of Year\")\nplt.ylabel(\"Weekly Sales\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1e29501c","cell_type":"markdown","source":"## Data Optimization","metadata":{}},{"id":"b6b38d9b","cell_type":"markdown","source":"#### Drop columns Correlation >= 80%","metadata":{}},{"id":"a98aa215","cell_type":"code","source":"%%skip\ndf_wm_train_pruned = df_wm_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"efcee736","cell_type":"code","source":"%%skip\n# absolute correlation of your numeric features\ndf_num = df_wm_train_pruned.select_dtypes(include=[np.number])\ncorr = df_num.corr().abs()\n\n# upper‐triangle mask\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# find all pairs with |r|>0.8\nthresh = 0.9\nhigh_corr = (\n    corr.where(~mask)               \n        .stack()                    \n        .loc[lambda s: s > thresh] \n        .sort_values(ascending=False)\n)\nhigh_corr\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"478732ad","cell_type":"code","source":"%%skip\ntarget_corr = corr['weekly_sales'].sort_values(ascending=False)\nprint(target_corr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"591780c3","cell_type":"code","source":"%%skip\n\nus_tax_mexico → keep us_tax_vietnam\n(Identical tariff signals.)\n\nweekofyr → keep week_sin/week_cos\n(Raw week number is redundant—cyclical encoding is sufficient.)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"606970db","cell_type":"code","source":"%%skip\nto_drop = [\n    'us_tax_mexico',\n    'weekofyr',\n]\n\ndf_wm_train_pruned = df_wm_train.drop(columns=to_drop)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8399b60f","cell_type":"code","source":"%%skip\ndf_wm_train_pruned","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1d867b77","cell_type":"markdown","source":"## ML Training","metadata":{}},{"id":"1f6c5ced","cell_type":"markdown","source":"### 1st selection","metadata":{}},{"id":"bda32584","cell_type":"markdown","source":"#### Train Test Split","metadata":{}},{"id":"b9d68cdd-6835-443b-bd86-483c7971cec7","cell_type":"code","source":"\n#Kaggle\ndf_wm_train_pruned = pd.read_csv('/kaggle/input/df-wm-train02/df_wm_train02.csv')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a7ecf7f1-2bd2-46b5-8c56-691b76b42703","cell_type":"code","source":"%%skip\n#VsCode\ndf_wm_train_pruned = pd.read_csv('csv_files/ml_train_data/df_wm_train02.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cda7d960","cell_type":"code","source":"df_merged = df_wm_train_pruned\ndf_merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2e34fafe","cell_type":"code","source":"df_merged['date'] = df_wm_train_pruned['date_ordinal'].apply(lambda x: datetime.fromordinal(int(x)))\n\n# Verify the resulting date range\nmin_date = df_merged['date'].min()\nmax_date = df_merged['date'].max()\n\nprint(f\"First date in series: {min_date.date()}\")\nprint(f\"Last date in series: {max_date.date()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ed184ad3","cell_type":"code","source":"df = df_merged.copy()\n\n# cutoff as 52 weeks before the last date\nlast_date = df['date'].max()\ncutoff   = last_date - pd.Timedelta(weeks=52)\n\n# split\ntrain = df[df['date'] <= cutoff]\ntest  = df[df['date'] >  cutoff]\n\nprint(f\"Training set: {train['date'].min().date()} → {train['date'].max().date()} ({len(train)} rows)\")\nprint(f\"Test set:     {test['date'].min().date()} → {test['date'].max().date()} ({len(test)} rows)\")\n\n\n# What share of rows are in TRAIN vs TEST?\n\ntotal_rows  = len(train) + len(test)\ntrain_pct   = len(train) / total_rows * 100\ntest_pct    = len(test)  / total_rows * 100\n\n\nprint(f\"Rows in training set : {len(train):,}  ({train_pct:5.1f} %)\")\nprint(f\"Rows in test set     : {len(test):,}  ({test_pct:5.1f} %)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ee5f09b8","cell_type":"code","source":"X_train = train.drop(columns=['weekly_sales', 'date', 'holiday_name'])\ny_train = train['weekly_sales']\n\nX_test  = test.drop(columns=['weekly_sales', 'date', 'holiday_name'])\ny_test  = test['weekly_sales']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a5d414cc","cell_type":"markdown","source":"#### Custome Evaluation metric (WMAE)","metadata":{}},{"id":"4c9f20f6","cell_type":"code","source":"ish = train['isholiday']   # 0/1 Series indexed same as X_train, y_train\n\ndef weight_fn(idx_labels):\n    \"\"\"\n    idx_labels: an Index of row‐labels from y_true\n    returns an array of 5s and 1s matching those labels.\n    \"\"\"\n    # .loc uses the actual labels (dates + store/dept multi‐index, if any)\n    return np.where(ish.loc[idx_labels] == 1, 5, 1)\n\n# Two‐arg WMAE that ModelTrainer expects\ndef wmae_custom(y_true, y_pred):\n    \"\"\"\n    y_true: pd.Series\n    y_pred: np.ndarray (same length)\n    \"\"\"\n    w = weight_fn(y_true.index)\n    # now compute weighted MAE\n    return (w * np.abs(y_true - y_pred)).sum() / w.sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"370e57b8","cell_type":"markdown","source":"#### LightGBM Hyper Parameters Search . CV Estimate (used to choose hyper-parameters faster)","metadata":{}},{"id":"9373a3d8","cell_type":"markdown","source":"##### LightGBM Search","metadata":{}},{"id":"7d25d820","cell_type":"code","source":"%%skip\ndef lgb_pipeline_factory(params):\n    return Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\",  lgb.LGBMRegressor(**params, random_state=7, n_jobs=-1))\n    ])\n\ntrainer = ModelTrainer(\n    X             = X_train,\n    y             = y_train,\n    model_factory = lgb_pipeline_factory, \n    search        = 'random',\n    param_grid    = {\n        'num_leaves':       [31, 50, 100, 150, 200],\n        'max_depth':        [-1, 5, 10, 15, 20],\n        'learning_rate':    [0.01, 0.05, 0.1],\n        'n_estimators':     [100, 300, 500],\n        'feature_fraction': [0.6, 0.8, 1.0],\n        'bagging_fraction': [0.6, 0.8, 1.0],\n        'bagging_freq':     [0, 5, 10],\n        'min_child_samples':[5, 10, 20, 50],\n        # new regularization / split-gain params:\n        'min_split_gain':   [0, 0.1, 0.5, 1.0],\n        'lambda_l1':        [0, 0.1, 0.5, 1.0],\n        'lambda_l2':        [0, 0.1, 0.5, 1.0],\n    },\n    n_iter           = 50,\n    cv_splitter      = TimeSeriesSplit(n_splits=2),\n    custom_metrics   = {'wmae': wmae_custom},\n    log_path         = 'csv_files/ml_train_data/lgbm_random_wmae01.csv',\n    model_name       = 'LGBM_WMAE',\n    problem_type     = 'reg',\n    n_jobs           = -1,\n    random_state     = 7\n)\n\n\n# Run the search\nbest_params, best_score = trainer.train()\nprint(\" Best params:\", best_params)\nprint(\" Best WMAE:  \", best_score)\n\n# explicitly close the CSV handle\ntrainer.csv_file.close()\n# drop the trainer object and force garbage collection\ndel trainer\nimport gc; gc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4925be82","cell_type":"code","source":"%%skip\nlgbm_random_wmae01 = pd.read_csv('csv_files/ml_train_data/lgbm_random_wmae01.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"41d9c30b","cell_type":"code","source":"%%skip\ndf = lgbm_random_wmae01.copy()             \n\nparams_df = (\n    df['param_json']\n    .apply(lambda s: json.loads(s) if pd.notnull(s) else {})  \n    .apply(pd.Series)                                         \n)\ndf = pd.concat([df, params_df], axis=1)\n\nmetric = 'wmae'\nignore = {'model_name', 'param_json',            \n          'mae', 'rmse', 'r2', metric, '__source__'}\n\nparam_cols = [c for c in df.columns if c not in ignore]\n\n# keep only numeric hyper-params\nX = df[param_cols].select_dtypes(include=[np.number])\ny = df[metric]\n\n# guard against “empty X” (all NaNs / non-numeric)\nif X.shape[1] == 0:\n    raise ValueError(\"No numeric hyper-parameter columns left after filtering!\")\n\n# urrogate model + analyses \nrf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\nrf.fit(X, y)\n\nimportances = pd.Series(rf.feature_importances_, index=X.columns)\nprint(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n\nperm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\nperm_imp = pd.Series(perm.importances_mean, index=X.columns)\nprint(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n\n# partial-dependence plots \ntop = importances.sort_values(ascending=False).index[:]\n\nfig, axes = plt.subplots(4, 4, figsize=(32, 32), sharey=True)\naxes = axes.flatten()\n\nfor ax, feat in zip(axes, top):\n    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n    ax.set_title(f\"PDP: {feat}\")\n\nfor ax in axes[len(top):]:\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b7150b09","cell_type":"code","source":"%%skip\ndef lgb_pipeline_factory(params):\n    return Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\",  lgb.LGBMRegressor(**params, random_state=7, n_jobs=-1))\n    ])\n\ntrainer = ModelTrainer(\n    X             = X_train,\n    y             = y_train,\n    model_factory = lgb_pipeline_factory, \n    search        = 'random',\n    param_grid    = {\n        'num_leaves':       [150],\n        'max_depth':        [0, 100, 200],\n        'learning_rate':    [0.1],\n        'n_estimators':     [500],\n        'feature_fraction': [1],\n        'bagging_fraction': [1],\n        'bagging_freq':     [0, 20, 40],\n        'min_child_samples':[1],\n        # new regularization / split-gain params:\n        'min_split_gain':   [3,4,5],\n        'lambda_l1':        [0.1],\n        'lambda_l2':        [3,4,5],\n    },\n    n_iter           = 100,\n    cv_splitter      = TimeSeriesSplit(n_splits=2),\n    custom_metrics   = {'wmae': wmae_custom},\n    log_path         = 'csv_files/ml_train_data/lgbm_random_wmae03.csv',\n    model_name       = 'LGBM_WMAE',\n    problem_type     = 'reg',\n    n_jobs           = -1,\n    random_state     = 7\n)\n\n# Run the search\nbest_params, best_score = trainer.train()\nprint(\" Best params:\", best_params)\nprint(\" Best WMAE:  \", best_score)\n\n# explicitly close the CSV handle\ntrainer.csv_file.close()\n# drop the trainer object and force garbage collection\ndel trainer\nimport gc; gc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a4413578","cell_type":"code","source":"%%skip\nlgbm_random_wmae02 = pd.read_csv('csv_files/ml_train_data/lgbm_random_wmae02.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2f2bbfcb","cell_type":"code","source":"%%skip\n# load & explode both CSVs if you haven’t yet —\npaths = [\n    'csv_files/ml_train_data/lgbm_random_wmae01.csv',\n    'csv_files/ml_train_data/lgbm_random_wmae02.csv',\n]\ndfs = []\nfor path in paths:\n    df = pd.read_csv(path)\n    params = df['param_json'].apply(json.loads).apply(pd.Series)\n    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n    df['__source__'] = path.split('/')[-1]\n    dfs.append(df)\ndf_all = pd.concat(dfs, ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"49786d56","cell_type":"code","source":"%%skip\n# choose metric & hyper‐param cols, drop any non‐numeric ones —\nmetric = 'wmae'\nignore = {'model_name','mae','rmse','r2', metric, '__source__'}\nparam_cols = [c for c in df_all.columns if c not in ignore]\n\n# keep only numeric hyperparams\nX = df_all[param_cols].select_dtypes(include=[np.number])\ny = df_all[metric]\n\n# train surrogate —\nrf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\nrf.fit(X, y)\n\n# RF importances\nimportances = pd.Series(rf.feature_importances_, index=X.columns)\nprint(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n\n# permutation importances\nperm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\nperm_imp = pd.Series(perm.importances_mean, index=X.columns)\nprint(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n\n# partial dependence plots for top-3 features, all in one figure —\ntop = importances.sort_values(ascending=False).index[:].tolist()\n\nfig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\naxes = axes.flatten()\n\nfor ax, feat in zip(axes, top):\n    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n    ax.set_title(f\"PDP: {feat}\")\n\n# turn off the last (unused) subplot\naxes[-1].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c70bccdb","cell_type":"code","source":"%%skip\ndef lgb_pipeline_factory(params):\n    return Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\",  lgb.LGBMRegressor(**params, random_state=7, n_jobs=-1))\n    ])\n\ntrainer = ModelTrainer(\n    X             = X_train,\n    y             = y_train,\n    model_factory = lgb_pipeline_factory, \n    search        = 'random',\n    param_grid    = {\n        'num_leaves':       [150],\n        'max_depth':        [0],\n        'learning_rate':    [0.1, 0.2, 0.3, 0.4],\n        'n_estimators':     [500,600,700,800],\n        'feature_fraction': [1],\n        'bagging_fraction': [1],\n        'bagging_freq':     [0],\n        'min_child_samples':[1],\n        # new regularization / split-gain params:\n        'min_split_gain':   [3],\n        'lambda_l1':        [0.1],\n        'lambda_l2':        [4, 6, 7],\n    },\n    n_iter           = 100,\n    cv_splitter      = TimeSeriesSplit(n_splits=2),\n    custom_metrics   = {'wmae': wmae_custom},\n    log_path         = 'csv_files/ml_train_data/lgbm_random_wmae03.csv',\n    model_name       = 'LGBM_WMAE',\n    problem_type     = 'reg',\n    n_jobs           = -1,\n    random_state     = 7\n)\n\n# Run the search\nbest_params, best_score = trainer.train()\nprint(\" Best params:\", best_params)\nprint(\" Best WMAE:  \", best_score)\n\n# explicitly close the CSV handle\ntrainer.csv_file.close()\n# drop the trainer object and force garbage collection\ndel trainer\nimport gc; gc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b469c2a9","cell_type":"code","source":"%%skip\n# load & explode both CSVs if you haven’t yet —\npaths = [\n    'csv_files/ml_train_data/lgbm_random_wmae01.csv',\n    'csv_files/ml_train_data/lgbm_random_wmae02.csv',\n    'csv_files/ml_train_data/lgbm_random_wmae03.csv',\n]\ndfs = []\nfor path in paths:\n    df = pd.read_csv(path)\n    params = df['param_json'].apply(json.loads).apply(pd.Series)\n    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n    df['__source__'] = path.split('/')[-1]\n    dfs.append(df)\ndf_all = pd.concat(dfs, ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e0d6365f","cell_type":"code","source":"%%skip\n# choose metric & hyper‐param cols, drop any non‐numeric ones —\nmetric = 'wmae'\nignore = {'model_name','mae','rmse','r2', metric, '__source__'}\nparam_cols = [c for c in df_all.columns if c not in ignore]\n\n# keep only numeric hyperparams\nX = df_all[param_cols].select_dtypes(include=[np.number])\ny = df_all[metric]\n\n# train surrogate —\nrf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\nrf.fit(X, y)\n\n# RF importances\nimportances = pd.Series(rf.feature_importances_, index=X.columns)\nprint(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n\n# permutation importances\nperm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\nperm_imp = pd.Series(perm.importances_mean, index=X.columns)\nprint(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n\n# partial dependence plots for top-3 features, all in one figure —\ntop = importances.sort_values(ascending=False).index[:].tolist()\n\nfig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\naxes = axes.flatten()\n\nfor ax, feat in zip(axes, top):\n    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n    ax.set_title(f\"PDP: {feat}\")\n\n# turn off the last (unused) subplot\naxes[-1].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6c4ddffb","cell_type":"code","source":"%%skip\ndef lgb_pipeline_factory(params):\n    return Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\",  lgb.LGBMRegressor(**params, random_state=7, n_jobs=-1))\n    ])\n\ntrainer = ModelTrainer(\n    X             = X_train,\n    y             = y_train,\n    model_factory = lgb_pipeline_factory, \n    search        = 'random',\n    param_grid    = {\n        'num_leaves':       [150],\n        'max_depth':        [0],\n        'learning_rate':    [0.1],\n        'n_estimators':     [500],\n        'feature_fraction': [1],\n        'bagging_fraction': [1],\n        'bagging_freq':     [0],\n        'min_child_samples':[0],\n        # new regularization / split-gain params:\n        'min_split_gain':   [3],\n        'lambda_l1':        [0.1],\n        'lambda_l2':        [6, 8, 9, 10, 11, 12],\n    },\n    n_iter           = 100,\n    cv_splitter      = TimeSeriesSplit(n_splits=2),\n    custom_metrics   = {'wmae': wmae_custom},\n    log_path         = 'csv_files/ml_train_data/lgbm_random_wmae04.csv',\n    model_name       = 'LGBM_WMAE',\n    problem_type     = 'reg',\n    n_jobs           = -1,\n    random_state     = 7\n)\n\n# Run the search\nbest_params, best_score = trainer.train()\nprint(\" Best params:\", best_params)\nprint(\" Best WMAE:  \", best_score)\n\n# explicitly close the CSV handle\ntrainer.csv_file.close()\n# drop the trainer object and force garbage collection\ndel trainer\nimport gc; gc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1a383ae4","cell_type":"code","source":"%%skip\n# load & explode both CSVs if you haven’t yet —\npaths = [\n    'csv_files/ml_train_data/lgbm_random_wmae01.csv',\n    'csv_files/ml_train_data/lgbm_random_wmae02.csv',\n    'csv_files/ml_train_data/lgbm_random_wmae03.csv',\n    'csv_files/ml_train_data/lgbm_random_wmae04.csv',\n\n]\ndfs = []\nfor path in paths:\n    df = pd.read_csv(path)\n    params = df['param_json'].apply(json.loads).apply(pd.Series)\n    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n    df['__source__'] = path.split('/')[-1]\n    dfs.append(df)\ndf_all = pd.concat(dfs, ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f884f00c","cell_type":"code","source":"%%skip\n# choose metric & hyper‐param cols, drop any non‐numeric ones —\nmetric = 'wmae'\nignore = {'model_name','mae','rmse','r2', metric, '__source__'}\nparam_cols = [c for c in df_all.columns if c not in ignore]\n\n# keep only numeric hyperparams\nX = df_all[param_cols].select_dtypes(include=[np.number])\ny = df_all[metric]\n\n# train surrogate —\nrf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\nrf.fit(X, y)\n\n# RF importances\nimportances = pd.Series(rf.feature_importances_, index=X.columns)\nprint(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n\n# permutation importances\nperm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\nperm_imp = pd.Series(perm.importances_mean, index=X.columns)\nprint(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n\n# partial dependence plots for top-3 features, all in one figure —\ntop = importances.sort_values(ascending=False).index[:].tolist()\n\nfig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\naxes = axes.flatten()\n\nfor ax, feat in zip(axes, top):\n    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n    ax.set_title(f\"PDP: {feat}\")\n\n# turn off the last (unused) subplot\naxes[-1].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"37d8462a","cell_type":"code","source":"%%skip\ndf_lgbm_random_04 = df_all","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"21b3ecbe","cell_type":"code","source":"%%skip\ndf_lgbm_random_04.to_csv('csv_files/ml_train_data/df_lgbm_random_04.csv', index=None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bcf2d254","cell_type":"code","source":"%%skip\ndf_lgbm_random_04 = pd.read_csv('csv_files/ml_train_data/df_lgbm_random_04.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"38f4c8a1","cell_type":"markdown","source":"##### LightGBM Best Model","metadata":{}},{"id":"49673cc4","cell_type":"markdown","source":"Get best params from csv","metadata":{}},{"id":"81db62ec","cell_type":"code","source":"%%skip\ndf = df_lgbm_random_04.copy()\n\n# pick the row with the lowest WMAE\nbest_row = df.loc[df['wmae'].idxmin()]\n\n# keep only the hyper-parameter columns\nnon_params = {'model_name', 'mae', 'rmse', 'r2', 'wmae', '__source__'}\nparam_series = best_row.drop(labels=non_params)\n\n# convert to a clean dict, turning floats like 500.0 → 500\nbest_params = {\n    k: (int(v) if isinstance(v, (int, float)) and not math.isnan(v) and v.is_integer() else v)\n    for k, v in param_series.items()\n    if pd.notnull(v)                           # skip NaNs\n}\n\n\nbest_params\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"de16f4cf","cell_type":"markdown","source":"Train best model ","metadata":{}},{"id":"d50c0e4a","cell_type":"code","source":"%%skip\n# Rebuild your best‐found model\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", lgb.LGBMRegressor(**best_params, random_state=7, n_jobs=-1))\n])\n\n# 1) Fit on the entire training set\npipe.fit(X_train, y_train)\n\n# 2) Predict on the test set\ny_pred = pipe.predict(X_test)\n\n# 3) Compute standard metrics\nmae  = mean_absolute_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nr2   = r2_score(y_test, y_pred)\n\n# 4) Compute WMAE (holiday weeks weight=5, others=1)\n#    assumes you still have `test` DataFrame with 'holiday_name'\nweights = np.where(test['holiday_name'].notnull(), 5, 1)\nwmae    = (weights * np.abs(y_test - y_pred)).sum() / weights.sum()\n\n# 5) Print them all\nprint(f\"Test MAE:   {mae:.4f}\")\nprint(f\"Test RMSE:  {rmse:.4f}\")\nprint(f\"Test R²:    {r2:.4f}\")\nprint(f\"Test WMAE:  {wmae:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9fd62ad4","cell_type":"markdown","source":"Check what features were used","metadata":{}},{"id":"ddf1bdb6","cell_type":"code","source":"%%skip\n# grab the fitted LGBM from your pipeline\nmodel = pipe.named_steps['model']\n\n# make a Series of the importances\nfi = pd.Series(\n    model.feature_importances_,\n    index = X_train.columns\n).sort_values(ascending=False)\n\n# print the top 10\nprint(\"🌳 LightGBM split-gain importances:\")\nprint(fi)\n\n# number of times the feature was used by the model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"73fdb39a","cell_type":"markdown","source":"##### LGBM Retrain wihout not used features (Best Model to CSV)","metadata":{}},{"id":"8d43ec05","cell_type":"code","source":"%%skip\ncutoff  = df_merged.date.max() - pd.Timedelta(weeks=52)\ntrain   = df_merged[df_merged.date <= cutoff]\ntest    = df_merged[df_merged.date >  cutoff]\n\ndrop = [\n    'weekly_sales','date','holiday_name','year',\n    'markdown5','markdown1','markdown2','markdown3','markdown4',\n    'us_tax_india','us_tax_vietnam','us_tax_china','us_tax_canada'\n]\n\nX_train, y_train = train.drop(columns=drop), train.weekly_sales\nX_test,  y_test  = test .drop(columns=drop),  test .weekly_sales\n\n\n# Fit model, keep only used features\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\",  lgb.LGBMRegressor(**best_params, random_state=7, n_jobs=-1))\n])\npipe.fit(X_train, y_train)\n\nused_feats = X_train.columns[pipe.named_steps['model'].feature_importances_ > 0]\npipe.fit(X_train[used_feats], y_train)\n\n\n# Predict & compute metrics\n\ny_pred  = pipe.predict(X_test[used_feats])\n\nmae     = mean_absolute_error(y_test, y_pred)\nrmse    = np.sqrt(mean_squared_error(y_test, y_pred))\nr2      = r2_score(y_test, y_pred)\nweights = np.where(test['holiday_name'].notnull(), 5, 1)\nwmae    = (weights * np.abs(y_test - y_pred)).sum() / weights.sum()\n\n\nrow = {\n    \"model_name\": 'LGBM01',\n    **best_params,\n    \"mae\":  mae,\n    \"rmse\": rmse,\n    \"r2\":   r2,\n    \"wmae\": wmae\n}\ndf_best_models = pd.DataFrame([row])\n\ndf_best_models\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1217948b","cell_type":"code","source":"%%skip\ndf_best_models.to_csv('csv_files/ml_train_data/df_best_models.csv', index=None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"903370b0","cell_type":"code","source":"%%skip\ndf_best_models = pd.read_csv('csv_files/ml_train_data/df_best_models.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"60ca00b9","cell_type":"code","source":"%%skip\n# to merge new best model : \nresults_df = pd.concat([results_df, df_best_models], ignore_index=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9f153ce4","cell_type":"markdown","source":"### XGBoostRegressor Hyper Parameters Search / CV Estimate (used to choose hyper-parameters faster)","metadata":{}},{"id":"f37252fb","cell_type":"markdown","source":"##### XGBoost Search","metadata":{}},{"id":"85e2709d","cell_type":"code","source":"%%skip\n# Define an XGBoost pipeline factory\ndef xgb_pipeline_factory(params):\n    return Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\",  XGBRegressor(**params, random_state=7, n_jobs=-1, verbosity=1))\n    ])\n\n# Set up the random search over a balanced grid of XGBoost hyper-params\ntrainer = ModelTrainer(\n    X             = X_train,\n    y             = y_train,\n    model_factory = xgb_pipeline_factory, \n    search        = 'random',\n    param_grid    = {\n        'n_estimators':      [700,800,900],\n        'max_depth':         [10, 11, 12, 13],\n        'learning_rate':     [0.2, 0.3, 0.4, 0.5],\n        'subsample':         [1],\n        'colsample_bytree':  [0.7, 0.8, 0.9],\n        'gamma':             [0.01, 0.05 ,0.1],\n        'min_child_weight':  [1, 5, 10, 20],\n        'reg_alpha':         [1, 2, 3],\n        'reg_lambda':        [0.8, 1.0, 1.2],\n    },\n    n_iter           = 100,\n    cv_splitter      = TimeSeriesSplit(n_splits=3),\n    custom_metrics   = {'wmae': wmae_custom},\n    log_path         = 'csv_files/ml_train_data/xgb_random_wmae01.csv',\n    model_name       = 'XGB_WMAE',\n    problem_type     = 'reg',\n    n_jobs           = -1,\n    random_state     = 7\n)\n\n# Run the search\nbest_params, best_score = trainer.train()\nprint(\"Best params:\", best_params)\nprint(\" Best WMAE:  \", best_score)\n\n# Clean up\ntrainer.csv_file.close()\ndel trainer\nimport gc; gc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fced5b6a","cell_type":"code","source":"%%skip\nxgb_random_wmae01 = pd.read_csv('csv_files/ml_train_data/xgb_random_wmae01.csv')\nxgb_random_wmae01","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ff092b55","cell_type":"code","source":"%%skip\ndf = xgb_random_wmae01.copy()             \n\nparams_df = (\n    df['param_json']\n    .apply(lambda s: json.loads(s) if pd.notnull(s) else {})  \n    .apply(pd.Series)                                         \n)\ndf = pd.concat([df, params_df], axis=1)\n\nmetric = 'wmae'\nignore = {'model_name', 'param_json',            \n          'mae', 'rmse', 'r2', metric, '__source__'}\n\nparam_cols = [c for c in df.columns if c not in ignore]\n\n# keep only numeric hyper-params\nX = df[param_cols].select_dtypes(include=[np.number])\ny = df[metric]\n\n# guard against “empty X” (all NaNs / non-numeric)\nif X.shape[1] == 0:\n    raise ValueError(\"No numeric hyper-parameter columns left after filtering!\")\n\n# urrogate model + analyses \nrf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\nrf.fit(X, y)\n\nimportances = pd.Series(rf.feature_importances_, index=X.columns)\nprint(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n\nperm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\nperm_imp = pd.Series(perm.importances_mean, index=X.columns)\nprint(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n\n# partial-dependence plots \ntop = importances.sort_values(ascending=False).index[:]\n\nfig, axes = plt.subplots(4, 4, figsize=(32, 32), sharey=True)\naxes = axes.flatten()\n\nfor ax, feat in zip(axes, top):\n    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n    ax.set_title(f\"PDP: {feat}\")\n\nfor ax in axes[len(top):]:\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"73a322dd","cell_type":"code","source":"%%skip\n# Define an XGBoost pipeline factory\ndef xgb_pipeline_factory(params):\n    return Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\",  XGBRegressor(**params, random_state=7, n_jobs=-1, verbosity=1))\n    ])\n\n# Set up the random search over a balanced grid of XGBoost hyper-params\ntrainer = ModelTrainer(\n    X             = X_train,\n    y             = y_train,\n    model_factory = xgb_pipeline_factory, \n    search        = 'random',\n    param_grid    = {\n        'n_estimators':      [200, 500, 700],\n        'max_depth':         [2,5,8,10],\n        'learning_rate':     [0, 0.05, 0.1, 0.2],\n        'subsample':         [0, 0.5, 1],\n        'colsample_bytree':  [0, 0.5, 0.8, 1],\n        'gamma':             [0 ,0.001],\n        'min_child_weight':  [0, 0.05, 0.1, 1],\n        'reg_alpha':         [0, 0.5, 5],\n        'reg_lambda':        [1.0],\n    },\n    n_iter           = 180,\n    cv_splitter      = TimeSeriesSplit(n_splits=3),\n    custom_metrics   = {'wmae': wmae_custom},\n    log_path         = 'csv_files/ml_train_data/xgb_random_wmae02.csv',\n    model_name       = 'XGB_WMAE',\n    problem_type     = 'reg',\n    n_jobs           = -1,\n    random_state     = 7\n)\n\n# 3) Run the search\nbest_params, best_score = trainer.train()\nprint(\"Best params:\", best_params)\nprint(\" Best WMAE:  \", best_score)\n\n# 4) Clean up\ntrainer.csv_file.close()\ndel trainer\nimport gc; gc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"426646f6","cell_type":"code","source":"%%skip\n# load & explode both CSVs if you haven’t yet —\npaths = [\n    'csv_files/ml_train_data/xgb_random_wmae01.csv',\n    'csv_files/ml_train_data/xgb_random_wmae02.csv',\n]\ndfs = []\nfor path in paths:\n    df = pd.read_csv(path)\n    params = df['param_json'].apply(json.loads).apply(pd.Series)\n    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n    df['__source__'] = path.split('/')[-1]\n    dfs.append(df)\ndf_all = pd.concat(dfs, ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ac5faf2b","cell_type":"code","source":"%%skip\n# choose metric & hyper‐param cols, drop any non‐numeric ones —\nmetric = 'wmae'\nignore = {'model_name','mae','rmse','r2', metric, '__source__'}\nparam_cols = [c for c in df_all.columns if c not in ignore]\n\n# keep only numeric hyperparams\nX = df_all[param_cols].select_dtypes(include=[np.number])\ny = df_all[metric]\n\n# train surrogate —\nrf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\nrf.fit(X, y)\n\n# RF importances\nimportances = pd.Series(rf.feature_importances_, index=X.columns)\nprint(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n\n# permutation importances\nperm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\nperm_imp = pd.Series(perm.importances_mean, index=X.columns)\nprint(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n\n# partial dependence plots for top-3 features, all in one figure —\ntop = importances.sort_values(ascending=False).index[:].tolist()\n\nfig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\naxes = axes.flatten()\n\nfor ax, feat in zip(axes, top):\n    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n    ax.set_title(f\"PDP: {feat}\")\n\n# turn off the last (unused) subplot\naxes[-1].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"27a5241d","cell_type":"code","source":"%%skip\ndf_xgb_random_02 = df_all\ndf_xgb_random_02.to_csv('csv_files/ml_train_data/df_xgb_random_02.csv', index=None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a1d25595","cell_type":"code","source":"%%skip\ndf_xgb_random_02 = pd.read_csv('csv_files/ml_train_data/df_xgb_random_02.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7d44af16","cell_type":"markdown","source":"##### XGBoost Best Model","metadata":{}},{"id":"f61509a0","cell_type":"markdown","source":"Get Best Params","metadata":{}},{"id":"14129844","cell_type":"code","source":"%%skip\ndf = df_xgb_random_02.copy()\n\n# pick the row with the lowest WMAE\nbest_row = df.loc[df['wmae'].idxmin()]\n\n# keep only the hyper-parameter columns\nnon_params = {'model_name', 'mae', 'rmse', 'r2', 'wmae', '__source__'}\nparam_series = best_row.drop(labels=non_params)\n\n# convert to a clean dict, turning floats like 500.0 → 500\nbest_params = {\n    k: (int(v) if isinstance(v, (int, float)) and not math.isnan(v) and v.is_integer() else v)\n    for k, v in param_series.items()\n    if pd.notnull(v)                           # skip NaNs\n}\n\n\nbest_params","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d3dbe50e","cell_type":"markdown","source":"Train Model with Best Params","metadata":{}},{"id":"ec7d1cff","cell_type":"code","source":"%%skip\n# Rebuild your best‐found model\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", XGBRegressor(**best_params, random_state=7, n_jobs=-1))\n])\n\n# 1) Fit on the entire training set\npipe.fit(X_train, y_train)\n\n# 2) Predict on the test set\ny_pred = pipe.predict(X_test)\n\n# 3) Compute standard metrics\nmae  = mean_absolute_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nr2   = r2_score(y_test, y_pred)\n\n# 4) Compute WMAE (holiday weeks weight=5, others=1)\n#    assumes you still have `test` DataFrame with 'holiday_name'\nweights = np.where(test['holiday_name'].notnull(), 5, 1)\nwmae    = (weights * np.abs(y_test - y_pred)).sum() / weights.sum()\n\n# 5) Print them all\nprint(f\"Test MAE:   {mae:.4f}\")\nprint(f\"Test RMSE:  {rmse:.4f}\")\nprint(f\"Test R²:    {r2:.4f}\")\nprint(f\"Test WMAE:  {wmae:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1ca6d13f","cell_type":"markdown","source":"Check what features were most used","metadata":{}},{"id":"21a3cc0e","cell_type":"code","source":"%%skip\n# grab the fitted LGBM from your pipeline\nmodel = pipe.named_steps['model']\n\n# make a Series of the importances\nfi = pd.Series(\n    model.feature_importances_,\n    index = X_train.columns\n).sort_values(ascending=False)\n\n# print the top 10\nprint(\"🌳 LightGBM split-gain importances:\")\nprint(fi)\n\n# number of times the feature was used by the model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"762892c9","cell_type":"markdown","source":"##### XGBOOST Retrain wihout not used features (Best Model to CSV)","metadata":{}},{"id":"2081d449","cell_type":"code","source":"%%skip\ncutoff  = df_merged.date.max() - pd.Timedelta(weeks=52)\ntrain   = df_merged[df_merged.date <= cutoff]\ntest    = df_merged[df_merged.date >  cutoff]\n\ndrop = [\n    'weekly_sales','date','holiday_name','pce_healthcare_services'\n]\n\nX_train, y_train = train.drop(columns=drop), train.weekly_sales\nX_test,  y_test  = test .drop(columns=drop),  test .weekly_sales\n\n\n# Fit model, keep only used features\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\",  XGBRegressor(**best_params, random_state=7, n_jobs=-1))\n])\npipe.fit(X_train, y_train)\n\nused_feats = X_train.columns[pipe.named_steps['model'].feature_importances_ > 0]\npipe.fit(X_train[used_feats], y_train)\n\n\n# Predict & compute metrics\n\ny_pred  = pipe.predict(X_test[used_feats])\n\nmae     = mean_absolute_error(y_test, y_pred)\nrmse    = np.sqrt(mean_squared_error(y_test, y_pred))\nr2      = r2_score(y_test, y_pred)\nweights = np.where(test['holiday_name'].notnull(), 5, 1)\nwmae    = (weights * np.abs(y_test - y_pred)).sum() / weights.sum()\n\n\nrow = {\n    \"model_name\": 'XGBOOST01',\n    **best_params,\n    \"mae\":  mae,\n    \"rmse\": rmse,\n    \"r2\":   r2,\n    \"wmae\": wmae\n}\ndf_best_xgboost = pd.DataFrame([row])\n\ndf_best_xgboost\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b613d446","cell_type":"code","source":"%%skip\n# to merge new best model : \nresults_df = pd.concat([df_best_xgboost, df_best_models], ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d8361eaa","cell_type":"code","source":"%%skip\ndf_best_models = results_df\ndf_best_models.to_csv('csv_files/ml_train_data/df_best_models.csv', index=None)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"665cb3f8","cell_type":"code","source":"%%skip\ndf_best_models = pd.read_csv('csv_files/ml_train_data/df_best_models.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6243d52a","cell_type":"markdown","source":"### CatBoost  Hyper Parameters Search / CV Estimate (used to choose hyper-parameters faster)","metadata":{}},{"id":"0ef14f4d","cell_type":"code","source":"%%skip\ndef cat_pipeline_factory(params):\n    \"\"\"Return a (scaler → CatBoost) pipeline with the given hyper-params.\"\"\"\n    return Pipeline([\n        (\"scaler\", StandardScaler()),                # numeric features only; harmless for CatBoost\n        (\"model\",  CatBoostRegressor(\n                        **params,\n                        random_state=7,\n                        verbose=0,                  # silence per-iteration logging\n                    )\n        )\n    ])\n\n# Wide, balanced CatBoost hyper-parameter grid\n\nparam_grid = {\n    # core learning-rate / depth / boosting length\n    'iterations'        : [ 500, 1000, 1500, 2000 ],\n    'depth'             : [ 4, 6, 8, 10 ],\n    'learning_rate'     : [ 0.01, 0.03, 0.1 ],\n    \n    # regularisation & tree shape\n    'l2_leaf_reg'       : [ 1, 3, 5, 10 ],\n    'min_data_in_leaf'  : [ 1, 5, 10, 20 ],\n    'random_strength'   : [ 0, 1, 5, 10 ],\n    'bagging_temperature': [ 0, 0.5, 1, 2 ],\n    \n    # sampling for rows / columns\n    'subsample'         : [ 0.6, 0.8, 1.0 ],\n    'rsm'               : [ 0.6, 0.8, 1.0 ],        # column sample\n       \n    # other structural knobs\n    'grow_policy'       : [ 'SymmetricTree', 'Depthwise', 'Lossguide' ],\n    'border_count'      : [ 32, 64, 128, 254 ],\n    'one_hot_max_size'  : [ 2, 5, 10 ],\n    \n    # objective variants (optional: restrict to one if you prefer)\n    'loss_function'     : [ 'MAE', 'RMSE']\n}\n\n# Random search trainer setup\n\ntrainer = ModelTrainer(\n    X               = X_train,\n    y               = y_train,\n    model_factory   = cat_pipeline_factory,\n    search          = 'random',\n    param_grid      = param_grid,\n    n_iter          = 90,                          # ← wide but balanced exploration\n    cv_splitter     = TimeSeriesSplit(n_splits=3),\n    custom_metrics  = {'wmae': wmae_custom},\n    log_path        = 'csv_files/ml_train_data/cat_random_wmae01.csv',\n    model_name      = 'CAT_WMAE',\n    problem_type    = 'reg',\n    n_jobs          = -1,\n    random_state    = 7\n)\n\n\n# Run the search\n\nbest_params, best_score = trainer.train()\n\nprint(\" Best WMAE: \", best_score)\n\n\n# Clean up\n\ntrainer.csv_file.close()\ndel trainer\nimport gc; gc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"49586d0e","cell_type":"code","source":"%%skip\ncat_random_wmae01 = pd.read_csv('csv_files/ml_train_data/cat_random_wmae01.csv')\ncat_random_wmae01","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d2ce7993","cell_type":"code","source":"%%skip\ndf = cat_random_wmae01.copy()             \n\nparams_df = (\n    df['param_json']\n    .apply(lambda s: json.loads(s) if pd.notnull(s) else {})  \n    .apply(pd.Series)                                         \n)\ndf = pd.concat([df, params_df], axis=1)\n\nmetric = 'wmae'\nignore = {'model_name', 'param_json',            \n          'mae', 'rmse', 'r2', metric, '__source__'}\n\nparam_cols = [c for c in df.columns if c not in ignore]\n\n# keep only numeric hyper-params\nX = df[param_cols].select_dtypes(include=[np.number])\ny = df[metric]\n\n# guard against “empty X” (all NaNs / non-numeric)\nif X.shape[1] == 0:\n    raise ValueError(\"No numeric hyper-parameter columns left after filtering!\")\n\n# urrogate model + analyses \nrf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\nrf.fit(X, y)\n\nimportances = pd.Series(rf.feature_importances_, index=X.columns)\nprint(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n\nperm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\nperm_imp = pd.Series(perm.importances_mean, index=X.columns)\nprint(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n\n# partial-dependence plots \ntop = importances.sort_values(ascending=False).index[:]\n\nfig, axes = plt.subplots(4, 4, figsize=(32, 32), sharey=True)\naxes = axes.flatten()\n\nfor ax, feat in zip(axes, top):\n    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n    ax.set_title(f\"PDP: {feat}\")\n\nfor ax in axes[len(top):]:\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2a9f0943","cell_type":"code","source":"%%skip\n#VSCode\ndef cat_pipeline_factory(params):\n    \"\"\"\n    Build a (StandardScaler → CatBoostRegressor) pipeline.\n\n    Adds early_stopping_rounds=200 on top of the hyper-parameters supplied\n    by the random-search engine.\n    \"\"\"\n    params = params.copy()\n    params.update(\n        early_stopping_rounds=200   # stop if validation metric hasn't improved\n                                    # for 200 consecutive trees\n    )\n\n    return Pipeline([\n        (\"scaler\", StandardScaler()),        # safe for purely numeric features\n        (\"model\", CatBoostRegressor(\n            **params,                        # hyper-parameters from search\n            random_state=7,                  # reproducible results\n            verbose=0                        # keep per-tree logging silent\n        ))\n    ])\n\n# Wide, balanced CatBoost hyper-parameter grid\n\nparam_grid = {\n    # core learning-rate / depth / boosting length\n    'iterations'        : [ 3000, 5000 ],\n    'depth'             : [10, 12, 14 ],\n    'learning_rate'     : [0.1, 0.2, 0.3 ],\n    \n    # regularisation & tree shape\n    'l2_leaf_reg'       : [ 3, 17, 20 ],\n    'min_data_in_leaf'  : [ 1,5],\n    'random_strength'   : [ 0.1],\n    'bagging_temperature': [ 0.1],\n    \n    # sampling for rows / columns\n    'subsample'         : [ 0.1, 0.8 ],\n    'rsm'               : [ 0.9 ],        # column sample\n       \n    # other structural knobs\n    'grow_policy'       : [ 'Depthwise', 'Lossguide' ],\n    'border_count'      : [ 254, 512, 1024 ],\n    'one_hot_max_size'  : [5],\n    \n    # objective variants (optional: restrict to one if you prefer)\n    'loss_function'     : [ 'MAE', 'RMSE']\n}\n\n# Random search trainer setup\n\ntrainer = ModelTrainer(\n    X               = X_train,\n    y               = y_train,\n    model_factory   = cat_pipeline_factory,\n    search          = 'random',\n    param_grid      = param_grid,\n    n_iter          = 50,                         \n    cv_splitter     = TimeSeriesSplit(n_splits=3),\n    custom_metrics  = {'wmae': wmae_custom},\n    log_path        = 'csv_files/ml_train_data/cat_random_wmae02.csv',\n    model_name      = 'CAT_WMAE',\n    problem_type    = 'reg',\n    n_jobs          = -1,\n    random_state    = 7\n)\n\n\n# Run the search\n\nbest_params, best_score = trainer.train()\n\nprint(\" Best WMAE: \", best_score)\n\n\n# Clean up\n\ntrainer.csv_file.close()\ndel trainer\nimport gc; gc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d0ef7815-ba2d-49ca-b80f-a24d0ce040eb","cell_type":"code","source":"%%skip\n#Kaggle\ndef cat_pipeline_factory(params):\n    params = params.copy()\n    params.update(\n        task_type=\"GPU\",\n        devices=\"0\",\n        metric_period=20,\n        early_stopping_rounds=200,\n        bootstrap_type=\"Bernoulli\"      # lets subsample work\n    )\n\n    return Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\", CatBoostRegressor(\n            **params,\n            random_state=7,\n            verbose=0\n        ))\n    ])\n\n\nparam_grid = {\n    # core learning-rate / depth / boosting length\n    \"iterations\"        : [3000, 5000],\n    \"depth\"             : [10, 12, 14],\n    \"learning_rate\"     : [0.1, 0.2, 0.3],\n\n    # regularisation & tree shape\n    \"l2_leaf_reg\"       : [3, 17, 20],\n    \"min_data_in_leaf\"  : [1, 5],\n    \"random_strength\"   : [0.1],\n\n    # sampling for rows\n    \"subsample\"         : [0.1, 0.8],        # row sampling now works\n    # rsm removed ──↑\n\n    # other structural knobs\n    \"grow_policy\"       : [\"Depthwise\", \"Lossguide\"],\n    \"border_count\"      : [254, 512, 1024],\n    \"one_hot_max_size\"  : [5],\n\n    # objective variants\n    \"loss_function\"     : [\"MAE\", \"RMSE\"],\n}\n\n\ntrainer = ModelTrainer(\n    X               = X_train,\n    y               = y_train,\n    model_factory   = cat_pipeline_factory,\n    search          = \"random\",\n    param_grid      = param_grid,\n    n_iter          = 50,\n    cv_splitter     = TimeSeriesSplit(n_splits=3),\n    custom_metrics  = {\"wmae\": wmae_custom},\n    log_path        = \"/kaggle/working/cat_random_wmae02.csv\",\n    model_name      = \"CAT_WMAE\",\n    problem_type    = \"reg\",\n    n_jobs          = 1,                 # one fit → one GPU\n    random_state    = 7\n)\n\n\nbest_params, best_score = trainer.train()\nprint(\"Best WMAE:\", best_score)\n\n\ntrainer.csv_file.close()\ndel trainer\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3385f830","cell_type":"code","source":"%%skip\n# load & explode both CSVs if you haven’t yet —\npaths = [\n    'csv_files/ml_train_data/cat_random_wmae01.csv',\n    'csv_files/ml_train_data/cat_random_wmae02.csv',\n]\ndfs = []\nfor path in paths:\n    df = pd.read_csv(path)\n    params = df['param_json'].apply(json.loads).apply(pd.Series)\n    df = pd.concat([df.drop(columns='param_json'), params], axis=1)\n    df['__source__'] = path.split('/')[-1]\n    dfs.append(df)\ndf_all = pd.concat(dfs, ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9d2cca21","cell_type":"code","source":"%%skip\n# choose metric & hyper‐param cols, drop any non‐numeric ones —\nmetric = 'wmae'\nignore = {'model_name','mae','rmse','r2', metric, '__source__'}\nparam_cols = [c for c in df_all.columns if c not in ignore]\n\n# keep only numeric hyperparams\nX = df_all[param_cols].select_dtypes(include=[np.number])\ny = df_all[metric]\n\n# train surrogate —\nrf = RandomForestRegressor(n_estimators=200, random_state=7, n_jobs=-1)\nrf.fit(X, y)\n\n# RF importances\nimportances = pd.Series(rf.feature_importances_, index=X.columns)\nprint(\"→ RF importances:\\n\", importances.sort_values(ascending=False), \"\\n\")\n\n# permutation importances\nperm = permutation_importance(rf, X, y, n_repeats=10, random_state=7, n_jobs=-1)\nperm_imp = pd.Series(perm.importances_mean, index=X.columns)\nprint(\"→ Permutation importances:\\n\", perm_imp.sort_values(ascending=False), \"\\n\")\n\n# partial dependence plots for top-3 features, all in one figure —\ntop = importances.sort_values(ascending=False).index[:].tolist()\n\nfig, axes = plt.subplots(4, 4 ,figsize=(32, 32), sharey=True)\naxes = axes.flatten()\n\nfor ax, feat in zip(axes, top):\n    PartialDependenceDisplay.from_estimator(rf, X, [feat], ax=ax)\n    ax.set_title(f\"PDP: {feat}\")\n\n# turn off the last (unused) subplot\naxes[-1].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3c35834c","cell_type":"code","source":"#Kaggle\ndef cat_pipeline_factory(params):\n    params = params.copy()\n    params.update(\n        task_type=\"GPU\",\n        devices=\"0\",\n        metric_period=20,\n        early_stopping_rounds=200,\n        bootstrap_type=\"Bernoulli\"      # lets subsample work\n    )\n\n    return Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\", CatBoostRegressor(\n            **params,\n            random_state=7,\n            verbose=0\n        ))\n    ])\n\n\nparam_grid = {\n    # core learning-rate / depth / boosting length\n    \"iterations\"        : [2000],\n    \"depth\"             : [12, 16],\n    \"learning_rate\"     : [0.3, 0.4, 0.5],\n\n    # regularisation & tree shape\n    \"l2_leaf_reg\"       : [17,30],\n    \"min_data_in_leaf\"  : [3, 5],\n    \"random_strength\"   : [5],\n\n    # sampling for rows\n    \"subsample\"         : [0.8],        # row sampling now works\n    # rsm removed ──↑\n\n    # other structural knobs\n    \"grow_policy\"       : [\"Depthwise\", \"Lossguide\"],\n    \"border_count\"      : [1024, 2048, 4096],\n    \"one_hot_max_size\"  : [10,15, 20],\n\n    # objective variants\n    \"loss_function\"     : [\"MAE\", \"RMSE\"],\n}\n\n\ntrainer = ModelTrainer(\n    X               = X_train,\n    y               = y_train,\n    model_factory   = cat_pipeline_factory,\n    search          = \"random\",\n    param_grid      = param_grid,\n    n_iter          = 50,\n    cv_splitter     = TimeSeriesSplit(n_splits=3),\n    custom_metrics  = {\"wmae\": wmae_custom},\n    log_path        = \"/kaggle/working/cat_random_wmae03.csv\",\n    model_name      = \"CAT_WMAE\",\n    problem_type    = \"reg\",\n    n_jobs          = 1,                 # one fit → one GPU\n    random_state    = 7\n)\n\n\nbest_params, best_score = trainer.train()\nprint(\"Best WMAE:\", best_score)\n\n\ntrainer.csv_file.close()\ndel trainer\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"54c867f2-aa1b-4028-9381-f05692ea6993","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}